{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BachChorales_HandsOnCh15_2020_01_03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar-abhishek/handson-ml2/blob/master/Final_Chris_BachChorales_HandsOnCh15_2020_01_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMoT_7Ov5NQ7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzKqBwsW3Er8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_CMpICKqmd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM9bjle8BQGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.test.is_gpu_available():\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fmXPTqmLxe9",
        "colab_type": "text"
      },
      "source": [
        "> Ch15 Q10 \n",
        ">\n",
        "> Download the Bach chorales dataset and unzip it. It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long, and each time step contains 4 integers, where each integer corresponds to a note’s index on a piano (except for the value 0, which means that no note is played). Train a model—recurrent, convolutional, or both—that can predict the next time step (four notes), given a sequence of time steps from a chorale.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzkbOqlon7bi",
        "colab_type": "text"
      },
      "source": [
        "The Bach chorales are available from:\n",
        "\n",
        "https://github.com/ageron/handson-ml2/blob/master/datasets/jsb_chorales/jsb_chorales.tgz\n",
        "\n",
        "I downloaded them to my Google Drive. Then I mounted my Google Drive to Google Colab:\n",
        "\n",
        "`Expand the left pane > Files > Mount Drive`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_jmEQ1eipAo",
        "colab_type": "code",
        "outputId": "313b32a1-024c-4b2b-859e-86be08aff7e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaddEguV2iJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_dir_of_chorales(dir):\n",
        "\n",
        "  print(\"\\nReading chorales in: \" + dir + \"\\n---------------\\n\")\n",
        "\n",
        "  chorales = []\n",
        "  file_counter = 0\n",
        "  for filename in sorted(os.listdir(dir)):\n",
        "    file_counter += 1\n",
        "    if (file_counter % 10) == 0:\n",
        "      print(str(file_counter) + \") filename: \" + filename)\n",
        "\n",
        "    one_training_chorale = pd.read_csv(os.path.join(dir, filename)).to_numpy()\n",
        "    chorales.append(one_training_chorale)\n",
        "\n",
        "  return chorales"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk9W_GSXtwRG",
        "colab_type": "code",
        "outputId": "af82239e-d3ef-4912-8fb3-e93374cae18f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "data_path = \"/content/drive/My Drive/jsb_chorales\"\n",
        "\n",
        "train_path = os.path.join(data_path, 'train')\n",
        "valid_path = os.path.join(data_path, 'valid')\n",
        "test_path = os.path.join(data_path, 'test')\n",
        "\n",
        "bach_training_chorales = read_dir_of_chorales(train_path)\n",
        "bach_validation_chorales = read_dir_of_chorales(valid_path)\n",
        "bach_test_chorales = read_dir_of_chorales(test_path)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Reading chorales in: /content/drive/My Drive/jsb_chorales/train\n",
            "---------------\n",
            "\n",
            "10) filename: chorale_009.csv\n",
            "20) filename: chorale_019.csv\n",
            "30) filename: chorale_029.csv\n",
            "40) filename: chorale_039.csv\n",
            "50) filename: chorale_049.csv\n",
            "60) filename: chorale_059.csv\n",
            "70) filename: chorale_069.csv\n",
            "80) filename: chorale_079.csv\n",
            "90) filename: chorale_089.csv\n",
            "100) filename: chorale_099.csv\n",
            "110) filename: chorale_109.csv\n",
            "120) filename: chorale_119.csv\n",
            "130) filename: chorale_129.csv\n",
            "140) filename: chorale_139.csv\n",
            "150) filename: chorale_149.csv\n",
            "160) filename: chorale_159.csv\n",
            "170) filename: chorale_169.csv\n",
            "180) filename: chorale_179.csv\n",
            "190) filename: chorale_189.csv\n",
            "200) filename: chorale_199.csv\n",
            "210) filename: chorale_209.csv\n",
            "220) filename: chorale_219.csv\n",
            "\n",
            "Reading chorales in: /content/drive/My Drive/jsb_chorales/valid\n",
            "---------------\n",
            "\n",
            "10) filename: chorale_238.csv\n",
            "20) filename: chorale_248.csv\n",
            "30) filename: chorale_258.csv\n",
            "40) filename: chorale_268.csv\n",
            "50) filename: chorale_278.csv\n",
            "60) filename: chorale_288.csv\n",
            "70) filename: chorale_298.csv\n",
            "\n",
            "Reading chorales in: /content/drive/My Drive/jsb_chorales/test\n",
            "---------------\n",
            "\n",
            "10) filename: chorale_314.csv\n",
            "20) filename: chorale_324.csv\n",
            "30) filename: chorale_334.csv\n",
            "40) filename: chorale_344.csv\n",
            "50) filename: chorale_354.csv\n",
            "60) filename: chorale_364.csv\n",
            "70) filename: chorale_374.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWz0K73P4zZS",
        "colab_type": "text"
      },
      "source": [
        "Sanity check for what the data looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5EoudnRuNDg",
        "colab_type": "code",
        "outputId": "7aa043b2-c842-44f5-ae3c-a95b996f1bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "chorale_counter = 0\n",
        "for chorale in bach_training_chorales:\n",
        "  chorale_counter += 1\n",
        "  if chorale_counter > 3: break\n",
        "\n",
        "  print(\"Chorale #\" + str(chorale_counter))\n",
        "  print(\"Shape: \" + str(chorale.shape))\n",
        "  print(chorale[:6])\n",
        "  print(\" ...\\n\")\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chorale #1\n",
            "Shape: (192, 4)\n",
            "[[74 70 65 58]\n",
            " [74 70 65 58]\n",
            " [74 70 65 58]\n",
            " [74 70 65 58]\n",
            " [75 70 58 55]\n",
            " [75 70 58 55]]\n",
            " ...\n",
            "\n",
            "Chorale #2\n",
            "Shape: (228, 4)\n",
            "[[69 64 61 57]\n",
            " [69 64 61 57]\n",
            " [69 64 61 57]\n",
            " [69 64 61 57]\n",
            " [71 64 59 56]\n",
            " [71 64 59 56]]\n",
            " ...\n",
            "\n",
            "Chorale #3\n",
            "Shape: (208, 4)\n",
            "[[67 62 59 55]\n",
            " [67 62 59 55]\n",
            " [67 62 59 55]\n",
            " [67 62 59 55]\n",
            " [67 64 60 48]\n",
            " [67 64 60 48]]\n",
            " ...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK4vwLm16elV",
        "colab_type": "text"
      },
      "source": [
        "> Ch 15 Q 10\n",
        ">\n",
        "> Train a model--recurrent, convolutional, or both--that can predict the next time step (four notes), given a sequence of time steps from a chorale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUZ4psO26yMl",
        "colab_type": "text"
      },
      "source": [
        "### Approaches\n",
        "\n",
        "Hmmm, three approaches seem reasonable to this problem:\n",
        "\n",
        "1) We could treat this as a regression problem where we try to predict the next note value along the real line. In this case, Mean Squared Error (MSE) seems like the most reasonable metric to use. (Note that MSE is the cross-entropy when we assume that there is gaussian noise added to each output value.)\n",
        "\n",
        "2) But in \"music space\" neighboring notes can actually be further apart than notes from the same chord or key, it may make more sense to treat each note as a separate class and measure performance as the cross-entropy of the \"multi-noulli\" distribution. \n",
        "\n",
        "3) We could design a custom loss that accounts for our knowledge of music theory. A loss that directly accounts for deviations from the chord or key.\n",
        "\n",
        "A custom loss would take a lot of time to implement. Plus, our model may well learn most of the music theory simply by observing the note combinations in the chorales. So let's rule out option 3.\n",
        "\n",
        "Since the notes are represented as numbers, regression looks like the obvious choice, but with my knowledge of music I am inclined toward option 2. I will start with option 2 and revisit option 1 if I have time.\n",
        "\n",
        "### The multiple outputs\n",
        "\n",
        "Another wrinkle in this task is that there are 4 outputs (4 notes) at each time step. With enough training data, we might be able to treat each separate combination of 4 notes as a separate class, but for this exercise my loss will just be the sum of the losses across the 4 notes. \n",
        "\n",
        "(Another idea would be to train a GAN style loss function that rates each 4 note combination on it's likelihood. We could create training data of random combinations of notes and train a classifier to distinguish them from real note combinations. But again, too complicated for this first stab.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BGOvX1u5_NN",
        "colab_type": "text"
      },
      "source": [
        "First, let's gather some baseline metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHRdQnPnBXzX",
        "colab_type": "text"
      },
      "source": [
        "### Multinouli MLE Loss\n",
        "\n",
        "So, our first model will be a classifier with average MLE (aka cross-entropy) loss across the 4 notes.\n",
        "\n",
        "But FIRST, I need to build a model with **1** input and **1** output for the sake of sanity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2QdLbpR18u-",
        "colab_type": "code",
        "outputId": "796087a5-87eb-4aed-ca75-a08a7f12c4e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "def split_off_bass(chorales):\n",
        "\n",
        "  bach_chorales_minus_last_notes = []\n",
        "  last_notes = []\n",
        "  next_to_last_notes = []\n",
        "  bach_chorales_bass_last_notes = []\n",
        "  bass_next_to_last = []\n",
        "  for chorale in chorales:\n",
        "    bach_chorales_minus_last_notes.append(chorale[:-1])\n",
        "    if len(chorale)>0: \n",
        "      last_notes.append(chorale[-1])\n",
        "    if len(next_to_last_notes)>1:\n",
        "      next_to_last_notes.append(chorale[-2])\n",
        "    if len(chorale)>0:\n",
        "      bach_chorales_bass_last_notes.append(chorale[1:])\n",
        "    if len(chorale)>1:\n",
        "      bass_next_to_last.append([chorale[-2, 0]])\n",
        "\n",
        "  return np.array(bach_chorales_minus_last_notes), np.array(last_notes), np.array(next_to_last_notes), np.array(bach_chorales_bass_last_notes), np.array(bass_next_to_last)\n",
        "\n",
        "training_minus_last_notes, training_last_notes, training_next_to_last_notes, bach_training_chorales_bass_last_notes, training_bass_next_to_last = split_off_bass(bach_training_chorales)\n",
        "validation_minus_last_notes, validation_last_notes, validation_next_to_last_notes, bach_validation_chorales_bass_last_notes, validation_bass_next_to_last = split_off_bass(bach_validation_chorales)\n",
        "test_minus_last_notes, test_last_notes, test_next_to_last_notes, bach_test_chorales_bass_last_notes, test_bass_next_to_last = split_off_bass(bach_test_chorales)\n",
        "\n",
        "print(\"len(bach_training_choraels[0]): \", len(bach_training_chorales[0]))\n",
        "print(\"len(training_minus_last_notes[0]): \", len(training_minus_last_notes[0]))\n",
        "print(\"bach_training_chorales[0][-1]: \", bach_training_chorales[0][-1])\n",
        "print(\"training_minus_last_notes[0][-1]: \", training_minus_last_notes[0][-1])\n",
        "\n",
        "print(\"training_last_notes.shape: \", training_last_notes.shape)\n",
        "print(\"training_last_notes[0]: \", training_last_notes[0])\n",
        "print(\"bach_training_chorales_bass_last_notes.shape: \", bach_training_chorales_bass_last_notes.shape)\n",
        "print(\"bach_training_chorales_bass_last_notes[0]: \", bach_training_chorales_bass_last_notes[0][0:10])\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(bach_training_choraels[0]):  192\n",
            "len(training_minus_last_notes[0]):  191\n",
            "bach_training_chorales[0][-1]:  [70 65 62 46]\n",
            "training_minus_last_notes[0][-1]:  [70 65 62 46]\n",
            "training_last_notes.shape:  (229, 4)\n",
            "training_last_notes[0]:  [70 65 62 46]\n",
            "bach_training_chorales_bass_last_notes.shape:  (229,)\n",
            "bach_training_chorales_bass_last_notes[0]:  [[74 70 65 58]\n",
            " [74 70 65 58]\n",
            " [74 70 65 58]\n",
            " [75 70 58 55]\n",
            " [75 70 58 55]\n",
            " [75 70 60 55]\n",
            " [75 70 60 55]\n",
            " [77 69 62 50]\n",
            " [77 69 62 50]\n",
            " [77 69 62 50]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZVwyta2JYA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_minus_last_notes_padded = tf.keras.preprocessing.sequence.pad_sequences(training_minus_last_notes, padding='post', maxlen=576)\n",
        "validation_minus_last_notes_padded =  tf.keras.preprocessing.sequence.pad_sequences(validation_minus_last_notes, padding='post', maxlen=576)\n",
        "bach_training_chorales_bass_last_notes_padded =  tf.keras.preprocessing.sequence.pad_sequences(bach_training_chorales_bass_last_notes, padding='post', maxlen=576)\n",
        "bach_validation_chorales_bass_last_notes_padded =  tf.keras.preprocessing.sequence.pad_sequences(bach_validation_chorales_bass_last_notes, padding='post', maxlen=576)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58eJSjyo_uy8",
        "colab_type": "code",
        "outputId": "d38f61ae-c0b0-4248-aac8-1af5b611562c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bach_training_chorales_bass_last_notes_padded.shape, training_minus_last_notes_padded.shape"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((229, 576, 4), (229, 576, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRtV0flyojTX",
        "colab_type": "code",
        "outputId": "912e59ae-4f55-4034-c0e1-ee342efe2629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bach_validation_chorales_bass_last_notes.shape, validation_minus_last_notes_padded.shape"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((76,), (76, 576, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYEJrJCjXvXo",
        "colab_type": "code",
        "outputId": "9a540d5b-ca1a-4a76-8905-3f94efa297b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print(training_minus_last_notes_padded[0:2])"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[74 70 65 58]\n",
            "  [74 70 65 58]\n",
            "  [74 70 65 58]\n",
            "  ...\n",
            "  [ 0  0  0  0]\n",
            "  [ 0  0  0  0]\n",
            "  [ 0  0  0  0]]\n",
            "\n",
            " [[69 64 61 57]\n",
            "  [69 64 61 57]\n",
            "  [69 64 61 57]\n",
            "  ...\n",
            "  [ 0  0  0  0]\n",
            "  [ 0  0  0  0]\n",
            "  [ 0  0  0  0]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce029nFkbBBS",
        "colab_type": "code",
        "outputId": "f7a461c6-b808-478c-9149-3c9b525b0f8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "\n",
        "n_features = 4\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "\n",
        "model.add(keras.layers.TimeDistributed(keras.layers.Dense(128), input_shape=(None, n_features)) ) # This line makes a lot of difference but why?\n",
        "model.add(keras.layers.LSTM(64, input_shape=(None, n_features), return_sequences=True))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "model.add(keras.layers.LSTM(32, return_sequences=True))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.Dense(4))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# fit model\n",
        "model.fit(training_minus_last_notes_padded, bach_training_chorales_bass_last_notes_padded, epochs=500, validation_data=(validation_minus_last_notes_padded, bach_validation_chorales_bass_last_notes_padded))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 229 samples, validate on 76 samples\n",
            "Epoch 1/500\n",
            "229/229 [==============================] - 3s 14ms/sample - loss: 1515.6284 - val_loss: 1556.4179\n",
            "Epoch 2/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1452.4940 - val_loss: 1542.9843\n",
            "Epoch 3/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1431.0871 - val_loss: 1528.7675\n",
            "Epoch 4/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1412.8712 - val_loss: 1513.9017\n",
            "Epoch 5/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1393.9675 - val_loss: 1498.8438\n",
            "Epoch 6/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1374.5487 - val_loss: 1481.5459\n",
            "Epoch 7/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1356.0047 - val_loss: 1462.7862\n",
            "Epoch 8/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1336.4075 - val_loss: 1442.4424\n",
            "Epoch 9/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1316.4579 - val_loss: 1421.1918\n",
            "Epoch 10/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1295.8091 - val_loss: 1399.0360\n",
            "Epoch 11/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1275.1410 - val_loss: 1375.6556\n",
            "Epoch 12/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1253.8830 - val_loss: 1352.3586\n",
            "Epoch 13/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1232.4159 - val_loss: 1327.9677\n",
            "Epoch 14/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1210.4637 - val_loss: 1303.2246\n",
            "Epoch 15/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1188.6808 - val_loss: 1278.2666\n",
            "Epoch 16/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1166.8716 - val_loss: 1252.1841\n",
            "Epoch 17/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1143.8279 - val_loss: 1226.8106\n",
            "Epoch 18/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1120.9621 - val_loss: 1200.8407\n",
            "Epoch 19/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1097.5228 - val_loss: 1173.8080\n",
            "Epoch 20/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1073.4985 - val_loss: 1146.1739\n",
            "Epoch 21/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1050.7459 - val_loss: 1119.8434\n",
            "Epoch 22/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1026.3150 - val_loss: 1092.2825\n",
            "Epoch 23/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 1002.4861 - val_loss: 1065.5606\n",
            "Epoch 24/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 978.0725 - val_loss: 1038.0091\n",
            "Epoch 25/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 954.1758 - val_loss: 1010.2525\n",
            "Epoch 26/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 929.5328 - val_loss: 981.1372\n",
            "Epoch 27/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 906.2879 - val_loss: 955.5938\n",
            "Epoch 28/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 882.3664 - val_loss: 927.5653\n",
            "Epoch 29/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 858.4431 - val_loss: 902.0624\n",
            "Epoch 30/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 834.2832 - val_loss: 874.4092\n",
            "Epoch 31/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 810.6388 - val_loss: 846.4191\n",
            "Epoch 32/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 786.4444 - val_loss: 820.4249\n",
            "Epoch 33/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 763.1789 - val_loss: 793.8107\n",
            "Epoch 34/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 740.2960 - val_loss: 767.1673\n",
            "Epoch 35/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 718.8089 - val_loss: 742.2443\n",
            "Epoch 36/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 695.1416 - val_loss: 716.5680\n",
            "Epoch 37/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 672.7621 - val_loss: 694.4869\n",
            "Epoch 38/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 650.0924 - val_loss: 664.2529\n",
            "Epoch 39/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 628.1527 - val_loss: 646.1240\n",
            "Epoch 40/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 609.8779 - val_loss: 624.0602\n",
            "Epoch 41/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 588.3874 - val_loss: 600.4900\n",
            "Epoch 42/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 566.6239 - val_loss: 578.0807\n",
            "Epoch 43/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 546.9476 - val_loss: 556.7004\n",
            "Epoch 44/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 527.1401 - val_loss: 537.3386\n",
            "Epoch 45/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 509.6477 - val_loss: 517.5699\n",
            "Epoch 46/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 489.8728 - val_loss: 504.0467\n",
            "Epoch 47/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 471.1639 - val_loss: 483.9268\n",
            "Epoch 48/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 452.2601 - val_loss: 466.8196\n",
            "Epoch 49/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 436.2568 - val_loss: 451.4771\n",
            "Epoch 50/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 417.7514 - val_loss: 434.2925\n",
            "Epoch 51/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 402.5638 - val_loss: 428.3052\n",
            "Epoch 52/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 386.0657 - val_loss: 407.6735\n",
            "Epoch 53/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 370.9195 - val_loss: 435.1046\n",
            "Epoch 54/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 359.0814 - val_loss: 368.3578\n",
            "Epoch 55/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 341.9138 - val_loss: 347.7114\n",
            "Epoch 56/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 329.3005 - val_loss: 351.3764\n",
            "Epoch 57/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 316.0027 - val_loss: 325.9571\n",
            "Epoch 58/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 304.2191 - val_loss: 335.0617\n",
            "Epoch 59/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 289.6649 - val_loss: 313.0211\n",
            "Epoch 60/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 278.7087 - val_loss: 307.7254\n",
            "Epoch 61/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 268.0698 - val_loss: 311.8590\n",
            "Epoch 62/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 256.1018 - val_loss: 300.6352\n",
            "Epoch 63/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 246.3837 - val_loss: 304.2110\n",
            "Epoch 64/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 238.2556 - val_loss: 212.7174\n",
            "Epoch 65/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 228.8554 - val_loss: 212.5160\n",
            "Epoch 66/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 220.0169 - val_loss: 322.9461\n",
            "Epoch 67/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 211.3821 - val_loss: 246.6614\n",
            "Epoch 68/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 202.3396 - val_loss: 219.6881\n",
            "Epoch 69/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 195.5368 - val_loss: 198.7683\n",
            "Epoch 70/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 187.5882 - val_loss: 185.3475\n",
            "Epoch 71/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 180.4117 - val_loss: 154.7151\n",
            "Epoch 72/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 173.4634 - val_loss: 151.8001\n",
            "Epoch 73/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 168.0680 - val_loss: 175.6359\n",
            "Epoch 74/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 159.7744 - val_loss: 166.6252\n",
            "Epoch 75/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 154.5183 - val_loss: 161.4815\n",
            "Epoch 76/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 150.4783 - val_loss: 147.3508\n",
            "Epoch 77/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 143.3815 - val_loss: 146.9993\n",
            "Epoch 78/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 137.4278 - val_loss: 133.5054\n",
            "Epoch 79/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 132.1638 - val_loss: 149.7404\n",
            "Epoch 80/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 127.0043 - val_loss: 140.0482\n",
            "Epoch 81/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 124.7973 - val_loss: 146.1144\n",
            "Epoch 82/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 119.8885 - val_loss: 125.6956\n",
            "Epoch 83/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 115.9869 - val_loss: 98.4838\n",
            "Epoch 84/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 109.2644 - val_loss: 87.8134\n",
            "Epoch 85/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 105.6315 - val_loss: 101.0972\n",
            "Epoch 86/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 103.1371 - val_loss: 81.7615\n",
            "Epoch 87/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 99.9843 - val_loss: 95.4848\n",
            "Epoch 88/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 94.8836 - val_loss: 78.4809\n",
            "Epoch 89/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 91.4567 - val_loss: 75.5903\n",
            "Epoch 90/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 87.2609 - val_loss: 83.0983\n",
            "Epoch 91/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 83.4294 - val_loss: 83.5212\n",
            "Epoch 92/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 81.2805 - val_loss: 70.9212\n",
            "Epoch 93/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 79.2905 - val_loss: 94.4230\n",
            "Epoch 94/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 75.2205 - val_loss: 66.7777\n",
            "Epoch 95/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 73.6512 - val_loss: 74.4771\n",
            "Epoch 96/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 70.4067 - val_loss: 71.3870\n",
            "Epoch 97/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 67.7226 - val_loss: 67.5655\n",
            "Epoch 98/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 65.7342 - val_loss: 56.8002\n",
            "Epoch 99/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 61.2959 - val_loss: 67.1410\n",
            "Epoch 100/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 61.4596 - val_loss: 60.4035\n",
            "Epoch 101/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 59.8737 - val_loss: 50.9800\n",
            "Epoch 102/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 56.2188 - val_loss: 59.3404\n",
            "Epoch 103/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 56.4889 - val_loss: 59.3846\n",
            "Epoch 104/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 51.9864 - val_loss: 60.2033\n",
            "Epoch 105/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 55.0155 - val_loss: 47.0169\n",
            "Epoch 106/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 48.4460 - val_loss: 50.7750\n",
            "Epoch 107/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 46.0111 - val_loss: 44.9038\n",
            "Epoch 108/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 48.7911 - val_loss: 42.6551\n",
            "Epoch 109/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 45.4035 - val_loss: 34.1094\n",
            "Epoch 110/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 43.1548 - val_loss: 43.4036\n",
            "Epoch 111/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 44.3553 - val_loss: 32.9107\n",
            "Epoch 112/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 39.5867 - val_loss: 69.7384\n",
            "Epoch 113/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 38.5452 - val_loss: 51.6671\n",
            "Epoch 114/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 37.3949 - val_loss: 27.9809\n",
            "Epoch 115/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 36.5679 - val_loss: 31.3086\n",
            "Epoch 116/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 40.2641 - val_loss: 53.4474\n",
            "Epoch 117/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 34.9052 - val_loss: 32.8245\n",
            "Epoch 118/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 33.2855 - val_loss: 23.2628\n",
            "Epoch 119/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 35.4618 - val_loss: 39.2574\n",
            "Epoch 120/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 33.8291 - val_loss: 83.3326\n",
            "Epoch 121/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 34.7846 - val_loss: 34.6578\n",
            "Epoch 122/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 30.1415 - val_loss: 18.3050\n",
            "Epoch 123/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 30.2745 - val_loss: 17.5314\n",
            "Epoch 124/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 28.2729 - val_loss: 27.8973\n",
            "Epoch 125/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 29.6119 - val_loss: 24.6314\n",
            "Epoch 126/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 28.1107 - val_loss: 21.7904\n",
            "Epoch 127/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 27.9320 - val_loss: 17.3131\n",
            "Epoch 128/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 29.4517 - val_loss: 26.0162\n",
            "Epoch 129/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 27.4913 - val_loss: 20.3661\n",
            "Epoch 130/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 25.1781 - val_loss: 16.2384\n",
            "Epoch 131/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 25.6217 - val_loss: 18.9793\n",
            "Epoch 132/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 25.0711 - val_loss: 22.8596\n",
            "Epoch 133/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 24.4424 - val_loss: 21.6353\n",
            "Epoch 134/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 23.0774 - val_loss: 26.1509\n",
            "Epoch 135/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 23.5056 - val_loss: 19.7446\n",
            "Epoch 136/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 24.2856 - val_loss: 18.7039\n",
            "Epoch 137/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 24.1672 - val_loss: 14.1382\n",
            "Epoch 138/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 25.5651 - val_loss: 18.7273\n",
            "Epoch 139/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 24.9882 - val_loss: 17.0733\n",
            "Epoch 140/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 22.3108 - val_loss: 10.8185\n",
            "Epoch 141/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 22.3975 - val_loss: 31.1732\n",
            "Epoch 142/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 23.3240 - val_loss: 12.9237\n",
            "Epoch 143/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 22.1314 - val_loss: 21.0565\n",
            "Epoch 144/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 22.2225 - val_loss: 17.2534\n",
            "Epoch 145/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.6661 - val_loss: 14.0892\n",
            "Epoch 146/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.2549 - val_loss: 8.9317\n",
            "Epoch 147/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 22.9964 - val_loss: 17.0508\n",
            "Epoch 148/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.5153 - val_loss: 16.4918\n",
            "Epoch 149/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.5676 - val_loss: 10.1155\n",
            "Epoch 150/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 22.4215 - val_loss: 8.9071\n",
            "Epoch 151/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.4217 - val_loss: 13.5145\n",
            "Epoch 152/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.8944 - val_loss: 17.2368\n",
            "Epoch 153/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.4784 - val_loss: 11.1421\n",
            "Epoch 154/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.9308 - val_loss: 8.6654\n",
            "Epoch 155/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.5876 - val_loss: 20.5946\n",
            "Epoch 156/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 22.5379 - val_loss: 15.9517\n",
            "Epoch 157/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.9976 - val_loss: 7.6879\n",
            "Epoch 158/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 22.2120 - val_loss: 9.4216\n",
            "Epoch 159/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.8210 - val_loss: 23.2995\n",
            "Epoch 160/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.9497 - val_loss: 29.7936\n",
            "Epoch 161/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.1765 - val_loss: 16.4221\n",
            "Epoch 162/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.1897 - val_loss: 8.1689\n",
            "Epoch 163/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.2910 - val_loss: 7.1663\n",
            "Epoch 164/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.0242 - val_loss: 7.3472\n",
            "Epoch 165/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.9217 - val_loss: 9.3659\n",
            "Epoch 166/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.9317 - val_loss: 11.6534\n",
            "Epoch 167/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.4439 - val_loss: 16.3041\n",
            "Epoch 168/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 22.5445 - val_loss: 10.3170\n",
            "Epoch 169/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.8669 - val_loss: 12.0990\n",
            "Epoch 170/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.7661 - val_loss: 9.6171\n",
            "Epoch 171/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.8738 - val_loss: 16.2821\n",
            "Epoch 172/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.9309 - val_loss: 7.2235\n",
            "Epoch 173/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.7647 - val_loss: 6.8822\n",
            "Epoch 174/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.8398 - val_loss: 28.3469\n",
            "Epoch 175/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.2119 - val_loss: 10.5513\n",
            "Epoch 176/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.3228 - val_loss: 6.8149\n",
            "Epoch 177/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.1702 - val_loss: 7.1989\n",
            "Epoch 178/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 22.2045 - val_loss: 11.4576\n",
            "Epoch 179/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.5409 - val_loss: 9.0007\n",
            "Epoch 180/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.5257 - val_loss: 12.2267\n",
            "Epoch 181/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.3064 - val_loss: 7.9201\n",
            "Epoch 182/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.4719 - val_loss: 9.0409\n",
            "Epoch 183/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.5432 - val_loss: 9.7918\n",
            "Epoch 184/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.4637 - val_loss: 7.3247\n",
            "Epoch 185/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.5758 - val_loss: 6.6902\n",
            "Epoch 186/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.7431 - val_loss: 7.3092\n",
            "Epoch 187/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.7353 - val_loss: 7.3596\n",
            "Epoch 188/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.1617 - val_loss: 13.1107\n",
            "Epoch 189/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.5951 - val_loss: 9.1835\n",
            "Epoch 190/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.5578 - val_loss: 12.0202\n",
            "Epoch 191/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.1676 - val_loss: 9.5834\n",
            "Epoch 192/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.3153 - val_loss: 7.6230\n",
            "Epoch 193/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.7332 - val_loss: 7.7203\n",
            "Epoch 194/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.4572 - val_loss: 7.5577\n",
            "Epoch 195/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.1014 - val_loss: 18.2937\n",
            "Epoch 196/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.1171 - val_loss: 7.4608\n",
            "Epoch 197/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.5565 - val_loss: 6.7059\n",
            "Epoch 198/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.3050 - val_loss: 7.5742\n",
            "Epoch 199/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.3214 - val_loss: 7.7097\n",
            "Epoch 200/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.4566 - val_loss: 8.7701\n",
            "Epoch 201/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.5301 - val_loss: 6.4900\n",
            "Epoch 202/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.7111 - val_loss: 6.3106\n",
            "Epoch 203/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.5179 - val_loss: 6.4063\n",
            "Epoch 204/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.8886 - val_loss: 8.7119\n",
            "Epoch 205/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.6894 - val_loss: 14.4373\n",
            "Epoch 206/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.5500 - val_loss: 14.8346\n",
            "Epoch 207/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.4052 - val_loss: 10.3908\n",
            "Epoch 208/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.0012 - val_loss: 8.8967\n",
            "Epoch 209/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.0851 - val_loss: 7.2996\n",
            "Epoch 210/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.3345 - val_loss: 6.9139\n",
            "Epoch 211/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.1408 - val_loss: 6.4229\n",
            "Epoch 212/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.2495 - val_loss: 6.6848\n",
            "Epoch 213/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.6395 - val_loss: 7.8021\n",
            "Epoch 214/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.4550 - val_loss: 6.5673\n",
            "Epoch 215/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.0889 - val_loss: 6.1375\n",
            "Epoch 216/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.7153 - val_loss: 6.3917\n",
            "Epoch 217/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.9078 - val_loss: 8.3706\n",
            "Epoch 218/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.4731 - val_loss: 17.2615\n",
            "Epoch 219/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.1678 - val_loss: 9.7820\n",
            "Epoch 220/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.5561 - val_loss: 6.0985\n",
            "Epoch 221/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.3335 - val_loss: 6.0595\n",
            "Epoch 222/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.9622 - val_loss: 8.6562\n",
            "Epoch 223/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.6933 - val_loss: 9.9718\n",
            "Epoch 224/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.5554 - val_loss: 8.7235\n",
            "Epoch 225/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.7361 - val_loss: 10.9780\n",
            "Epoch 226/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.9932 - val_loss: 24.0256\n",
            "Epoch 227/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.1888 - val_loss: 10.0077\n",
            "Epoch 228/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.7512 - val_loss: 7.3669\n",
            "Epoch 229/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.5742 - val_loss: 8.2799\n",
            "Epoch 230/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.7430 - val_loss: 9.6423\n",
            "Epoch 231/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.2231 - val_loss: 6.4228\n",
            "Epoch 232/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.4411 - val_loss: 6.0232\n",
            "Epoch 233/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.6162 - val_loss: 6.1951\n",
            "Epoch 234/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.2890 - val_loss: 5.9165\n",
            "Epoch 235/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.5871 - val_loss: 6.6963\n",
            "Epoch 236/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.7604 - val_loss: 7.8049\n",
            "Epoch 237/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.0828 - val_loss: 5.9465\n",
            "Epoch 238/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.6068 - val_loss: 6.4200\n",
            "Epoch 239/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.1972 - val_loss: 6.4729\n",
            "Epoch 240/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.9773 - val_loss: 7.3059\n",
            "Epoch 241/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.6590 - val_loss: 6.4681\n",
            "Epoch 242/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.0733 - val_loss: 6.2387\n",
            "Epoch 243/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 22.4020 - val_loss: 6.2200\n",
            "Epoch 244/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.0566 - val_loss: 8.1905\n",
            "Epoch 245/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.4308 - val_loss: 7.0258\n",
            "Epoch 246/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.1094 - val_loss: 6.1651\n",
            "Epoch 247/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.5844 - val_loss: 6.0736\n",
            "Epoch 248/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.3609 - val_loss: 6.2027\n",
            "Epoch 249/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.5795 - val_loss: 6.8404\n",
            "Epoch 250/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.7555 - val_loss: 5.8739\n",
            "Epoch 251/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.4894 - val_loss: 11.0149\n",
            "Epoch 252/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.9810 - val_loss: 6.0665\n",
            "Epoch 253/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.0463 - val_loss: 9.2832\n",
            "Epoch 254/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.0022 - val_loss: 7.4964\n",
            "Epoch 255/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.8114 - val_loss: 6.7305\n",
            "Epoch 256/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.4116 - val_loss: 6.6082\n",
            "Epoch 257/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.5794 - val_loss: 6.3640\n",
            "Epoch 258/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.3778 - val_loss: 7.9472\n",
            "Epoch 259/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.3114 - val_loss: 12.0884\n",
            "Epoch 260/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.4111 - val_loss: 6.2836\n",
            "Epoch 261/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.1290 - val_loss: 10.7225\n",
            "Epoch 262/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.3305 - val_loss: 12.4820\n",
            "Epoch 263/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.8193 - val_loss: 6.4297\n",
            "Epoch 264/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.4581 - val_loss: 5.8323\n",
            "Epoch 265/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.3855 - val_loss: 6.1481\n",
            "Epoch 266/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.1832 - val_loss: 5.8424\n",
            "Epoch 267/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.7601 - val_loss: 7.6651\n",
            "Epoch 268/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.9126 - val_loss: 10.4283\n",
            "Epoch 269/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.8724 - val_loss: 10.1168\n",
            "Epoch 270/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.7218 - val_loss: 5.7479\n",
            "Epoch 271/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.3420 - val_loss: 10.5824\n",
            "Epoch 272/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.1826 - val_loss: 19.6500\n",
            "Epoch 273/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.1750 - val_loss: 8.7547\n",
            "Epoch 274/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.1164 - val_loss: 8.3432\n",
            "Epoch 275/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.2819 - val_loss: 5.7646\n",
            "Epoch 276/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.3562 - val_loss: 5.9013\n",
            "Epoch 277/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.8988 - val_loss: 8.0653\n",
            "Epoch 278/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.0254 - val_loss: 5.8657\n",
            "Epoch 279/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.6908 - val_loss: 5.8408\n",
            "Epoch 280/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.7494 - val_loss: 6.4983\n",
            "Epoch 281/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.3845 - val_loss: 6.9346\n",
            "Epoch 282/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.2308 - val_loss: 6.0437\n",
            "Epoch 283/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.7882 - val_loss: 6.1147\n",
            "Epoch 284/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.6534 - val_loss: 5.8464\n",
            "Epoch 285/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.1567 - val_loss: 5.9505\n",
            "Epoch 286/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.9983 - val_loss: 6.8844\n",
            "Epoch 287/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.3608 - val_loss: 7.2004\n",
            "Epoch 288/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.2843 - val_loss: 7.5900\n",
            "Epoch 289/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.6439 - val_loss: 7.9333\n",
            "Epoch 290/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.3545 - val_loss: 7.1028\n",
            "Epoch 291/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.7433 - val_loss: 6.7767\n",
            "Epoch 292/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.8053 - val_loss: 7.5848\n",
            "Epoch 293/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.9066 - val_loss: 6.1815\n",
            "Epoch 294/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.6530 - val_loss: 6.2175\n",
            "Epoch 295/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.3906 - val_loss: 9.8414\n",
            "Epoch 296/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.3878 - val_loss: 6.3156\n",
            "Epoch 297/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.2193 - val_loss: 8.5052\n",
            "Epoch 298/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.5688 - val_loss: 6.8349\n",
            "Epoch 299/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.2671 - val_loss: 7.1462\n",
            "Epoch 300/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.5629 - val_loss: 5.9262\n",
            "Epoch 301/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.6014 - val_loss: 9.5902\n",
            "Epoch 302/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.7174 - val_loss: 9.5652\n",
            "Epoch 303/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.8794 - val_loss: 8.0184\n",
            "Epoch 304/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.1142 - val_loss: 7.9109\n",
            "Epoch 305/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.4613 - val_loss: 7.6156\n",
            "Epoch 306/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.7013 - val_loss: 6.8885\n",
            "Epoch 307/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.5752 - val_loss: 5.7128\n",
            "Epoch 308/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.2957 - val_loss: 6.2701\n",
            "Epoch 309/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.0251 - val_loss: 6.9439\n",
            "Epoch 310/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.4520 - val_loss: 6.8076\n",
            "Epoch 311/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.6945 - val_loss: 6.1279\n",
            "Epoch 312/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.3061 - val_loss: 5.8191\n",
            "Epoch 313/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.3325 - val_loss: 10.3041\n",
            "Epoch 314/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.9080 - val_loss: 6.4300\n",
            "Epoch 315/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.5145 - val_loss: 9.8888\n",
            "Epoch 316/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.0877 - val_loss: 21.0586\n",
            "Epoch 317/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.9365 - val_loss: 7.2361\n",
            "Epoch 318/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.6733 - val_loss: 9.1535\n",
            "Epoch 319/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.8031 - val_loss: 7.5143\n",
            "Epoch 320/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.2846 - val_loss: 9.6035\n",
            "Epoch 321/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.5171 - val_loss: 10.0330\n",
            "Epoch 322/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.6203 - val_loss: 8.4802\n",
            "Epoch 323/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.4539 - val_loss: 7.5071\n",
            "Epoch 324/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.3342 - val_loss: 5.7478\n",
            "Epoch 325/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.3697 - val_loss: 8.0444\n",
            "Epoch 326/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.5208 - val_loss: 6.8407\n",
            "Epoch 327/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.4379 - val_loss: 9.0921\n",
            "Epoch 328/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.7128 - val_loss: 9.1350\n",
            "Epoch 329/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.4510 - val_loss: 5.6506\n",
            "Epoch 330/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.3038 - val_loss: 5.8544\n",
            "Epoch 331/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.9776 - val_loss: 5.8170\n",
            "Epoch 332/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.5271 - val_loss: 5.6504\n",
            "Epoch 333/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.2599 - val_loss: 7.0994\n",
            "Epoch 334/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.1650 - val_loss: 6.6333\n",
            "Epoch 335/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.8156 - val_loss: 7.0659\n",
            "Epoch 336/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.0484 - val_loss: 6.0834\n",
            "Epoch 337/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.9099 - val_loss: 7.2354\n",
            "Epoch 338/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.1341 - val_loss: 5.8916\n",
            "Epoch 339/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.2992 - val_loss: 6.2571\n",
            "Epoch 340/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.3129 - val_loss: 9.8333\n",
            "Epoch 341/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.9615 - val_loss: 6.6272\n",
            "Epoch 342/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.7121 - val_loss: 5.7354\n",
            "Epoch 343/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.2050 - val_loss: 6.1761\n",
            "Epoch 344/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.2825 - val_loss: 6.2551\n",
            "Epoch 345/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.2102 - val_loss: 10.1013\n",
            "Epoch 346/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.4188 - val_loss: 9.4052\n",
            "Epoch 347/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.7041 - val_loss: 7.2218\n",
            "Epoch 348/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.7349 - val_loss: 5.6952\n",
            "Epoch 349/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.1612 - val_loss: 5.8388\n",
            "Epoch 350/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.4505 - val_loss: 6.1990\n",
            "Epoch 351/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.2063 - val_loss: 6.2979\n",
            "Epoch 352/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.6140 - val_loss: 5.6292\n",
            "Epoch 353/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.4955 - val_loss: 6.5762\n",
            "Epoch 354/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.4761 - val_loss: 8.2171\n",
            "Epoch 355/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.4462 - val_loss: 6.3519\n",
            "Epoch 356/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.2662 - val_loss: 6.0216\n",
            "Epoch 357/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.8439 - val_loss: 6.1159\n",
            "Epoch 358/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.5608 - val_loss: 6.7970\n",
            "Epoch 359/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.9728 - val_loss: 6.8018\n",
            "Epoch 360/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.3334 - val_loss: 5.8937\n",
            "Epoch 361/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.4788 - val_loss: 5.7262\n",
            "Epoch 362/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.6498 - val_loss: 6.2540\n",
            "Epoch 363/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.6763 - val_loss: 6.7261\n",
            "Epoch 364/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.7771 - val_loss: 7.5583\n",
            "Epoch 365/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.5473 - val_loss: 8.1908\n",
            "Epoch 366/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.7541 - val_loss: 6.0723\n",
            "Epoch 367/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.8983 - val_loss: 6.7355\n",
            "Epoch 368/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.4975 - val_loss: 7.3313\n",
            "Epoch 369/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.5568 - val_loss: 7.0747\n",
            "Epoch 370/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.3834 - val_loss: 5.8821\n",
            "Epoch 371/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.2194 - val_loss: 5.8876\n",
            "Epoch 372/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.7360 - val_loss: 6.3379\n",
            "Epoch 373/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.5183 - val_loss: 7.1672\n",
            "Epoch 374/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.1162 - val_loss: 5.7838\n",
            "Epoch 375/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.8809 - val_loss: 5.8658\n",
            "Epoch 376/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.6503 - val_loss: 7.4834\n",
            "Epoch 377/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.7715 - val_loss: 13.8568\n",
            "Epoch 378/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.3456 - val_loss: 5.8997\n",
            "Epoch 379/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.8708 - val_loss: 6.3527\n",
            "Epoch 380/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.3016 - val_loss: 5.8507\n",
            "Epoch 381/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.9861 - val_loss: 9.4212\n",
            "Epoch 382/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.2260 - val_loss: 10.2811\n",
            "Epoch 383/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.3152 - val_loss: 8.7845\n",
            "Epoch 384/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.7906 - val_loss: 7.5437\n",
            "Epoch 385/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.4220 - val_loss: 9.0527\n",
            "Epoch 386/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.4384 - val_loss: 13.6544\n",
            "Epoch 387/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.9538 - val_loss: 10.9014\n",
            "Epoch 388/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.0875 - val_loss: 7.5933\n",
            "Epoch 389/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.6355 - val_loss: 8.1281\n",
            "Epoch 390/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.7576 - val_loss: 6.0434\n",
            "Epoch 391/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.1782 - val_loss: 6.8458\n",
            "Epoch 392/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.6287 - val_loss: 9.0686\n",
            "Epoch 393/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.9648 - val_loss: 6.0280\n",
            "Epoch 394/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.5365 - val_loss: 5.7899\n",
            "Epoch 395/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.4889 - val_loss: 9.5351\n",
            "Epoch 396/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.2293 - val_loss: 7.7226\n",
            "Epoch 397/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.7868 - val_loss: 8.9896\n",
            "Epoch 398/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.4074 - val_loss: 6.1053\n",
            "Epoch 399/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.7475 - val_loss: 5.8679\n",
            "Epoch 400/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.7971 - val_loss: 6.2252\n",
            "Epoch 401/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.6322 - val_loss: 10.8654\n",
            "Epoch 402/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.2126 - val_loss: 5.8327\n",
            "Epoch 403/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.1530 - val_loss: 5.7572\n",
            "Epoch 404/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.7216 - val_loss: 5.6693\n",
            "Epoch 405/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.4663 - val_loss: 5.9136\n",
            "Epoch 406/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.3510 - val_loss: 5.8587\n",
            "Epoch 407/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.9425 - val_loss: 6.2586\n",
            "Epoch 408/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.2587 - val_loss: 6.0188\n",
            "Epoch 409/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.9639 - val_loss: 5.7023\n",
            "Epoch 410/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.3473 - val_loss: 6.2482\n",
            "Epoch 411/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.9756 - val_loss: 5.6884\n",
            "Epoch 412/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.5743 - val_loss: 7.0004\n",
            "Epoch 413/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.6648 - val_loss: 5.8513\n",
            "Epoch 414/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.1027 - val_loss: 9.7747\n",
            "Epoch 415/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.8863 - val_loss: 7.8887\n",
            "Epoch 416/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.1208 - val_loss: 6.0871\n",
            "Epoch 417/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.0587 - val_loss: 5.6851\n",
            "Epoch 418/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.6007 - val_loss: 6.2050\n",
            "Epoch 419/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.2858 - val_loss: 9.3412\n",
            "Epoch 420/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.9893 - val_loss: 12.4280\n",
            "Epoch 421/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.6307 - val_loss: 5.9461\n",
            "Epoch 422/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.1301 - val_loss: 6.5050\n",
            "Epoch 423/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.3048 - val_loss: 7.8134\n",
            "Epoch 424/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.6466 - val_loss: 6.9696\n",
            "Epoch 425/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.5659 - val_loss: 8.1935\n",
            "Epoch 426/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.5022 - val_loss: 11.2260\n",
            "Epoch 427/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.8060 - val_loss: 6.0216\n",
            "Epoch 428/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.4491 - val_loss: 5.7024\n",
            "Epoch 429/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.4629 - val_loss: 5.5931\n",
            "Epoch 430/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.6754 - val_loss: 6.9335\n",
            "Epoch 431/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.5670 - val_loss: 6.3146\n",
            "Epoch 432/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.3904 - val_loss: 6.2150\n",
            "Epoch 433/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.0813 - val_loss: 6.7117\n",
            "Epoch 434/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.0441 - val_loss: 5.6734\n",
            "Epoch 435/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.0210 - val_loss: 5.7971\n",
            "Epoch 436/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.8206 - val_loss: 5.8509\n",
            "Epoch 437/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.9865 - val_loss: 5.9611\n",
            "Epoch 438/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.1059 - val_loss: 7.0306\n",
            "Epoch 439/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.9749 - val_loss: 6.9933\n",
            "Epoch 440/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.2860 - val_loss: 11.7868\n",
            "Epoch 441/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.8149 - val_loss: 7.3275\n",
            "Epoch 442/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.0973 - val_loss: 6.0482\n",
            "Epoch 443/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.6336 - val_loss: 5.8977\n",
            "Epoch 444/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.0337 - val_loss: 5.9369\n",
            "Epoch 445/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.6060 - val_loss: 6.5893\n",
            "Epoch 446/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 22.5923 - val_loss: 10.4445\n",
            "Epoch 447/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.3673 - val_loss: 6.1333\n",
            "Epoch 448/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.6800 - val_loss: 5.9919\n",
            "Epoch 449/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.7324 - val_loss: 10.5038\n",
            "Epoch 450/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.5032 - val_loss: 8.6786\n",
            "Epoch 451/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.6469 - val_loss: 5.9765\n",
            "Epoch 452/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.7349 - val_loss: 7.4664\n",
            "Epoch 453/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.0541 - val_loss: 7.5624\n",
            "Epoch 454/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.9355 - val_loss: 6.2390\n",
            "Epoch 455/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.8762 - val_loss: 5.9309\n",
            "Epoch 456/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.5426 - val_loss: 7.0870\n",
            "Epoch 457/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.3342 - val_loss: 6.0328\n",
            "Epoch 458/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.3882 - val_loss: 6.5233\n",
            "Epoch 459/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.9364 - val_loss: 7.6479\n",
            "Epoch 460/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.5473 - val_loss: 7.2500\n",
            "Epoch 461/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.0162 - val_loss: 6.2641\n",
            "Epoch 462/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.5005 - val_loss: 7.0604\n",
            "Epoch 463/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.4240 - val_loss: 6.1174\n",
            "Epoch 464/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.3020 - val_loss: 8.3340\n",
            "Epoch 465/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.4419 - val_loss: 11.0798\n",
            "Epoch 466/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.3901 - val_loss: 13.7401\n",
            "Epoch 467/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.9409 - val_loss: 16.5027\n",
            "Epoch 468/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.7260 - val_loss: 8.0285\n",
            "Epoch 469/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.6848 - val_loss: 11.3326\n",
            "Epoch 470/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.7527 - val_loss: 9.6979\n",
            "Epoch 471/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.7588 - val_loss: 6.9644\n",
            "Epoch 472/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.4898 - val_loss: 12.8741\n",
            "Epoch 473/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.6479 - val_loss: 6.0121\n",
            "Epoch 474/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 20.9607 - val_loss: 6.2418\n",
            "Epoch 475/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.9662 - val_loss: 6.4917\n",
            "Epoch 476/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.4120 - val_loss: 5.7144\n",
            "Epoch 477/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.9362 - val_loss: 5.9696\n",
            "Epoch 478/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.7015 - val_loss: 6.1673\n",
            "Epoch 479/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.0121 - val_loss: 5.7933\n",
            "Epoch 480/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.5026 - val_loss: 5.6479\n",
            "Epoch 481/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.4503 - val_loss: 5.6882\n",
            "Epoch 482/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.7611 - val_loss: 6.2235\n",
            "Epoch 483/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.5413 - val_loss: 9.7032\n",
            "Epoch 484/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.2399 - val_loss: 9.2960\n",
            "Epoch 485/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 16.2336 - val_loss: 5.7064\n",
            "Epoch 486/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.5385 - val_loss: 7.4570\n",
            "Epoch 487/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 25.1743 - val_loss: 11.0407\n",
            "Epoch 488/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.9532 - val_loss: 7.2954\n",
            "Epoch 489/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.2567 - val_loss: 5.8275\n",
            "Epoch 490/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.4247 - val_loss: 6.0570\n",
            "Epoch 491/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.7075 - val_loss: 5.7099\n",
            "Epoch 492/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 19.8405 - val_loss: 6.4640\n",
            "Epoch 493/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.2199 - val_loss: 5.8740\n",
            "Epoch 494/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 21.6690 - val_loss: 6.3276\n",
            "Epoch 495/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.6341 - val_loss: 7.1010\n",
            "Epoch 496/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 18.5381 - val_loss: 6.3541\n",
            "Epoch 497/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.3668 - val_loss: 5.9247\n",
            "Epoch 498/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.3537 - val_loss: 9.4321\n",
            "Epoch 499/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.1559 - val_loss: 7.3048\n",
            "Epoch 500/500\n",
            "229/229 [==============================] - 0s 2ms/sample - loss: 17.3566 - val_loss: 9.4954\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f098d802940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6AQhs6sl3OM",
        "colab_type": "code",
        "outputId": "86f90217-40b9-42d3-c68e-3568a74c8d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "# demonstrate prediction\n",
        "x_input = training_minus_last_notes_padded[0][5:40]\n",
        "x_input = x_input.reshape((1, len(x_input), 4))\n",
        "print(x_input)\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "print(yhat[0][-1])\n",
        "print('expected: ', training_minus_last_notes_padded[0][41])"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[75 70 58 55]\n",
            "  [75 70 60 55]\n",
            "  [75 70 60 55]\n",
            "  [77 69 62 50]\n",
            "  [77 69 62 50]\n",
            "  [77 69 62 50]\n",
            "  [77 69 62 50]\n",
            "  [77 70 62 55]\n",
            "  [77 70 62 55]\n",
            "  [77 69 62 55]\n",
            "  [77 69 62 55]\n",
            "  [75 67 63 48]\n",
            "  [75 67 63 48]\n",
            "  [75 69 63 48]\n",
            "  [75 69 63 48]\n",
            "  [74 70 65 46]\n",
            "  [74 70 65 46]\n",
            "  [74 70 65 46]\n",
            "  [74 70 65 46]\n",
            "  [72 69 65 53]\n",
            "  [72 69 65 53]\n",
            "  [72 69 65 53]\n",
            "  [72 69 65 53]\n",
            "  [72 69 65 53]\n",
            "  [72 69 65 53]\n",
            "  [72 69 65 53]\n",
            "  [72 69 65 53]\n",
            "  [74 70 65 46]\n",
            "  [74 70 65 46]\n",
            "  [74 70 65 46]\n",
            "  [74 70 65 46]\n",
            "  [75 69 63 48]\n",
            "  [75 69 63 48]\n",
            "  [75 67 63 48]\n",
            "  [75 67 63 48]]]\n",
            "[70.16625  65.31986  60.064274 46.48195 ]\n",
            "expected:  [77 65 62 50]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f14qVjh4Hi0e",
        "colab_type": "text"
      },
      "source": [
        "Q10[part-2] Then use this model to generate Bach-like music, one note at a time: you can do this by giving the model the start of a chorale and asking it to predict the next time step, then appending these time steps to the input sequence and asking the model for the next note, and so on. Also make sure to check out Google’s Coconet model, which was used for a nice Google doodle about Bach.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh-2jgsZHkup",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a15c7e97-ca8b-495e-af29-3933ad245add"
      },
      "source": [
        "import random\n",
        "n_steps = None\n",
        "# convert into input/output\n",
        "i=1\n",
        "n=1\n",
        "x_input = np.array([training_minus_last_notes_padded[5][0]]) #using 5th chorale\n",
        "\n",
        "x_input = x_input.reshape((1, len(x_input), n_features))\n",
        "while i<len(training_minus_last_notes_padded[5]):\n",
        "  # demonstrate prediction\n",
        "  print('Input: ', x_input)\n",
        "  print('---------------')\n",
        "  yhat = model.predict(x_input, verbose=1)\n",
        "  output = np.array([yhat[0][-1]])\n",
        "  print('out:',output)\n",
        "  for j in range(len(output[0])):\n",
        "    output[0][j] = int(output[0][j] + random.random())\n",
        "  print('Predicted Output: ', output)\n",
        "  print('expected: ', bach_training_chorales_bass_last_notes_padded[5][i])\n",
        "  print('\\n\\n')\n",
        "\n",
        "  output = output.reshape((1, len(output), n_features))\n",
        "  x_input = np.concatenate([x_input, output], axis=1)\n",
        "  i += 1\n",
        "  if i>20:\n",
        "    break"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  [[[71 66 62 47]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 8ms/sample\n",
            "out: [[67.91701  62.819077 59.154694 46.598526]]\n",
            "Predicted Output:  [[68. 63. 59. 46.]]\n",
            "expected:  [71 66 62 47]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/sample\n",
            "out: [[67.15555  62.926598 58.29028  45.984703]]\n",
            "Predicted Output:  [[67. 63. 58. 46.]]\n",
            "expected:  [71 66 62 47]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 5ms/sample\n",
            "out: [[66.06477  62.32431  57.541103 45.240593]]\n",
            "Predicted Output:  [[66. 63. 58. 45.]]\n",
            "expected:  [71 67 64 52]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/sample\n",
            "out: [[63.933865 60.612312 56.071384 43.514526]]\n",
            "Predicted Output:  [[64. 60. 56. 44.]]\n",
            "expected:  [71 67 64 52]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/sample\n",
            "out: [[61.947475 58.686172 54.17954  42.34578 ]]\n",
            "Predicted Output:  [[62. 59. 55. 43.]]\n",
            "expected:  [71 67 64 54]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]\n",
            "  [62. 59. 55. 43.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/sample\n",
            "out: [[60.776676 57.688995 53.372078 41.562103]]\n",
            "Predicted Output:  [[61. 58. 53. 41.]]\n",
            "expected:  [71 67 64 54]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]\n",
            "  [62. 59. 55. 43.]\n",
            "  [61. 58. 53. 41.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 6ms/sample\n",
            "out: [[60.005764 56.72702  52.36365  40.435104]]\n",
            "Predicted Output:  [[60. 57. 52. 40.]]\n",
            "expected:  [71 67 64 55]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]\n",
            "  [62. 59. 55. 43.]\n",
            "  [61. 58. 53. 41.]\n",
            "  [60. 57. 52. 40.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/sample\n",
            "out: [[59.638355 56.32346  52.016827 40.05329 ]]\n",
            "Predicted Output:  [[60. 56. 52. 40.]]\n",
            "expected:  [71 67 64 55]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]\n",
            "  [62. 59. 55. 43.]\n",
            "  [61. 58. 53. 41.]\n",
            "  [60. 57. 52. 40.]\n",
            "  [60. 56. 52. 40.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/sample\n",
            "out: [[59.54102  56.16221  51.857845 39.967937]]\n",
            "Predicted Output:  [[60. 56. 52. 40.]]\n",
            "expected:  [71 67 64 57]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]\n",
            "  [62. 59. 55. 43.]\n",
            "  [61. 58. 53. 41.]\n",
            "  [60. 57. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/sample\n",
            "out: [[59.55025  56.16832  51.872955 39.973396]]\n",
            "Predicted Output:  [[60. 56. 52. 40.]]\n",
            "expected:  [71 67 64 57]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]\n",
            "  [62. 59. 55. 43.]\n",
            "  [61. 58. 53. 41.]\n",
            "  [60. 57. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/sample\n",
            "out: [[59.533535 56.148754 51.8539   39.958313]]\n",
            "Predicted Output:  [[59. 56. 52. 40.]]\n",
            "expected:  [78 66 62 59]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]\n",
            "  [62. 59. 55. 43.]\n",
            "  [61. 58. 53. 41.]\n",
            "  [60. 57. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/sample\n",
            "out: [[59.181843 55.994987 51.828495 39.942154]]\n",
            "Predicted Output:  [[59. 56. 52. 40.]]\n",
            "expected:  [78 66 62 59]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]\n",
            "  [62. 59. 55. 43.]\n",
            "  [61. 58. 53. 41.]\n",
            "  [60. 57. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 7ms/sample\n",
            "out: [[59.110493 55.925423 51.750553 39.869347]]\n",
            "Predicted Output:  [[59. 56. 52. 40.]]\n",
            "expected:  [78 68 62 62]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]\n",
            "  [62. 59. 55. 43.]\n",
            "  [61. 58. 53. 41.]\n",
            "  [60. 57. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 9ms/sample\n",
            "out: [[59.112934 55.92848  51.767242 39.884056]]\n",
            "Predicted Output:  [[59. 56. 52. 40.]]\n",
            "expected:  [78 68 62 62]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]\n",
            "  [62. 59. 55. 43.]\n",
            "  [61. 58. 53. 41.]\n",
            "  [60. 57. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 5ms/sample\n",
            "out: [[59.09168  55.906128 51.751114 39.86388 ]]\n",
            "Predicted Output:  [[59. 56. 52. 40.]]\n",
            "expected:  [76 70 67 61]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]\n",
            "  [62. 59. 55. 43.]\n",
            "  [61. 58. 53. 41.]\n",
            "  [60. 57. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 6ms/sample\n",
            "out: [[59.08281  55.894672 51.745766 39.856647]]\n",
            "Predicted Output:  [[59. 55. 51. 40.]]\n",
            "expected:  [76 70 67 61]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]\n",
            "  [62. 59. 55. 43.]\n",
            "  [61. 58. 53. 41.]\n",
            "  [60. 57. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 55. 51. 40.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 8ms/sample\n",
            "out: [[58.78453  55.419678 51.12687  39.675797]]\n",
            "Predicted Output:  [[59. 56. 51. 40.]]\n",
            "expected:  [76 71 67 59]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]\n",
            "  [62. 59. 55. 43.]\n",
            "  [61. 58. 53. 41.]\n",
            "  [60. 57. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 55. 51. 40.]\n",
            "  [59. 56. 51. 40.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 5ms/sample\n",
            "out: [[59.03922  55.726143 51.438152 39.848125]]\n",
            "Predicted Output:  [[59. 55. 52. 40.]]\n",
            "expected:  [76 71 67 59]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]\n",
            "  [62. 59. 55. 43.]\n",
            "  [61. 58. 53. 41.]\n",
            "  [60. 57. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 55. 51. 40.]\n",
            "  [59. 56. 51. 40.]\n",
            "  [59. 55. 52. 40.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/sample\n",
            "out: [[58.977116 55.718014 51.595108 39.772335]]\n",
            "Predicted Output:  [[59. 56. 52. 40.]]\n",
            "expected:  [78 73 66 58]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[71. 66. 62. 47.]\n",
            "  [68. 63. 59. 46.]\n",
            "  [67. 63. 58. 46.]\n",
            "  [66. 63. 58. 45.]\n",
            "  [64. 60. 56. 44.]\n",
            "  [62. 59. 55. 43.]\n",
            "  [61. 58. 53. 41.]\n",
            "  [60. 57. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [60. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 56. 52. 40.]\n",
            "  [59. 55. 51. 40.]\n",
            "  [59. 56. 51. 40.]\n",
            "  [59. 55. 52. 40.]\n",
            "  [59. 56. 52. 40.]]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 7ms/sample\n",
            "out: [[59.078754 55.87196  51.732975 39.847054]]\n",
            "Predicted Output:  [[59. 56. 51. 40.]]\n",
            "expected:  [78 73 66 58]\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEKnvo2gMpMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}