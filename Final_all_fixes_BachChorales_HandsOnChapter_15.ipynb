{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BachChorales_HandsOnChapter-15.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar-abhishek/handson-ml2/blob/master/Final_all_fixes_BachChorales_HandsOnChapter_15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fmXPTqmLxe9",
        "colab_type": "text"
      },
      "source": [
        "**Q10[part-1]. Download the Bach chorales dataset and unzip it. It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long, and each time step contains 4 integers, where each integer corresponds to a note’s index on a piano (except for the value 0, which means that no note is played). Train a model—recurrent, convolutional, or both—that can predict the next time step (four notes), given a sequence of time steps from a chorale. **bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k9Phc0e7EFM",
        "colab_type": "code",
        "outputId": "c52c3791-0de4-4b14-ff54-e42344a18a37",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7d178ceb-1819-4cfd-800d-653669609b6e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7d178ceb-1819-4cfd-800d-653669609b6e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving chorale_229.csv to chorale_229.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfrFpSvRUFUD",
        "colab_type": "code",
        "outputId": "842192c2-2740-426b-c8c3-f6ced06e6cef",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "uploaded_validation = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8ff49e8a-515e-4764-b78b-727cdb88af80\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8ff49e8a-515e-4764-b78b-727cdb88af80\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving chorale_000.csv to chorale_000.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpnbslo39PR_",
        "colab_type": "code",
        "outputId": "455f967e-fc75-4824-b923-9fe37150d47b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "X_train_000 = pd.read_csv('chorale_000.csv')\n",
        "print(X_train_000.head(10))\n",
        "\n",
        "X_valid_229 = pd.read_csv('chorale_229.csv')\n",
        "print(X_valid_229.head(10))\n",
        "\n",
        "X_train_000 = X_train_000.to_numpy()\n",
        "X_valid_229 = X_valid_229.to_numpy()\n",
        "\n",
        "dataX=X_train_000[:-1]\n",
        "dataY=X_train_000[1:]\n",
        "dataValidationX=X_valid_229[:-1]\n",
        "dataValidationY=X_valid_229[1:]\n",
        "\n",
        "print(dataX[0:10])\n",
        "print(dataY[0:10])\n",
        "print(type(dataX))\n",
        "\n",
        "scaler = MinMaxScaler( feature_range=(0, 1) )\n",
        "dataXScaler = scaler.fit_transform(dataX)\n",
        "dataYScaler = scaler.fit_transform(dataY)\n",
        "print(dataXScaler[0:10])\n",
        "print(dataYScaler[0:10])\n",
        "\n",
        "inversedX = scaler.inverse_transform(dataXScaler[0:10])\n",
        "print('inversed:')\n",
        "print(inversedX[0:10])\n",
        "print(scaler.inverse_transform(dataXScaler)[2:30])\n",
        "\n",
        "scalerValidation = MinMaxScaler( feature_range=(0, 1) )\n",
        "dataValidationX = scalerValidation.fit_transform(dataValidationX)\n",
        "dataValidationY = scalerValidation.fit_transform(dataValidationY)\n",
        "print(dataValidationX[0:5])\n",
        "print(dataValidationY[0:5])\n",
        "\n",
        "\n",
        "print(scaler.inverse_transform(dataXScaler)[2:30])\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   note0  note1  note2  note3\n",
            "0     74     70     65     58\n",
            "1     74     70     65     58\n",
            "2     74     70     65     58\n",
            "3     74     70     65     58\n",
            "4     75     70     58     55\n",
            "5     75     70     58     55\n",
            "6     75     70     60     55\n",
            "7     75     70     60     55\n",
            "8     77     69     62     50\n",
            "9     77     69     62     50\n",
            "   note0  note1  note2  note3\n",
            "0     72     67     60     48\n",
            "1     72     67     60     48\n",
            "2     72     67     60     48\n",
            "3     72     67     60     48\n",
            "4     72     67     64     48\n",
            "5     72     67     64     48\n",
            "6     72     67     64     50\n",
            "7     72     67     64     50\n",
            "8     72     67     64     52\n",
            "9     72     67     64     52\n",
            "[[74 70 65 58]\n",
            " [74 70 65 58]\n",
            " [74 70 65 58]\n",
            " [74 70 65 58]\n",
            " [75 70 58 55]\n",
            " [75 70 58 55]\n",
            " [75 70 60 55]\n",
            " [75 70 60 55]\n",
            " [77 69 62 50]\n",
            " [77 69 62 50]]\n",
            "[[74 70 65 58]\n",
            " [74 70 65 58]\n",
            " [74 70 65 58]\n",
            " [75 70 58 55]\n",
            " [75 70 58 55]\n",
            " [75 70 60 55]\n",
            " [75 70 60 55]\n",
            " [77 69 62 50]\n",
            " [77 69 62 50]\n",
            " [77 69 62 50]]\n",
            "<class 'numpy.ndarray'>\n",
            "[[0.44444444 0.77777778 1.         0.88235294]\n",
            " [0.44444444 0.77777778 1.         0.88235294]\n",
            " [0.44444444 0.77777778 1.         0.88235294]\n",
            " [0.44444444 0.77777778 1.         0.88235294]\n",
            " [0.55555556 0.77777778 0.41666667 0.70588235]\n",
            " [0.55555556 0.77777778 0.41666667 0.70588235]\n",
            " [0.55555556 0.77777778 0.58333333 0.70588235]\n",
            " [0.55555556 0.77777778 0.58333333 0.70588235]\n",
            " [0.77777778 0.66666667 0.75       0.41176471]\n",
            " [0.77777778 0.66666667 0.75       0.41176471]]\n",
            "[[0.44444444 0.77777778 1.         0.88235294]\n",
            " [0.44444444 0.77777778 1.         0.88235294]\n",
            " [0.44444444 0.77777778 1.         0.88235294]\n",
            " [0.55555556 0.77777778 0.41666667 0.70588235]\n",
            " [0.55555556 0.77777778 0.41666667 0.70588235]\n",
            " [0.55555556 0.77777778 0.58333333 0.70588235]\n",
            " [0.55555556 0.77777778 0.58333333 0.70588235]\n",
            " [0.77777778 0.66666667 0.75       0.41176471]\n",
            " [0.77777778 0.66666667 0.75       0.41176471]\n",
            " [0.77777778 0.66666667 0.75       0.41176471]]\n",
            "inversed:\n",
            "[[74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]]\n",
            "[[74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]]\n",
            "[[0.41666667 0.375      0.41666667 0.2       ]\n",
            " [0.41666667 0.375      0.41666667 0.2       ]\n",
            " [0.41666667 0.375      0.41666667 0.2       ]\n",
            " [0.41666667 0.375      0.41666667 0.2       ]\n",
            " [0.41666667 0.375      0.75       0.2       ]]\n",
            "[[0.41666667 0.375      0.41666667 0.2       ]\n",
            " [0.41666667 0.375      0.41666667 0.2       ]\n",
            " [0.41666667 0.375      0.41666667 0.2       ]\n",
            " [0.41666667 0.375      0.75       0.2       ]\n",
            " [0.41666667 0.375      0.75       0.2       ]]\n",
            "[[74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "eb81c8b8-3784-4135-a555-818fc3c102cf",
        "id": "9gLSJCJeoNQg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# convert into input/output\n",
        "i,j=0,0\n",
        "k,l=0,0\n",
        "X, y=[], []\n",
        "validX, validY=[], []\n",
        "\n",
        "while i<len(dataXScaler):\n",
        "  X.append(np.array([dataXScaler[i]]))\n",
        "  i += 1\n",
        "\n",
        "while j<len(dataYScaler):\n",
        "  y.append(dataYScaler[j])\n",
        "  j+= 1\n",
        "\n",
        "while k<len(dataValidationX):\n",
        "  validX.append(np.array([dataValidationX[k]]))\n",
        "  k += 1\n",
        "\n",
        "while l<len(dataValidationY):\n",
        "  validY.append(dataValidationY[l])\n",
        "  l+=1\n",
        "\n",
        "X=np.asarray(X)\n",
        "y=np.asarray(y)\n",
        "print(X[0:2])\n",
        "print(y[0])\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "\n",
        "validX=np.asarray(validX)\n",
        "validY=np.asarray(validY)\n",
        "print(validX[0:2])\n",
        "print(validY[0])\n",
        "print(validX.shape, validY.shape)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0.44444444 0.77777778 1.         0.88235294]]\n",
            "\n",
            " [[0.44444444 0.77777778 1.         0.88235294]]]\n",
            "[0.44444444 0.77777778 1.         0.88235294]\n",
            "(191, 1, 4) (191, 4)\n",
            "[[[0.41666667 0.375      0.41666667 0.2       ]]\n",
            "\n",
            " [[0.41666667 0.375      0.41666667 0.2       ]]]\n",
            "[0.41666667 0.375      0.41666667 0.2       ]\n",
            "(195, 1, 4) (195, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5kzBrOIC6gB",
        "colab_type": "code",
        "outputId": "35cf3223-a563-45cf-c1b2-70315cd51686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, TimeDistributed\n",
        "# the dataset knows the number of features, e.g. 2\n",
        "n_features = 4\n",
        "\n",
        "# choose a number of time steps\n",
        "n_steps = None\n",
        "\n",
        "model = Sequential()\n",
        "model.add(TimeDistributed(Dense(128), input_shape=(None, n_features)) ) # This line makes a lot of difference but why?\n",
        "model.add(LSTM(64, input_shape=(None, n_features), return_sequences=True))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(32))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(n_features))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# fit model\n",
        "model.fit(X, y, epochs=200, validation_data=(validX, validY))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 191 samples, validate on 195 samples\n",
            "Epoch 1/200\n",
            "191/191 [==============================] - 12s 61ms/step - loss: 0.2875 - val_loss: 0.2520\n",
            "Epoch 2/200\n",
            "191/191 [==============================] - 0s 363us/step - loss: 0.2435 - val_loss: 0.2039\n",
            "Epoch 3/200\n",
            "191/191 [==============================] - 0s 332us/step - loss: 0.1859 - val_loss: 0.1397\n",
            "Epoch 4/200\n",
            "191/191 [==============================] - 0s 366us/step - loss: 0.1156 - val_loss: 0.0740\n",
            "Epoch 5/200\n",
            "191/191 [==============================] - 0s 389us/step - loss: 0.0622 - val_loss: 0.0537\n",
            "Epoch 6/200\n",
            "191/191 [==============================] - 0s 394us/step - loss: 0.0588 - val_loss: 0.0574\n",
            "Epoch 7/200\n",
            "191/191 [==============================] - 0s 369us/step - loss: 0.0594 - val_loss: 0.0485\n",
            "Epoch 8/200\n",
            "191/191 [==============================] - 0s 370us/step - loss: 0.0515 - val_loss: 0.0467\n",
            "Epoch 9/200\n",
            "191/191 [==============================] - 0s 381us/step - loss: 0.0491 - val_loss: 0.0460\n",
            "Epoch 10/200\n",
            "191/191 [==============================] - 0s 398us/step - loss: 0.0486 - val_loss: 0.0451\n",
            "Epoch 11/200\n",
            "191/191 [==============================] - 0s 382us/step - loss: 0.0503 - val_loss: 0.0441\n",
            "Epoch 12/200\n",
            "191/191 [==============================] - 0s 393us/step - loss: 0.0452 - val_loss: 0.0415\n",
            "Epoch 13/200\n",
            "191/191 [==============================] - 0s 359us/step - loss: 0.0430 - val_loss: 0.0392\n",
            "Epoch 14/200\n",
            "191/191 [==============================] - 0s 352us/step - loss: 0.0393 - val_loss: 0.0369\n",
            "Epoch 15/200\n",
            "191/191 [==============================] - 0s 358us/step - loss: 0.0372 - val_loss: 0.0346\n",
            "Epoch 16/200\n",
            "191/191 [==============================] - 0s 362us/step - loss: 0.0368 - val_loss: 0.0327\n",
            "Epoch 17/200\n",
            "191/191 [==============================] - 0s 454us/step - loss: 0.0342 - val_loss: 0.0309\n",
            "Epoch 18/200\n",
            "191/191 [==============================] - 0s 396us/step - loss: 0.0323 - val_loss: 0.0279\n",
            "Epoch 19/200\n",
            "191/191 [==============================] - 0s 350us/step - loss: 0.0309 - val_loss: 0.0263\n",
            "Epoch 20/200\n",
            "191/191 [==============================] - 0s 371us/step - loss: 0.0297 - val_loss: 0.0257\n",
            "Epoch 21/200\n",
            "191/191 [==============================] - 0s 320us/step - loss: 0.0293 - val_loss: 0.0239\n",
            "Epoch 22/200\n",
            "191/191 [==============================] - 0s 407us/step - loss: 0.0272 - val_loss: 0.0233\n",
            "Epoch 23/200\n",
            "191/191 [==============================] - 0s 431us/step - loss: 0.0255 - val_loss: 0.0220\n",
            "Epoch 24/200\n",
            "191/191 [==============================] - 0s 403us/step - loss: 0.0276 - val_loss: 0.0226\n",
            "Epoch 25/200\n",
            "191/191 [==============================] - 0s 381us/step - loss: 0.0260 - val_loss: 0.0219\n",
            "Epoch 26/200\n",
            "191/191 [==============================] - 0s 354us/step - loss: 0.0269 - val_loss: 0.0213\n",
            "Epoch 27/200\n",
            "191/191 [==============================] - 0s 374us/step - loss: 0.0254 - val_loss: 0.0212\n",
            "Epoch 28/200\n",
            "191/191 [==============================] - 0s 404us/step - loss: 0.0241 - val_loss: 0.0202\n",
            "Epoch 29/200\n",
            "191/191 [==============================] - 0s 399us/step - loss: 0.0259 - val_loss: 0.0195\n",
            "Epoch 30/200\n",
            "191/191 [==============================] - 0s 353us/step - loss: 0.0253 - val_loss: 0.0198\n",
            "Epoch 31/200\n",
            "191/191 [==============================] - 0s 384us/step - loss: 0.0237 - val_loss: 0.0211\n",
            "Epoch 32/200\n",
            "191/191 [==============================] - 0s 377us/step - loss: 0.0238 - val_loss: 0.0194\n",
            "Epoch 33/200\n",
            "191/191 [==============================] - 0s 428us/step - loss: 0.0248 - val_loss: 0.0191\n",
            "Epoch 34/200\n",
            "191/191 [==============================] - 0s 377us/step - loss: 0.0242 - val_loss: 0.0192\n",
            "Epoch 35/200\n",
            "191/191 [==============================] - 0s 367us/step - loss: 0.0254 - val_loss: 0.0190\n",
            "Epoch 36/200\n",
            "191/191 [==============================] - 0s 334us/step - loss: 0.0238 - val_loss: 0.0182\n",
            "Epoch 37/200\n",
            "191/191 [==============================] - 0s 361us/step - loss: 0.0223 - val_loss: 0.0187\n",
            "Epoch 38/200\n",
            "191/191 [==============================] - 0s 347us/step - loss: 0.0221 - val_loss: 0.0194\n",
            "Epoch 39/200\n",
            "191/191 [==============================] - 0s 365us/step - loss: 0.0226 - val_loss: 0.0179\n",
            "Epoch 40/200\n",
            "191/191 [==============================] - 0s 373us/step - loss: 0.0252 - val_loss: 0.0187\n",
            "Epoch 41/200\n",
            "191/191 [==============================] - 0s 318us/step - loss: 0.0225 - val_loss: 0.0190\n",
            "Epoch 42/200\n",
            "191/191 [==============================] - 0s 420us/step - loss: 0.0241 - val_loss: 0.0180\n",
            "Epoch 43/200\n",
            "191/191 [==============================] - 0s 334us/step - loss: 0.0215 - val_loss: 0.0179\n",
            "Epoch 44/200\n",
            "191/191 [==============================] - 0s 386us/step - loss: 0.0210 - val_loss: 0.0181\n",
            "Epoch 45/200\n",
            "191/191 [==============================] - 0s 371us/step - loss: 0.0229 - val_loss: 0.0186\n",
            "Epoch 46/200\n",
            "191/191 [==============================] - 0s 367us/step - loss: 0.0226 - val_loss: 0.0182\n",
            "Epoch 47/200\n",
            "191/191 [==============================] - 0s 340us/step - loss: 0.0211 - val_loss: 0.0183\n",
            "Epoch 48/200\n",
            "191/191 [==============================] - 0s 372us/step - loss: 0.0220 - val_loss: 0.0183\n",
            "Epoch 49/200\n",
            "191/191 [==============================] - 0s 343us/step - loss: 0.0243 - val_loss: 0.0179\n",
            "Epoch 50/200\n",
            "191/191 [==============================] - 0s 346us/step - loss: 0.0238 - val_loss: 0.0180\n",
            "Epoch 51/200\n",
            "191/191 [==============================] - 0s 367us/step - loss: 0.0223 - val_loss: 0.0183\n",
            "Epoch 52/200\n",
            "191/191 [==============================] - 0s 328us/step - loss: 0.0206 - val_loss: 0.0182\n",
            "Epoch 53/200\n",
            "191/191 [==============================] - 0s 387us/step - loss: 0.0236 - val_loss: 0.0179\n",
            "Epoch 54/200\n",
            "191/191 [==============================] - 0s 405us/step - loss: 0.0221 - val_loss: 0.0184\n",
            "Epoch 55/200\n",
            "191/191 [==============================] - 0s 337us/step - loss: 0.0220 - val_loss: 0.0184\n",
            "Epoch 56/200\n",
            "191/191 [==============================] - 0s 370us/step - loss: 0.0211 - val_loss: 0.0188\n",
            "Epoch 57/200\n",
            "191/191 [==============================] - 0s 356us/step - loss: 0.0215 - val_loss: 0.0182\n",
            "Epoch 58/200\n",
            "191/191 [==============================] - 0s 558us/step - loss: 0.0202 - val_loss: 0.0186\n",
            "Epoch 59/200\n",
            "191/191 [==============================] - 0s 377us/step - loss: 0.0213 - val_loss: 0.0179\n",
            "Epoch 60/200\n",
            "191/191 [==============================] - 0s 387us/step - loss: 0.0224 - val_loss: 0.0179\n",
            "Epoch 61/200\n",
            "191/191 [==============================] - 0s 397us/step - loss: 0.0221 - val_loss: 0.0186\n",
            "Epoch 62/200\n",
            "191/191 [==============================] - 0s 418us/step - loss: 0.0193 - val_loss: 0.0183\n",
            "Epoch 63/200\n",
            "191/191 [==============================] - 0s 365us/step - loss: 0.0210 - val_loss: 0.0181\n",
            "Epoch 64/200\n",
            "191/191 [==============================] - 0s 374us/step - loss: 0.0211 - val_loss: 0.0186\n",
            "Epoch 65/200\n",
            "191/191 [==============================] - 0s 396us/step - loss: 0.0226 - val_loss: 0.0183\n",
            "Epoch 66/200\n",
            "191/191 [==============================] - 0s 375us/step - loss: 0.0209 - val_loss: 0.0183\n",
            "Epoch 67/200\n",
            "191/191 [==============================] - 0s 377us/step - loss: 0.0215 - val_loss: 0.0192\n",
            "Epoch 68/200\n",
            "191/191 [==============================] - 0s 351us/step - loss: 0.0213 - val_loss: 0.0184\n",
            "Epoch 69/200\n",
            "191/191 [==============================] - 0s 352us/step - loss: 0.0194 - val_loss: 0.0181\n",
            "Epoch 70/200\n",
            "191/191 [==============================] - 0s 371us/step - loss: 0.0192 - val_loss: 0.0184\n",
            "Epoch 71/200\n",
            "191/191 [==============================] - 0s 403us/step - loss: 0.0206 - val_loss: 0.0185\n",
            "Epoch 72/200\n",
            "191/191 [==============================] - 0s 379us/step - loss: 0.0227 - val_loss: 0.0183\n",
            "Epoch 73/200\n",
            "191/191 [==============================] - 0s 427us/step - loss: 0.0209 - val_loss: 0.0199\n",
            "Epoch 74/200\n",
            "191/191 [==============================] - 0s 406us/step - loss: 0.0183 - val_loss: 0.0186\n",
            "Epoch 75/200\n",
            "191/191 [==============================] - 0s 385us/step - loss: 0.0212 - val_loss: 0.0187\n",
            "Epoch 76/200\n",
            "191/191 [==============================] - 0s 359us/step - loss: 0.0189 - val_loss: 0.0190\n",
            "Epoch 77/200\n",
            "191/191 [==============================] - 0s 383us/step - loss: 0.0204 - val_loss: 0.0187\n",
            "Epoch 78/200\n",
            "191/191 [==============================] - 0s 390us/step - loss: 0.0213 - val_loss: 0.0185\n",
            "Epoch 79/200\n",
            "191/191 [==============================] - 0s 418us/step - loss: 0.0198 - val_loss: 0.0189\n",
            "Epoch 80/200\n",
            "191/191 [==============================] - 0s 363us/step - loss: 0.0198 - val_loss: 0.0186\n",
            "Epoch 81/200\n",
            "191/191 [==============================] - 0s 377us/step - loss: 0.0203 - val_loss: 0.0196\n",
            "Epoch 82/200\n",
            "191/191 [==============================] - 0s 361us/step - loss: 0.0194 - val_loss: 0.0192\n",
            "Epoch 83/200\n",
            "191/191 [==============================] - 0s 364us/step - loss: 0.0212 - val_loss: 0.0190\n",
            "Epoch 84/200\n",
            "191/191 [==============================] - 0s 373us/step - loss: 0.0206 - val_loss: 0.0187\n",
            "Epoch 85/200\n",
            "191/191 [==============================] - 0s 350us/step - loss: 0.0205 - val_loss: 0.0186\n",
            "Epoch 86/200\n",
            "191/191 [==============================] - 0s 405us/step - loss: 0.0206 - val_loss: 0.0194\n",
            "Epoch 87/200\n",
            "191/191 [==============================] - 0s 392us/step - loss: 0.0210 - val_loss: 0.0183\n",
            "Epoch 88/200\n",
            "191/191 [==============================] - 0s 370us/step - loss: 0.0218 - val_loss: 0.0185\n",
            "Epoch 89/200\n",
            "191/191 [==============================] - 0s 346us/step - loss: 0.0197 - val_loss: 0.0184\n",
            "Epoch 90/200\n",
            "191/191 [==============================] - 0s 331us/step - loss: 0.0187 - val_loss: 0.0188\n",
            "Epoch 91/200\n",
            "191/191 [==============================] - 0s 306us/step - loss: 0.0205 - val_loss: 0.0189\n",
            "Epoch 92/200\n",
            "191/191 [==============================] - 0s 325us/step - loss: 0.0205 - val_loss: 0.0189\n",
            "Epoch 93/200\n",
            "191/191 [==============================] - 0s 384us/step - loss: 0.0202 - val_loss: 0.0187\n",
            "Epoch 94/200\n",
            "191/191 [==============================] - 0s 338us/step - loss: 0.0211 - val_loss: 0.0189\n",
            "Epoch 95/200\n",
            "191/191 [==============================] - 0s 350us/step - loss: 0.0217 - val_loss: 0.0187\n",
            "Epoch 96/200\n",
            "191/191 [==============================] - 0s 364us/step - loss: 0.0184 - val_loss: 0.0187\n",
            "Epoch 97/200\n",
            "191/191 [==============================] - 0s 368us/step - loss: 0.0187 - val_loss: 0.0190\n",
            "Epoch 98/200\n",
            "191/191 [==============================] - 0s 372us/step - loss: 0.0189 - val_loss: 0.0187\n",
            "Epoch 99/200\n",
            "191/191 [==============================] - 0s 410us/step - loss: 0.0194 - val_loss: 0.0187\n",
            "Epoch 100/200\n",
            "191/191 [==============================] - 0s 389us/step - loss: 0.0198 - val_loss: 0.0188\n",
            "Epoch 101/200\n",
            "191/191 [==============================] - 0s 450us/step - loss: 0.0191 - val_loss: 0.0186\n",
            "Epoch 102/200\n",
            "191/191 [==============================] - 0s 334us/step - loss: 0.0206 - val_loss: 0.0196\n",
            "Epoch 103/200\n",
            "191/191 [==============================] - 0s 388us/step - loss: 0.0191 - val_loss: 0.0194\n",
            "Epoch 104/200\n",
            "191/191 [==============================] - 0s 358us/step - loss: 0.0199 - val_loss: 0.0192\n",
            "Epoch 105/200\n",
            "191/191 [==============================] - 0s 322us/step - loss: 0.0202 - val_loss: 0.0190\n",
            "Epoch 106/200\n",
            "191/191 [==============================] - 0s 373us/step - loss: 0.0209 - val_loss: 0.0201\n",
            "Epoch 107/200\n",
            "191/191 [==============================] - 0s 344us/step - loss: 0.0197 - val_loss: 0.0192\n",
            "Epoch 108/200\n",
            "191/191 [==============================] - 0s 347us/step - loss: 0.0181 - val_loss: 0.0191\n",
            "Epoch 109/200\n",
            "191/191 [==============================] - 0s 317us/step - loss: 0.0189 - val_loss: 0.0196\n",
            "Epoch 110/200\n",
            "191/191 [==============================] - 0s 338us/step - loss: 0.0195 - val_loss: 0.0189\n",
            "Epoch 111/200\n",
            "191/191 [==============================] - 0s 388us/step - loss: 0.0200 - val_loss: 0.0189\n",
            "Epoch 112/200\n",
            "191/191 [==============================] - 0s 380us/step - loss: 0.0203 - val_loss: 0.0195\n",
            "Epoch 113/200\n",
            "191/191 [==============================] - 0s 365us/step - loss: 0.0189 - val_loss: 0.0187\n",
            "Epoch 114/200\n",
            "191/191 [==============================] - 0s 370us/step - loss: 0.0188 - val_loss: 0.0192\n",
            "Epoch 115/200\n",
            "191/191 [==============================] - 0s 366us/step - loss: 0.0192 - val_loss: 0.0190\n",
            "Epoch 116/200\n",
            "191/191 [==============================] - 0s 354us/step - loss: 0.0191 - val_loss: 0.0190\n",
            "Epoch 117/200\n",
            "191/191 [==============================] - 0s 354us/step - loss: 0.0188 - val_loss: 0.0193\n",
            "Epoch 118/200\n",
            "191/191 [==============================] - 0s 337us/step - loss: 0.0193 - val_loss: 0.0195\n",
            "Epoch 119/200\n",
            "191/191 [==============================] - 0s 356us/step - loss: 0.0195 - val_loss: 0.0191\n",
            "Epoch 120/200\n",
            "191/191 [==============================] - 0s 398us/step - loss: 0.0190 - val_loss: 0.0200\n",
            "Epoch 121/200\n",
            "191/191 [==============================] - 0s 382us/step - loss: 0.0202 - val_loss: 0.0189\n",
            "Epoch 122/200\n",
            "191/191 [==============================] - 0s 401us/step - loss: 0.0188 - val_loss: 0.0191\n",
            "Epoch 123/200\n",
            "191/191 [==============================] - 0s 349us/step - loss: 0.0192 - val_loss: 0.0190\n",
            "Epoch 124/200\n",
            "191/191 [==============================] - 0s 347us/step - loss: 0.0176 - val_loss: 0.0187\n",
            "Epoch 125/200\n",
            "191/191 [==============================] - 0s 361us/step - loss: 0.0197 - val_loss: 0.0196\n",
            "Epoch 126/200\n",
            "191/191 [==============================] - 0s 392us/step - loss: 0.0184 - val_loss: 0.0193\n",
            "Epoch 127/200\n",
            "191/191 [==============================] - 0s 399us/step - loss: 0.0174 - val_loss: 0.0192\n",
            "Epoch 128/200\n",
            "191/191 [==============================] - 0s 367us/step - loss: 0.0196 - val_loss: 0.0196\n",
            "Epoch 129/200\n",
            "191/191 [==============================] - 0s 395us/step - loss: 0.0204 - val_loss: 0.0200\n",
            "Epoch 130/200\n",
            "191/191 [==============================] - 0s 336us/step - loss: 0.0186 - val_loss: 0.0189\n",
            "Epoch 131/200\n",
            "191/191 [==============================] - 0s 341us/step - loss: 0.0196 - val_loss: 0.0188\n",
            "Epoch 132/200\n",
            "191/191 [==============================] - 0s 367us/step - loss: 0.0192 - val_loss: 0.0194\n",
            "Epoch 133/200\n",
            "191/191 [==============================] - 0s 363us/step - loss: 0.0197 - val_loss: 0.0189\n",
            "Epoch 134/200\n",
            "191/191 [==============================] - 0s 375us/step - loss: 0.0203 - val_loss: 0.0196\n",
            "Epoch 135/200\n",
            "191/191 [==============================] - 0s 345us/step - loss: 0.0187 - val_loss: 0.0190\n",
            "Epoch 136/200\n",
            "191/191 [==============================] - 0s 395us/step - loss: 0.0191 - val_loss: 0.0189\n",
            "Epoch 137/200\n",
            "191/191 [==============================] - 0s 342us/step - loss: 0.0184 - val_loss: 0.0193\n",
            "Epoch 138/200\n",
            "191/191 [==============================] - 0s 362us/step - loss: 0.0189 - val_loss: 0.0188\n",
            "Epoch 139/200\n",
            "191/191 [==============================] - 0s 373us/step - loss: 0.0191 - val_loss: 0.0193\n",
            "Epoch 140/200\n",
            "191/191 [==============================] - 0s 337us/step - loss: 0.0197 - val_loss: 0.0190\n",
            "Epoch 141/200\n",
            "191/191 [==============================] - 0s 358us/step - loss: 0.0183 - val_loss: 0.0196\n",
            "Epoch 142/200\n",
            "191/191 [==============================] - 0s 335us/step - loss: 0.0186 - val_loss: 0.0195\n",
            "Epoch 143/200\n",
            "191/191 [==============================] - 0s 422us/step - loss: 0.0196 - val_loss: 0.0196\n",
            "Epoch 144/200\n",
            "191/191 [==============================] - 0s 345us/step - loss: 0.0186 - val_loss: 0.0193\n",
            "Epoch 145/200\n",
            "191/191 [==============================] - 0s 338us/step - loss: 0.0194 - val_loss: 0.0190\n",
            "Epoch 146/200\n",
            "191/191 [==============================] - 0s 348us/step - loss: 0.0177 - val_loss: 0.0200\n",
            "Epoch 147/200\n",
            "191/191 [==============================] - 0s 339us/step - loss: 0.0185 - val_loss: 0.0192\n",
            "Epoch 148/200\n",
            "191/191 [==============================] - 0s 411us/step - loss: 0.0186 - val_loss: 0.0192\n",
            "Epoch 149/200\n",
            "191/191 [==============================] - 0s 390us/step - loss: 0.0183 - val_loss: 0.0194\n",
            "Epoch 150/200\n",
            "191/191 [==============================] - 0s 406us/step - loss: 0.0182 - val_loss: 0.0190\n",
            "Epoch 151/200\n",
            "191/191 [==============================] - 0s 334us/step - loss: 0.0193 - val_loss: 0.0189\n",
            "Epoch 152/200\n",
            "191/191 [==============================] - 0s 335us/step - loss: 0.0179 - val_loss: 0.0198\n",
            "Epoch 153/200\n",
            "191/191 [==============================] - 0s 365us/step - loss: 0.0183 - val_loss: 0.0192\n",
            "Epoch 154/200\n",
            "191/191 [==============================] - 0s 335us/step - loss: 0.0180 - val_loss: 0.0188\n",
            "Epoch 155/200\n",
            "191/191 [==============================] - 0s 359us/step - loss: 0.0191 - val_loss: 0.0194\n",
            "Epoch 156/200\n",
            "191/191 [==============================] - 0s 376us/step - loss: 0.0183 - val_loss: 0.0191\n",
            "Epoch 157/200\n",
            "191/191 [==============================] - 0s 378us/step - loss: 0.0186 - val_loss: 0.0197\n",
            "Epoch 158/200\n",
            "191/191 [==============================] - 0s 371us/step - loss: 0.0177 - val_loss: 0.0193\n",
            "Epoch 159/200\n",
            "191/191 [==============================] - 0s 381us/step - loss: 0.0181 - val_loss: 0.0195\n",
            "Epoch 160/200\n",
            "191/191 [==============================] - 0s 350us/step - loss: 0.0189 - val_loss: 0.0193\n",
            "Epoch 161/200\n",
            "191/191 [==============================] - 0s 371us/step - loss: 0.0175 - val_loss: 0.0190\n",
            "Epoch 162/200\n",
            "191/191 [==============================] - 0s 314us/step - loss: 0.0183 - val_loss: 0.0197\n",
            "Epoch 163/200\n",
            "191/191 [==============================] - 0s 357us/step - loss: 0.0172 - val_loss: 0.0190\n",
            "Epoch 164/200\n",
            "191/191 [==============================] - 0s 404us/step - loss: 0.0193 - val_loss: 0.0189\n",
            "Epoch 165/200\n",
            "191/191 [==============================] - 0s 363us/step - loss: 0.0183 - val_loss: 0.0194\n",
            "Epoch 166/200\n",
            "191/191 [==============================] - 0s 368us/step - loss: 0.0185 - val_loss: 0.0194\n",
            "Epoch 167/200\n",
            "191/191 [==============================] - 0s 335us/step - loss: 0.0188 - val_loss: 0.0190\n",
            "Epoch 168/200\n",
            "191/191 [==============================] - 0s 387us/step - loss: 0.0189 - val_loss: 0.0192\n",
            "Epoch 169/200\n",
            "191/191 [==============================] - 0s 377us/step - loss: 0.0181 - val_loss: 0.0196\n",
            "Epoch 170/200\n",
            "191/191 [==============================] - 0s 345us/step - loss: 0.0178 - val_loss: 0.0191\n",
            "Epoch 171/200\n",
            "191/191 [==============================] - 0s 375us/step - loss: 0.0194 - val_loss: 0.0191\n",
            "Epoch 172/200\n",
            "191/191 [==============================] - 0s 388us/step - loss: 0.0174 - val_loss: 0.0196\n",
            "Epoch 173/200\n",
            "191/191 [==============================] - 0s 344us/step - loss: 0.0163 - val_loss: 0.0191\n",
            "Epoch 174/200\n",
            "191/191 [==============================] - 0s 381us/step - loss: 0.0175 - val_loss: 0.0196\n",
            "Epoch 175/200\n",
            "191/191 [==============================] - 0s 389us/step - loss: 0.0157 - val_loss: 0.0192\n",
            "Epoch 176/200\n",
            "191/191 [==============================] - 0s 367us/step - loss: 0.0184 - val_loss: 0.0193\n",
            "Epoch 177/200\n",
            "191/191 [==============================] - 0s 384us/step - loss: 0.0169 - val_loss: 0.0194\n",
            "Epoch 178/200\n",
            "191/191 [==============================] - 0s 321us/step - loss: 0.0164 - val_loss: 0.0191\n",
            "Epoch 179/200\n",
            "191/191 [==============================] - 0s 350us/step - loss: 0.0180 - val_loss: 0.0189\n",
            "Epoch 180/200\n",
            "191/191 [==============================] - 0s 308us/step - loss: 0.0183 - val_loss: 0.0197\n",
            "Epoch 181/200\n",
            "191/191 [==============================] - 0s 355us/step - loss: 0.0184 - val_loss: 0.0189\n",
            "Epoch 182/200\n",
            "191/191 [==============================] - 0s 365us/step - loss: 0.0175 - val_loss: 0.0190\n",
            "Epoch 183/200\n",
            "191/191 [==============================] - 0s 402us/step - loss: 0.0177 - val_loss: 0.0204\n",
            "Epoch 184/200\n",
            "191/191 [==============================] - 0s 326us/step - loss: 0.0174 - val_loss: 0.0191\n",
            "Epoch 185/200\n",
            "191/191 [==============================] - 0s 399us/step - loss: 0.0192 - val_loss: 0.0190\n",
            "Epoch 186/200\n",
            "191/191 [==============================] - 0s 393us/step - loss: 0.0188 - val_loss: 0.0199\n",
            "Epoch 187/200\n",
            "191/191 [==============================] - 0s 352us/step - loss: 0.0190 - val_loss: 0.0192\n",
            "Epoch 188/200\n",
            "191/191 [==============================] - 0s 369us/step - loss: 0.0177 - val_loss: 0.0199\n",
            "Epoch 189/200\n",
            "191/191 [==============================] - 0s 376us/step - loss: 0.0180 - val_loss: 0.0197\n",
            "Epoch 190/200\n",
            "191/191 [==============================] - 0s 390us/step - loss: 0.0174 - val_loss: 0.0194\n",
            "Epoch 191/200\n",
            "191/191 [==============================] - 0s 341us/step - loss: 0.0181 - val_loss: 0.0192\n",
            "Epoch 192/200\n",
            "191/191 [==============================] - 0s 381us/step - loss: 0.0171 - val_loss: 0.0194\n",
            "Epoch 193/200\n",
            "191/191 [==============================] - 0s 385us/step - loss: 0.0172 - val_loss: 0.0194\n",
            "Epoch 194/200\n",
            "191/191 [==============================] - 0s 361us/step - loss: 0.0160 - val_loss: 0.0194\n",
            "Epoch 195/200\n",
            "191/191 [==============================] - 0s 330us/step - loss: 0.0178 - val_loss: 0.0198\n",
            "Epoch 196/200\n",
            "191/191 [==============================] - 0s 360us/step - loss: 0.0172 - val_loss: 0.0195\n",
            "Epoch 197/200\n",
            "191/191 [==============================] - 0s 397us/step - loss: 0.0174 - val_loss: 0.0200\n",
            "Epoch 198/200\n",
            "191/191 [==============================] - 0s 510us/step - loss: 0.0173 - val_loss: 0.0197\n",
            "Epoch 199/200\n",
            "191/191 [==============================] - 0s 388us/step - loss: 0.0175 - val_loss: 0.0192\n",
            "Epoch 200/200\n",
            "191/191 [==============================] - 0s 377us/step - loss: 0.0182 - val_loss: 0.0198\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f70eaa6b320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJHLBiJEVd8r",
        "colab_type": "code",
        "outputId": "8ff854bd-df2f-484b-e6a0-8146191a8608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# demonstrate prediction\n",
        "print(scaler.inverse_transform(dataXScaler)[2:30])\n",
        "\n",
        "x_input = dataXScaler[2:30]\n",
        "x_input = x_input.reshape((1, len(x_input), 4))\n",
        "print(scaler.inverse_transform(dataXScaler)[2:30])\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "print(scaler.inverse_transform(yhat))\n",
        "print('expected: ', scaler.inverse_transform(dataYScaler)[30])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]]\n",
            "[[74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]]\n",
            "[[76.146225 80.504166 80.397415 58.849724]]\n",
            "expected:  [72. 69. 65. 53.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpvggFUajEDg",
        "colab_type": "text"
      },
      "source": [
        "**Q10[part-2] Then use this model to generate Bach-like music, one note at a time: you can do this by giving the model the start of a chorale and asking it to predict the next time step, then appending these time steps to the input sequence and asking the model for the next note, and so on. Also make sure to check out Google’s Coconet model, which was used for a nice Google doodle about Bach.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do-eQEQTQaJj",
        "colab_type": "code",
        "outputId": "176e0581-ed77-40a6-991b-48398e0ed38f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# choose a number of time steps\n",
        "n_steps = None\n",
        "# convert into input/output\n",
        "i=1\n",
        "n=1\n",
        "x_input = np.array([dataXScaler[0]])\n",
        "\n",
        "x_input = x_input.reshape((1, len(x_input), n_features))\n",
        "while i<len(dataXScaler):\n",
        "  # demonstrate prediction\n",
        "  print('Input: ')\n",
        "  for input in x_input:\n",
        "    print(scaler.inverse_transform(input))\n",
        "  print('---------------')\n",
        "  yhat = model.predict(x_input, verbose=1)\n",
        "  print('Predicted Output: ', scaler.inverse_transform(yhat))\n",
        "  print('expected: ', scaler.inverse_transform(dataYScaler)[i])\n",
        "  print('\\n\\n')\n",
        "\n",
        "  yhat = yhat.reshape((1, len(yhat), n_features))\n",
        "  x_input = np.concatenate([x_input, yhat], axis=1)\n",
        "  i += 1\n",
        "  if i>20:\n",
        "    break"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            "[[74. 70. 65. 58.]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 6ms/step\n",
            "Predicted Output:  [[74.54715  69.76618  63.121742 56.327858]]\n",
            "expected:  [74. 70. 65. 58.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[77.23478  73.9541   68.32366  62.923805]]\n",
            "expected:  [74. 70. 65. 58.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[79.70574  76.36403  69.889984 65.56413 ]]\n",
            "expected:  [75. 70. 58. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[81.360855 77.60397  70.846245 66.62571 ]]\n",
            "expected:  [75. 70. 58. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[82.44376  78.37003  71.6061   67.100395]]\n",
            "expected:  [75. 70. 60. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[83.23671 78.86662 72.21388 67.19486]]\n",
            "expected:  [75. 70. 60. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[83.88913  79.200356 72.71889  66.99939 ]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[84.47743 79.42563 73.18292 66.63158]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[85.0241   79.571594 73.62938  66.17026 ]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[85.52838  79.657135 74.051544 65.68278 ]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[85.9681   79.700615 74.42506  65.21108 ]]\n",
            "expected:  [77. 70. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[86.318146 79.72244  74.72584  64.78165 ]]\n",
            "expected:  [77. 70. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 5ms/step\n",
            "Predicted Output:  [[86.56785  79.73961  74.95093  64.406975]]\n",
            "expected:  [77. 69. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]\n",
            " [86.56784201 79.73960853 74.95092535 64.4069792 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[86.730415 79.75778  75.10298  64.07971 ]]\n",
            "expected:  [77. 69. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]\n",
            " [86.56784201 79.73960853 74.95092535 64.4069792 ]\n",
            " [86.73041821 79.7577821  75.10298157 64.07971442]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[86.82736  79.782585 75.206726 63.79483 ]]\n",
            "expected:  [75. 67. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]\n",
            " [86.56784201 79.73960853 74.95092535 64.4069792 ]\n",
            " [86.73041821 79.7577821  75.10298157 64.07971442]\n",
            " [86.82736397 79.78258502 75.20672369 63.79483306]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[86.8788   79.82025  75.282196 63.53804 ]]\n",
            "expected:  [75. 67. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]\n",
            " [86.56784201 79.73960853 74.95092535 64.4069792 ]\n",
            " [86.73041821 79.7577821  75.10298157 64.07971442]\n",
            " [86.82736397 79.78258502 75.20672369 63.79483306]\n",
            " [86.87880552 79.82025075 75.28220034 63.5380398 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 5ms/step\n",
            "Predicted Output:  [[86.90543 79.8703  75.3465  63.31592]]\n",
            "expected:  [75. 69. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]\n",
            " [86.56784201 79.73960853 74.95092535 64.4069792 ]\n",
            " [86.73041821 79.7577821  75.10298157 64.07971442]\n",
            " [86.82736397 79.78258502 75.20672369 63.79483306]\n",
            " [86.87880552 79.82025075 75.28220034 63.5380398 ]\n",
            " [86.90543664 79.87029326 75.34649611 63.31592298]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[86.919495 79.931496 75.40709  63.12721 ]]\n",
            "expected:  [75. 69. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]\n",
            " [86.56784201 79.73960853 74.95092535 64.4069792 ]\n",
            " [86.73041821 79.7577821  75.10298157 64.07971442]\n",
            " [86.82736397 79.78258502 75.20672369 63.79483306]\n",
            " [86.87880552 79.82025075 75.28220034 63.5380398 ]\n",
            " [86.90543664 79.87029326 75.34649611 63.31592298]\n",
            " [86.91949034 79.93149483 75.40709114 63.1272099 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[86.92838  79.99997  75.46632  62.970604]]\n",
            "expected:  [74. 70. 65. 46.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]\n",
            " [86.56784201 79.73960853 74.95092535 64.4069792 ]\n",
            " [86.73041821 79.7577821  75.10298157 64.07971442]\n",
            " [86.82736397 79.78258502 75.20672369 63.79483306]\n",
            " [86.87880552 79.82025075 75.28220034 63.5380398 ]\n",
            " [86.90543664 79.87029326 75.34649611 63.31592298]\n",
            " [86.91949034 79.93149483 75.40709114 63.1272099 ]\n",
            " [86.92838669 79.99997163 75.46631861 62.97060156]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 5ms/step\n",
            "Predicted Output:  [[86.93542  80.07022  75.52325  62.843777]]\n",
            "expected:  [74. 70. 65. 46.]\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjgMqOxYXRvf",
        "colab_type": "code",
        "outputId": "4edd6f20-fe44-4253-9ead-b25cfb4adb42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Dense, SimpleRNN\n",
        "\n",
        "# Using SimpleRNN \n",
        "print(n_features)\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(TimeDistributed(Dense(128), input_shape=(None, n_features)))\n",
        "model_rnn.add(SimpleRNN(100, input_shape=[None, n_features], return_sequences=True ))\n",
        "model_rnn.add(SimpleRNN(100))\n",
        "model_rnn.add(Dense(n_features))\n",
        "\n",
        "model_rnn.compile(optimizer='adam', loss='mse')\n",
        "model_rnn.fit(X, y, epochs=200, validation_data=(validX, validY))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "Train on 191 samples, validate on 195 samples\n",
            "Epoch 1/200\n",
            "191/191 [==============================] - 12s 63ms/step - loss: 0.1649 - val_loss: 0.0683\n",
            "Epoch 2/200\n",
            "191/191 [==============================] - 0s 223us/step - loss: 0.0653 - val_loss: 0.0404\n",
            "Epoch 3/200\n",
            "191/191 [==============================] - 0s 208us/step - loss: 0.0267 - val_loss: 0.0297\n",
            "Epoch 4/200\n",
            "191/191 [==============================] - 0s 240us/step - loss: 0.0278 - val_loss: 0.0274\n",
            "Epoch 5/200\n",
            "191/191 [==============================] - 0s 205us/step - loss: 0.0179 - val_loss: 0.0185\n",
            "Epoch 6/200\n",
            "191/191 [==============================] - 0s 206us/step - loss: 0.0167 - val_loss: 0.0196\n",
            "Epoch 7/200\n",
            "191/191 [==============================] - 0s 227us/step - loss: 0.0153 - val_loss: 0.0170\n",
            "Epoch 8/200\n",
            "191/191 [==============================] - 0s 215us/step - loss: 0.0151 - val_loss: 0.0183\n",
            "Epoch 9/200\n",
            "191/191 [==============================] - 0s 210us/step - loss: 0.0148 - val_loss: 0.0167\n",
            "Epoch 10/200\n",
            "191/191 [==============================] - 0s 194us/step - loss: 0.0143 - val_loss: 0.0173\n",
            "Epoch 11/200\n",
            "191/191 [==============================] - 0s 244us/step - loss: 0.0144 - val_loss: 0.0165\n",
            "Epoch 12/200\n",
            "191/191 [==============================] - 0s 186us/step - loss: 0.0142 - val_loss: 0.0165\n",
            "Epoch 13/200\n",
            "191/191 [==============================] - 0s 213us/step - loss: 0.0141 - val_loss: 0.0166\n",
            "Epoch 14/200\n",
            "191/191 [==============================] - 0s 225us/step - loss: 0.0142 - val_loss: 0.0166\n",
            "Epoch 15/200\n",
            "191/191 [==============================] - 0s 235us/step - loss: 0.0139 - val_loss: 0.0170\n",
            "Epoch 16/200\n",
            "191/191 [==============================] - 0s 252us/step - loss: 0.0144 - val_loss: 0.0169\n",
            "Epoch 17/200\n",
            "191/191 [==============================] - 0s 202us/step - loss: 0.0145 - val_loss: 0.0181\n",
            "Epoch 18/200\n",
            "191/191 [==============================] - 0s 210us/step - loss: 0.0142 - val_loss: 0.0164\n",
            "Epoch 19/200\n",
            "191/191 [==============================] - 0s 189us/step - loss: 0.0141 - val_loss: 0.0170\n",
            "Epoch 20/200\n",
            "191/191 [==============================] - 0s 232us/step - loss: 0.0142 - val_loss: 0.0168\n",
            "Epoch 21/200\n",
            "191/191 [==============================] - 0s 218us/step - loss: 0.0140 - val_loss: 0.0164\n",
            "Epoch 22/200\n",
            "191/191 [==============================] - 0s 245us/step - loss: 0.0141 - val_loss: 0.0168\n",
            "Epoch 23/200\n",
            "191/191 [==============================] - 0s 244us/step - loss: 0.0141 - val_loss: 0.0165\n",
            "Epoch 24/200\n",
            "191/191 [==============================] - 0s 270us/step - loss: 0.0141 - val_loss: 0.0170\n",
            "Epoch 25/200\n",
            "191/191 [==============================] - 0s 249us/step - loss: 0.0139 - val_loss: 0.0173\n",
            "Epoch 26/200\n",
            "191/191 [==============================] - 0s 263us/step - loss: 0.0142 - val_loss: 0.0174\n",
            "Epoch 27/200\n",
            "191/191 [==============================] - 0s 202us/step - loss: 0.0145 - val_loss: 0.0172\n",
            "Epoch 28/200\n",
            "191/191 [==============================] - 0s 247us/step - loss: 0.0145 - val_loss: 0.0173\n",
            "Epoch 29/200\n",
            "191/191 [==============================] - 0s 244us/step - loss: 0.0142 - val_loss: 0.0166\n",
            "Epoch 30/200\n",
            "191/191 [==============================] - 0s 267us/step - loss: 0.0145 - val_loss: 0.0176\n",
            "Epoch 31/200\n",
            "191/191 [==============================] - 0s 228us/step - loss: 0.0147 - val_loss: 0.0166\n",
            "Epoch 32/200\n",
            "191/191 [==============================] - 0s 221us/step - loss: 0.0151 - val_loss: 0.0172\n",
            "Epoch 33/200\n",
            "191/191 [==============================] - 0s 236us/step - loss: 0.0144 - val_loss: 0.0182\n",
            "Epoch 34/200\n",
            "191/191 [==============================] - 0s 271us/step - loss: 0.0148 - val_loss: 0.0165\n",
            "Epoch 35/200\n",
            "191/191 [==============================] - 0s 212us/step - loss: 0.0140 - val_loss: 0.0172\n",
            "Epoch 36/200\n",
            "191/191 [==============================] - 0s 199us/step - loss: 0.0143 - val_loss: 0.0163\n",
            "Epoch 37/200\n",
            "191/191 [==============================] - 0s 255us/step - loss: 0.0140 - val_loss: 0.0167\n",
            "Epoch 38/200\n",
            "191/191 [==============================] - 0s 234us/step - loss: 0.0140 - val_loss: 0.0166\n",
            "Epoch 39/200\n",
            "191/191 [==============================] - 0s 219us/step - loss: 0.0141 - val_loss: 0.0166\n",
            "Epoch 40/200\n",
            "191/191 [==============================] - 0s 272us/step - loss: 0.0142 - val_loss: 0.0169\n",
            "Epoch 41/200\n",
            "191/191 [==============================] - 0s 276us/step - loss: 0.0146 - val_loss: 0.0172\n",
            "Epoch 42/200\n",
            "191/191 [==============================] - 0s 257us/step - loss: 0.0149 - val_loss: 0.0182\n",
            "Epoch 43/200\n",
            "191/191 [==============================] - 0s 259us/step - loss: 0.0150 - val_loss: 0.0167\n",
            "Epoch 44/200\n",
            "191/191 [==============================] - 0s 213us/step - loss: 0.0147 - val_loss: 0.0166\n",
            "Epoch 45/200\n",
            "191/191 [==============================] - 0s 297us/step - loss: 0.0139 - val_loss: 0.0178\n",
            "Epoch 46/200\n",
            "191/191 [==============================] - 0s 291us/step - loss: 0.0143 - val_loss: 0.0169\n",
            "Epoch 47/200\n",
            "191/191 [==============================] - 0s 258us/step - loss: 0.0146 - val_loss: 0.0174\n",
            "Epoch 48/200\n",
            "191/191 [==============================] - 0s 266us/step - loss: 0.0145 - val_loss: 0.0170\n",
            "Epoch 49/200\n",
            "191/191 [==============================] - 0s 265us/step - loss: 0.0144 - val_loss: 0.0166\n",
            "Epoch 50/200\n",
            "191/191 [==============================] - 0s 234us/step - loss: 0.0143 - val_loss: 0.0164\n",
            "Epoch 51/200\n",
            "191/191 [==============================] - 0s 221us/step - loss: 0.0144 - val_loss: 0.0168\n",
            "Epoch 52/200\n",
            "191/191 [==============================] - 0s 287us/step - loss: 0.0142 - val_loss: 0.0167\n",
            "Epoch 53/200\n",
            "191/191 [==============================] - 0s 236us/step - loss: 0.0142 - val_loss: 0.0166\n",
            "Epoch 54/200\n",
            "191/191 [==============================] - 0s 262us/step - loss: 0.0142 - val_loss: 0.0174\n",
            "Epoch 55/200\n",
            "191/191 [==============================] - 0s 300us/step - loss: 0.0145 - val_loss: 0.0167\n",
            "Epoch 56/200\n",
            "191/191 [==============================] - 0s 265us/step - loss: 0.0142 - val_loss: 0.0171\n",
            "Epoch 57/200\n",
            "191/191 [==============================] - 0s 265us/step - loss: 0.0141 - val_loss: 0.0173\n",
            "Epoch 58/200\n",
            "191/191 [==============================] - 0s 306us/step - loss: 0.0142 - val_loss: 0.0169\n",
            "Epoch 59/200\n",
            "191/191 [==============================] - 0s 216us/step - loss: 0.0144 - val_loss: 0.0173\n",
            "Epoch 60/200\n",
            "191/191 [==============================] - 0s 255us/step - loss: 0.0143 - val_loss: 0.0164\n",
            "Epoch 61/200\n",
            "191/191 [==============================] - 0s 219us/step - loss: 0.0144 - val_loss: 0.0164\n",
            "Epoch 62/200\n",
            "191/191 [==============================] - 0s 262us/step - loss: 0.0143 - val_loss: 0.0174\n",
            "Epoch 63/200\n",
            "191/191 [==============================] - 0s 228us/step - loss: 0.0143 - val_loss: 0.0168\n",
            "Epoch 64/200\n",
            "191/191 [==============================] - 0s 212us/step - loss: 0.0142 - val_loss: 0.0172\n",
            "Epoch 65/200\n",
            "191/191 [==============================] - 0s 209us/step - loss: 0.0141 - val_loss: 0.0167\n",
            "Epoch 66/200\n",
            "191/191 [==============================] - 0s 204us/step - loss: 0.0140 - val_loss: 0.0166\n",
            "Epoch 67/200\n",
            "191/191 [==============================] - 0s 211us/step - loss: 0.0140 - val_loss: 0.0167\n",
            "Epoch 68/200\n",
            "191/191 [==============================] - 0s 226us/step - loss: 0.0141 - val_loss: 0.0166\n",
            "Epoch 69/200\n",
            "191/191 [==============================] - 0s 209us/step - loss: 0.0141 - val_loss: 0.0172\n",
            "Epoch 70/200\n",
            "191/191 [==============================] - 0s 204us/step - loss: 0.0142 - val_loss: 0.0165\n",
            "Epoch 71/200\n",
            "191/191 [==============================] - 0s 222us/step - loss: 0.0141 - val_loss: 0.0170\n",
            "Epoch 72/200\n",
            "191/191 [==============================] - 0s 192us/step - loss: 0.0140 - val_loss: 0.0168\n",
            "Epoch 73/200\n",
            "191/191 [==============================] - 0s 251us/step - loss: 0.0141 - val_loss: 0.0166\n",
            "Epoch 74/200\n",
            "191/191 [==============================] - 0s 221us/step - loss: 0.0140 - val_loss: 0.0167\n",
            "Epoch 75/200\n",
            "191/191 [==============================] - 0s 233us/step - loss: 0.0141 - val_loss: 0.0167\n",
            "Epoch 76/200\n",
            "191/191 [==============================] - 0s 230us/step - loss: 0.0140 - val_loss: 0.0171\n",
            "Epoch 77/200\n",
            "191/191 [==============================] - 0s 204us/step - loss: 0.0140 - val_loss: 0.0166\n",
            "Epoch 78/200\n",
            "191/191 [==============================] - 0s 206us/step - loss: 0.0140 - val_loss: 0.0169\n",
            "Epoch 79/200\n",
            "191/191 [==============================] - 0s 214us/step - loss: 0.0139 - val_loss: 0.0169\n",
            "Epoch 80/200\n",
            "191/191 [==============================] - 0s 212us/step - loss: 0.0144 - val_loss: 0.0177\n",
            "Epoch 81/200\n",
            "191/191 [==============================] - 0s 222us/step - loss: 0.0144 - val_loss: 0.0170\n",
            "Epoch 82/200\n",
            "191/191 [==============================] - 0s 223us/step - loss: 0.0141 - val_loss: 0.0167\n",
            "Epoch 83/200\n",
            "191/191 [==============================] - 0s 250us/step - loss: 0.0139 - val_loss: 0.0165\n",
            "Epoch 84/200\n",
            "191/191 [==============================] - 0s 215us/step - loss: 0.0140 - val_loss: 0.0173\n",
            "Epoch 85/200\n",
            "191/191 [==============================] - 0s 263us/step - loss: 0.0146 - val_loss: 0.0170\n",
            "Epoch 86/200\n",
            "191/191 [==============================] - 0s 235us/step - loss: 0.0146 - val_loss: 0.0181\n",
            "Epoch 87/200\n",
            "191/191 [==============================] - 0s 253us/step - loss: 0.0152 - val_loss: 0.0169\n",
            "Epoch 88/200\n",
            "191/191 [==============================] - 0s 206us/step - loss: 0.0148 - val_loss: 0.0176\n",
            "Epoch 89/200\n",
            "191/191 [==============================] - 0s 222us/step - loss: 0.0144 - val_loss: 0.0169\n",
            "Epoch 90/200\n",
            "191/191 [==============================] - 0s 212us/step - loss: 0.0145 - val_loss: 0.0169\n",
            "Epoch 91/200\n",
            "191/191 [==============================] - 0s 229us/step - loss: 0.0143 - val_loss: 0.0176\n",
            "Epoch 92/200\n",
            "191/191 [==============================] - 0s 208us/step - loss: 0.0145 - val_loss: 0.0168\n",
            "Epoch 93/200\n",
            "191/191 [==============================] - 0s 243us/step - loss: 0.0143 - val_loss: 0.0170\n",
            "Epoch 94/200\n",
            "191/191 [==============================] - 0s 212us/step - loss: 0.0144 - val_loss: 0.0168\n",
            "Epoch 95/200\n",
            "191/191 [==============================] - 0s 226us/step - loss: 0.0142 - val_loss: 0.0167\n",
            "Epoch 96/200\n",
            "191/191 [==============================] - 0s 220us/step - loss: 0.0142 - val_loss: 0.0166\n",
            "Epoch 97/200\n",
            "191/191 [==============================] - 0s 179us/step - loss: 0.0140 - val_loss: 0.0172\n",
            "Epoch 98/200\n",
            "191/191 [==============================] - 0s 182us/step - loss: 0.0141 - val_loss: 0.0168\n",
            "Epoch 99/200\n",
            "191/191 [==============================] - 0s 205us/step - loss: 0.0141 - val_loss: 0.0166\n",
            "Epoch 100/200\n",
            "191/191 [==============================] - 0s 210us/step - loss: 0.0142 - val_loss: 0.0170\n",
            "Epoch 101/200\n",
            "191/191 [==============================] - 0s 230us/step - loss: 0.0142 - val_loss: 0.0172\n",
            "Epoch 102/200\n",
            "191/191 [==============================] - 0s 201us/step - loss: 0.0141 - val_loss: 0.0169\n",
            "Epoch 103/200\n",
            "191/191 [==============================] - 0s 194us/step - loss: 0.0141 - val_loss: 0.0170\n",
            "Epoch 104/200\n",
            "191/191 [==============================] - 0s 214us/step - loss: 0.0144 - val_loss: 0.0168\n",
            "Epoch 105/200\n",
            "191/191 [==============================] - 0s 244us/step - loss: 0.0143 - val_loss: 0.0171\n",
            "Epoch 106/200\n",
            "191/191 [==============================] - 0s 216us/step - loss: 0.0146 - val_loss: 0.0173\n",
            "Epoch 107/200\n",
            "191/191 [==============================] - 0s 197us/step - loss: 0.0148 - val_loss: 0.0177\n",
            "Epoch 108/200\n",
            "191/191 [==============================] - 0s 227us/step - loss: 0.0144 - val_loss: 0.0190\n",
            "Epoch 109/200\n",
            "191/191 [==============================] - 0s 199us/step - loss: 0.0150 - val_loss: 0.0176\n",
            "Epoch 110/200\n",
            "191/191 [==============================] - 0s 240us/step - loss: 0.0146 - val_loss: 0.0173\n",
            "Epoch 111/200\n",
            "191/191 [==============================] - 0s 261us/step - loss: 0.0143 - val_loss: 0.0169\n",
            "Epoch 112/200\n",
            "191/191 [==============================] - 0s 223us/step - loss: 0.0141 - val_loss: 0.0172\n",
            "Epoch 113/200\n",
            "191/191 [==============================] - 0s 207us/step - loss: 0.0141 - val_loss: 0.0171\n",
            "Epoch 114/200\n",
            "191/191 [==============================] - 0s 256us/step - loss: 0.0142 - val_loss: 0.0168\n",
            "Epoch 115/200\n",
            "191/191 [==============================] - 0s 228us/step - loss: 0.0139 - val_loss: 0.0170\n",
            "Epoch 116/200\n",
            "191/191 [==============================] - 0s 226us/step - loss: 0.0141 - val_loss: 0.0169\n",
            "Epoch 117/200\n",
            "191/191 [==============================] - 0s 197us/step - loss: 0.0143 - val_loss: 0.0172\n",
            "Epoch 118/200\n",
            "191/191 [==============================] - 0s 221us/step - loss: 0.0142 - val_loss: 0.0166\n",
            "Epoch 119/200\n",
            "191/191 [==============================] - 0s 192us/step - loss: 0.0143 - val_loss: 0.0168\n",
            "Epoch 120/200\n",
            "191/191 [==============================] - 0s 222us/step - loss: 0.0141 - val_loss: 0.0169\n",
            "Epoch 121/200\n",
            "191/191 [==============================] - 0s 225us/step - loss: 0.0144 - val_loss: 0.0169\n",
            "Epoch 122/200\n",
            "191/191 [==============================] - 0s 238us/step - loss: 0.0139 - val_loss: 0.0166\n",
            "Epoch 123/200\n",
            "191/191 [==============================] - 0s 273us/step - loss: 0.0138 - val_loss: 0.0166\n",
            "Epoch 124/200\n",
            "191/191 [==============================] - 0s 274us/step - loss: 0.0141 - val_loss: 0.0166\n",
            "Epoch 125/200\n",
            "191/191 [==============================] - 0s 220us/step - loss: 0.0141 - val_loss: 0.0166\n",
            "Epoch 126/200\n",
            "191/191 [==============================] - 0s 222us/step - loss: 0.0141 - val_loss: 0.0168\n",
            "Epoch 127/200\n",
            "191/191 [==============================] - 0s 263us/step - loss: 0.0140 - val_loss: 0.0170\n",
            "Epoch 128/200\n",
            "191/191 [==============================] - 0s 278us/step - loss: 0.0140 - val_loss: 0.0172\n",
            "Epoch 129/200\n",
            "191/191 [==============================] - 0s 333us/step - loss: 0.0141 - val_loss: 0.0168\n",
            "Epoch 130/200\n",
            "191/191 [==============================] - 0s 199us/step - loss: 0.0141 - val_loss: 0.0171\n",
            "Epoch 131/200\n",
            "191/191 [==============================] - 0s 231us/step - loss: 0.0141 - val_loss: 0.0165\n",
            "Epoch 132/200\n",
            "191/191 [==============================] - 0s 261us/step - loss: 0.0140 - val_loss: 0.0172\n",
            "Epoch 133/200\n",
            "191/191 [==============================] - 0s 249us/step - loss: 0.0143 - val_loss: 0.0170\n",
            "Epoch 134/200\n",
            "191/191 [==============================] - 0s 242us/step - loss: 0.0145 - val_loss: 0.0174\n",
            "Epoch 135/200\n",
            "191/191 [==============================] - 0s 227us/step - loss: 0.0150 - val_loss: 0.0167\n",
            "Epoch 136/200\n",
            "191/191 [==============================] - 0s 223us/step - loss: 0.0146 - val_loss: 0.0167\n",
            "Epoch 137/200\n",
            "191/191 [==============================] - 0s 195us/step - loss: 0.0141 - val_loss: 0.0169\n",
            "Epoch 138/200\n",
            "191/191 [==============================] - 0s 211us/step - loss: 0.0141 - val_loss: 0.0168\n",
            "Epoch 139/200\n",
            "191/191 [==============================] - 0s 246us/step - loss: 0.0144 - val_loss: 0.0168\n",
            "Epoch 140/200\n",
            "191/191 [==============================] - 0s 272us/step - loss: 0.0143 - val_loss: 0.0166\n",
            "Epoch 141/200\n",
            "191/191 [==============================] - 0s 208us/step - loss: 0.0143 - val_loss: 0.0164\n",
            "Epoch 142/200\n",
            "191/191 [==============================] - 0s 239us/step - loss: 0.0139 - val_loss: 0.0166\n",
            "Epoch 143/200\n",
            "191/191 [==============================] - 0s 215us/step - loss: 0.0139 - val_loss: 0.0169\n",
            "Epoch 144/200\n",
            "191/191 [==============================] - 0s 259us/step - loss: 0.0138 - val_loss: 0.0168\n",
            "Epoch 145/200\n",
            "191/191 [==============================] - 0s 236us/step - loss: 0.0139 - val_loss: 0.0168\n",
            "Epoch 146/200\n",
            "191/191 [==============================] - 0s 252us/step - loss: 0.0140 - val_loss: 0.0165\n",
            "Epoch 147/200\n",
            "191/191 [==============================] - 0s 260us/step - loss: 0.0141 - val_loss: 0.0167\n",
            "Epoch 148/200\n",
            "191/191 [==============================] - 0s 260us/step - loss: 0.0145 - val_loss: 0.0167\n",
            "Epoch 149/200\n",
            "191/191 [==============================] - 0s 290us/step - loss: 0.0145 - val_loss: 0.0169\n",
            "Epoch 150/200\n",
            "191/191 [==============================] - 0s 236us/step - loss: 0.0143 - val_loss: 0.0174\n",
            "Epoch 151/200\n",
            "191/191 [==============================] - 0s 201us/step - loss: 0.0148 - val_loss: 0.0167\n",
            "Epoch 152/200\n",
            "191/191 [==============================] - 0s 242us/step - loss: 0.0146 - val_loss: 0.0167\n",
            "Epoch 153/200\n",
            "191/191 [==============================] - 0s 233us/step - loss: 0.0141 - val_loss: 0.0164\n",
            "Epoch 154/200\n",
            "191/191 [==============================] - 0s 238us/step - loss: 0.0145 - val_loss: 0.0167\n",
            "Epoch 155/200\n",
            "191/191 [==============================] - 0s 210us/step - loss: 0.0139 - val_loss: 0.0168\n",
            "Epoch 156/200\n",
            "191/191 [==============================] - 0s 215us/step - loss: 0.0142 - val_loss: 0.0168\n",
            "Epoch 157/200\n",
            "191/191 [==============================] - 0s 210us/step - loss: 0.0141 - val_loss: 0.0169\n",
            "Epoch 158/200\n",
            "191/191 [==============================] - 0s 197us/step - loss: 0.0141 - val_loss: 0.0164\n",
            "Epoch 159/200\n",
            "191/191 [==============================] - 0s 218us/step - loss: 0.0139 - val_loss: 0.0166\n",
            "Epoch 160/200\n",
            "191/191 [==============================] - 0s 203us/step - loss: 0.0140 - val_loss: 0.0173\n",
            "Epoch 161/200\n",
            "191/191 [==============================] - 0s 205us/step - loss: 0.0140 - val_loss: 0.0167\n",
            "Epoch 162/200\n",
            "191/191 [==============================] - 0s 221us/step - loss: 0.0138 - val_loss: 0.0165\n",
            "Epoch 163/200\n",
            "191/191 [==============================] - 0s 210us/step - loss: 0.0139 - val_loss: 0.0163\n",
            "Epoch 164/200\n",
            "191/191 [==============================] - 0s 217us/step - loss: 0.0138 - val_loss: 0.0167\n",
            "Epoch 165/200\n",
            "191/191 [==============================] - 0s 269us/step - loss: 0.0139 - val_loss: 0.0170\n",
            "Epoch 166/200\n",
            "191/191 [==============================] - 0s 246us/step - loss: 0.0138 - val_loss: 0.0168\n",
            "Epoch 167/200\n",
            "191/191 [==============================] - 0s 219us/step - loss: 0.0138 - val_loss: 0.0165\n",
            "Epoch 168/200\n",
            "191/191 [==============================] - 0s 237us/step - loss: 0.0140 - val_loss: 0.0165\n",
            "Epoch 169/200\n",
            "191/191 [==============================] - 0s 207us/step - loss: 0.0143 - val_loss: 0.0166\n",
            "Epoch 170/200\n",
            "191/191 [==============================] - 0s 242us/step - loss: 0.0140 - val_loss: 0.0169\n",
            "Epoch 171/200\n",
            "191/191 [==============================] - 0s 205us/step - loss: 0.0144 - val_loss: 0.0170\n",
            "Epoch 172/200\n",
            "191/191 [==============================] - 0s 222us/step - loss: 0.0141 - val_loss: 0.0167\n",
            "Epoch 173/200\n",
            "191/191 [==============================] - 0s 230us/step - loss: 0.0140 - val_loss: 0.0167\n",
            "Epoch 174/200\n",
            "191/191 [==============================] - 0s 227us/step - loss: 0.0142 - val_loss: 0.0165\n",
            "Epoch 175/200\n",
            "191/191 [==============================] - 0s 230us/step - loss: 0.0141 - val_loss: 0.0171\n",
            "Epoch 176/200\n",
            "191/191 [==============================] - 0s 252us/step - loss: 0.0140 - val_loss: 0.0167\n",
            "Epoch 177/200\n",
            "191/191 [==============================] - 0s 258us/step - loss: 0.0144 - val_loss: 0.0172\n",
            "Epoch 178/200\n",
            "191/191 [==============================] - 0s 251us/step - loss: 0.0142 - val_loss: 0.0166\n",
            "Epoch 179/200\n",
            "191/191 [==============================] - 0s 273us/step - loss: 0.0143 - val_loss: 0.0166\n",
            "Epoch 180/200\n",
            "191/191 [==============================] - 0s 204us/step - loss: 0.0139 - val_loss: 0.0166\n",
            "Epoch 181/200\n",
            "191/191 [==============================] - 0s 231us/step - loss: 0.0140 - val_loss: 0.0168\n",
            "Epoch 182/200\n",
            "191/191 [==============================] - 0s 213us/step - loss: 0.0142 - val_loss: 0.0171\n",
            "Epoch 183/200\n",
            "191/191 [==============================] - 0s 238us/step - loss: 0.0141 - val_loss: 0.0167\n",
            "Epoch 184/200\n",
            "191/191 [==============================] - 0s 226us/step - loss: 0.0142 - val_loss: 0.0175\n",
            "Epoch 185/200\n",
            "191/191 [==============================] - 0s 262us/step - loss: 0.0144 - val_loss: 0.0168\n",
            "Epoch 186/200\n",
            "191/191 [==============================] - 0s 189us/step - loss: 0.0142 - val_loss: 0.0173\n",
            "Epoch 187/200\n",
            "191/191 [==============================] - 0s 228us/step - loss: 0.0139 - val_loss: 0.0172\n",
            "Epoch 188/200\n",
            "191/191 [==============================] - 0s 250us/step - loss: 0.0141 - val_loss: 0.0167\n",
            "Epoch 189/200\n",
            "191/191 [==============================] - 0s 225us/step - loss: 0.0142 - val_loss: 0.0169\n",
            "Epoch 190/200\n",
            "191/191 [==============================] - 0s 222us/step - loss: 0.0139 - val_loss: 0.0166\n",
            "Epoch 191/200\n",
            "191/191 [==============================] - 0s 285us/step - loss: 0.0141 - val_loss: 0.0167\n",
            "Epoch 192/200\n",
            "191/191 [==============================] - 0s 209us/step - loss: 0.0141 - val_loss: 0.0169\n",
            "Epoch 193/200\n",
            "191/191 [==============================] - 0s 215us/step - loss: 0.0140 - val_loss: 0.0167\n",
            "Epoch 194/200\n",
            "191/191 [==============================] - 0s 241us/step - loss: 0.0139 - val_loss: 0.0166\n",
            "Epoch 195/200\n",
            "191/191 [==============================] - 0s 223us/step - loss: 0.0139 - val_loss: 0.0170\n",
            "Epoch 196/200\n",
            "191/191 [==============================] - 0s 283us/step - loss: 0.0141 - val_loss: 0.0167\n",
            "Epoch 197/200\n",
            "191/191 [==============================] - 0s 230us/step - loss: 0.0138 - val_loss: 0.0168\n",
            "Epoch 198/200\n",
            "191/191 [==============================] - 0s 258us/step - loss: 0.0141 - val_loss: 0.0171\n",
            "Epoch 199/200\n",
            "191/191 [==============================] - 0s 221us/step - loss: 0.0141 - val_loss: 0.0167\n",
            "Epoch 200/200\n",
            "191/191 [==============================] - 0s 217us/step - loss: 0.0140 - val_loss: 0.0165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f70e9b80748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "35765474-f502-4d42-9e12-b81b9ed24094",
        "id": "JTCPTgyb5q0s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# demonstrate prediction\n",
        "x_input = dataXScaler[2:30]\n",
        "x_input = x_input.reshape((1, len(x_input), 4))\n",
        "print(scaler.inverse_transform(dataXScaler)[2:30])\n",
        "yhat = model_rnn.predict(x_input, verbose=0)\n",
        "print(scaler.inverse_transform(yhat))\n",
        "print('expected: ', scaler.inverse_transform(dataYScaler)[30])"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]]\n",
            "[[75.66913 67.57323 61.02341 40.88458]]\n",
            "expected:  [72. 69. 65. 53.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbHx1t9X6nb4",
        "colab_type": "code",
        "outputId": "10eef5de-d62e-48d4-9f96-156fa824f37c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# choose a number of time steps\n",
        "n_steps = None\n",
        "# convert into input/output\n",
        "i=1\n",
        "n=1\n",
        "x_input = np.array([dataXScaler[0]])\n",
        "\n",
        "x_input = x_input.reshape((1, len(x_input), n_features))\n",
        "while i<len(dataXScaler):\n",
        "  # demonstrate prediction\n",
        "  print('Input: ')\n",
        "  for input in x_input:\n",
        "    print(scaler.inverse_transform(input))\n",
        "  print('---------------')\n",
        "  yhat = model.predict(x_input, verbose=1)\n",
        "  print('Predicted Output: ', scaler.inverse_transform(yhat))\n",
        "  print('expected: ', scaler.inverse_transform(dataYScaler)[i])\n",
        "  print('\\n\\n')\n",
        "\n",
        "  yhat = yhat.reshape((1, len(yhat), n_features))\n",
        "  x_input = np.concatenate([x_input, yhat], axis=1)\n",
        "  i += 1\n",
        "  if i>20:\n",
        "    break"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            "[[74. 70. 65. 58.]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[74.54715  69.76618  63.121742 56.327858]]\n",
            "expected:  [74. 70. 65. 58.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[77.23478  73.9541   68.32366  62.923805]]\n",
            "expected:  [74. 70. 65. 58.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[79.70574  76.36403  69.889984 65.56413 ]]\n",
            "expected:  [75. 70. 58. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[81.360855 77.60397  70.846245 66.62571 ]]\n",
            "expected:  [75. 70. 58. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[82.44376  78.37003  71.6061   67.100395]]\n",
            "expected:  [75. 70. 60. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[83.23671 78.86662 72.21388 67.19486]]\n",
            "expected:  [75. 70. 60. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[83.88913  79.200356 72.71889  66.99939 ]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "Predicted Output:  [[84.47743 79.42563 73.18292 66.63158]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[85.0241   79.571594 73.62938  66.17026 ]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 5ms/step\n",
            "Predicted Output:  [[85.52838  79.657135 74.051544 65.68278 ]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[85.9681   79.700615 74.42506  65.21108 ]]\n",
            "expected:  [77. 70. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[86.318146 79.72244  74.72584  64.78165 ]]\n",
            "expected:  [77. 70. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 6ms/step\n",
            "Predicted Output:  [[86.56785  79.73961  74.95093  64.406975]]\n",
            "expected:  [77. 69. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]\n",
            " [86.56784201 79.73960853 74.95092535 64.4069792 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 5ms/step\n",
            "Predicted Output:  [[86.730415 79.75778  75.10298  64.07971 ]]\n",
            "expected:  [77. 69. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]\n",
            " [86.56784201 79.73960853 74.95092535 64.4069792 ]\n",
            " [86.73041821 79.7577821  75.10298157 64.07971442]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 5ms/step\n",
            "Predicted Output:  [[86.82736  79.782585 75.206726 63.79483 ]]\n",
            "expected:  [75. 67. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]\n",
            " [86.56784201 79.73960853 74.95092535 64.4069792 ]\n",
            " [86.73041821 79.7577821  75.10298157 64.07971442]\n",
            " [86.82736397 79.78258502 75.20672369 63.79483306]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 6ms/step\n",
            "Predicted Output:  [[86.8788   79.82025  75.282196 63.53804 ]]\n",
            "expected:  [75. 67. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]\n",
            " [86.56784201 79.73960853 74.95092535 64.4069792 ]\n",
            " [86.73041821 79.7577821  75.10298157 64.07971442]\n",
            " [86.82736397 79.78258502 75.20672369 63.79483306]\n",
            " [86.87880552 79.82025075 75.28220034 63.5380398 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 6ms/step\n",
            "Predicted Output:  [[86.90543 79.8703  75.3465  63.31592]]\n",
            "expected:  [75. 69. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]\n",
            " [86.56784201 79.73960853 74.95092535 64.4069792 ]\n",
            " [86.73041821 79.7577821  75.10298157 64.07971442]\n",
            " [86.82736397 79.78258502 75.20672369 63.79483306]\n",
            " [86.87880552 79.82025075 75.28220034 63.5380398 ]\n",
            " [86.90543664 79.87029326 75.34649611 63.31592298]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 5ms/step\n",
            "Predicted Output:  [[86.919495 79.931496 75.40709  63.12721 ]]\n",
            "expected:  [75. 69. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]\n",
            " [86.56784201 79.73960853 74.95092535 64.4069792 ]\n",
            " [86.73041821 79.7577821  75.10298157 64.07971442]\n",
            " [86.82736397 79.78258502 75.20672369 63.79483306]\n",
            " [86.87880552 79.82025075 75.28220034 63.5380398 ]\n",
            " [86.90543664 79.87029326 75.34649611 63.31592298]\n",
            " [86.91949034 79.93149483 75.40709114 63.1272099 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "Predicted Output:  [[86.92838  79.99997  75.46632  62.970604]]\n",
            "expected:  [74. 70. 65. 46.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.54714358 69.76617962 63.12173986 56.32785881]\n",
            " [77.23478138 73.95410085 68.3236618  62.92380428]\n",
            " [79.70574284 76.36403131 69.88998318 65.56412959]\n",
            " [81.36086214 77.60397577 70.84624434 66.62570953]\n",
            " [82.44376433 78.37003827 71.60609913 67.10039628]\n",
            " [83.23671544 78.86662996 72.21387911 67.1948663 ]\n",
            " [83.88913381 79.20035362 72.71889114 66.99938655]\n",
            " [84.47743022 79.42562807 73.18291759 66.63157845]\n",
            " [85.02410305 79.57159603 73.62937307 66.17025685]\n",
            " [85.52838516 79.65713811 74.05154276 65.6827805 ]\n",
            " [85.96810651 79.70062101 74.4250536  65.21108091]\n",
            " [86.31815088 79.72244024 74.72583723 64.78164923]\n",
            " [86.56784201 79.73960853 74.95092535 64.4069792 ]\n",
            " [86.73041821 79.7577821  75.10298157 64.07971442]\n",
            " [86.82736397 79.78258502 75.20672369 63.79483306]\n",
            " [86.87880552 79.82025075 75.28220034 63.5380398 ]\n",
            " [86.90543664 79.87029326 75.34649611 63.31592298]\n",
            " [86.91949034 79.93149483 75.40709114 63.1272099 ]\n",
            " [86.92838669 79.99997163 75.46631861 62.97060156]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 11ms/step\n",
            "Predicted Output:  [[86.93542  80.07022  75.52325  62.843777]]\n",
            "expected:  [74. 70. 65. 46.]\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx1PEBFFRy1h",
        "colab_type": "code",
        "outputId": "c3247aa2-0c1b-4691-b044-71fa8afdd900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# using GRU\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GRU, Dense\n",
        "\n",
        "# choose a number of time steps\n",
        "n_steps = None\n",
        "n_features = 4\n",
        "\n",
        "\n",
        "\n",
        "model_gru = Sequential()\n",
        "model_gru.add(TimeDistributed(Dense(128), input_shape=(None, n_features)))\n",
        "model_gru.add(GRU(64, activation='tanh', input_shape=(None, n_features), return_sequences=True))\n",
        "model_gru.add(BatchNormalization())\n",
        "\n",
        "model_gru.add(GRU(32 , activation = 'tanh'))\n",
        "model_gru.add(BatchNormalization())\n",
        "model_gru.add(Dropout(0.2))\n",
        "model_gru.add(Dense(n_features))\n",
        "model_gru.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# fit model\n",
        "#model_gru.fit_generator(generator, epochs=500, validation_data=validation_generator)\n",
        "model_gru.fit(X, y, epochs=200, validation_data=(validX, validY))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 191 samples, validate on 195 samples\n",
            "Epoch 1/200\n",
            "191/191 [==============================] - 13s 69ms/step - loss: 1.5210 - val_loss: 0.9882\n",
            "Epoch 2/200\n",
            "191/191 [==============================] - 0s 386us/step - loss: 0.8841 - val_loss: 1.2071\n",
            "Epoch 3/200\n",
            "191/191 [==============================] - 0s 352us/step - loss: 0.7568 - val_loss: 0.8654\n",
            "Epoch 4/200\n",
            "191/191 [==============================] - 0s 375us/step - loss: 0.6692 - val_loss: 0.4700\n",
            "Epoch 5/200\n",
            "191/191 [==============================] - 0s 366us/step - loss: 0.5930 - val_loss: 0.4772\n",
            "Epoch 6/200\n",
            "191/191 [==============================] - 0s 366us/step - loss: 0.5898 - val_loss: 0.4964\n",
            "Epoch 7/200\n",
            "191/191 [==============================] - 0s 400us/step - loss: 0.5719 - val_loss: 0.3791\n",
            "Epoch 8/200\n",
            "191/191 [==============================] - 0s 350us/step - loss: 0.4966 - val_loss: 0.2142\n",
            "Epoch 9/200\n",
            "191/191 [==============================] - 0s 377us/step - loss: 0.5391 - val_loss: 0.1964\n",
            "Epoch 10/200\n",
            "191/191 [==============================] - 0s 366us/step - loss: 0.4315 - val_loss: 0.1978\n",
            "Epoch 11/200\n",
            "191/191 [==============================] - 0s 374us/step - loss: 0.4556 - val_loss: 0.2998\n",
            "Epoch 12/200\n",
            "191/191 [==============================] - 0s 370us/step - loss: 0.4686 - val_loss: 0.2918\n",
            "Epoch 13/200\n",
            "191/191 [==============================] - 0s 363us/step - loss: 0.4414 - val_loss: 0.2077\n",
            "Epoch 14/200\n",
            "191/191 [==============================] - 0s 349us/step - loss: 0.4504 - val_loss: 0.1654\n",
            "Epoch 15/200\n",
            "191/191 [==============================] - 0s 400us/step - loss: 0.4343 - val_loss: 0.1984\n",
            "Epoch 16/200\n",
            "191/191 [==============================] - 0s 376us/step - loss: 0.3418 - val_loss: 0.2137\n",
            "Epoch 17/200\n",
            "191/191 [==============================] - 0s 350us/step - loss: 0.3974 - val_loss: 0.1420\n",
            "Epoch 18/200\n",
            "191/191 [==============================] - 0s 388us/step - loss: 0.4118 - val_loss: 0.1307\n",
            "Epoch 19/200\n",
            "191/191 [==============================] - 0s 335us/step - loss: 0.3925 - val_loss: 0.1091\n",
            "Epoch 20/200\n",
            "191/191 [==============================] - 0s 315us/step - loss: 0.4072 - val_loss: 0.1034\n",
            "Epoch 21/200\n",
            "191/191 [==============================] - 0s 372us/step - loss: 0.3628 - val_loss: 0.0692\n",
            "Epoch 22/200\n",
            "191/191 [==============================] - 0s 416us/step - loss: 0.3544 - val_loss: 0.0636\n",
            "Epoch 23/200\n",
            "191/191 [==============================] - 0s 377us/step - loss: 0.3405 - val_loss: 0.0806\n",
            "Epoch 24/200\n",
            "191/191 [==============================] - 0s 368us/step - loss: 0.3353 - val_loss: 0.0842\n",
            "Epoch 25/200\n",
            "191/191 [==============================] - 0s 348us/step - loss: 0.3157 - val_loss: 0.0809\n",
            "Epoch 26/200\n",
            "191/191 [==============================] - 0s 377us/step - loss: 0.3250 - val_loss: 0.0742\n",
            "Epoch 27/200\n",
            "191/191 [==============================] - 0s 365us/step - loss: 0.3013 - val_loss: 0.0775\n",
            "Epoch 28/200\n",
            "191/191 [==============================] - 0s 362us/step - loss: 0.3160 - val_loss: 0.1004\n",
            "Epoch 29/200\n",
            "191/191 [==============================] - 0s 403us/step - loss: 0.2825 - val_loss: 0.1036\n",
            "Epoch 30/200\n",
            "191/191 [==============================] - 0s 320us/step - loss: 0.3341 - val_loss: 0.0629\n",
            "Epoch 31/200\n",
            "191/191 [==============================] - 0s 358us/step - loss: 0.2607 - val_loss: 0.0676\n",
            "Epoch 32/200\n",
            "191/191 [==============================] - 0s 363us/step - loss: 0.2943 - val_loss: 0.0716\n",
            "Epoch 33/200\n",
            "191/191 [==============================] - 0s 374us/step - loss: 0.2769 - val_loss: 0.0601\n",
            "Epoch 34/200\n",
            "191/191 [==============================] - 0s 331us/step - loss: 0.2892 - val_loss: 0.0705\n",
            "Epoch 35/200\n",
            "191/191 [==============================] - 0s 399us/step - loss: 0.2826 - val_loss: 0.0728\n",
            "Epoch 36/200\n",
            "191/191 [==============================] - 0s 355us/step - loss: 0.2889 - val_loss: 0.0841\n",
            "Epoch 37/200\n",
            "191/191 [==============================] - 0s 388us/step - loss: 0.2448 - val_loss: 0.0760\n",
            "Epoch 38/200\n",
            "191/191 [==============================] - 0s 374us/step - loss: 0.2678 - val_loss: 0.0712\n",
            "Epoch 39/200\n",
            "191/191 [==============================] - 0s 361us/step - loss: 0.2483 - val_loss: 0.0794\n",
            "Epoch 40/200\n",
            "191/191 [==============================] - 0s 337us/step - loss: 0.2267 - val_loss: 0.0867\n",
            "Epoch 41/200\n",
            "191/191 [==============================] - 0s 331us/step - loss: 0.2520 - val_loss: 0.0699\n",
            "Epoch 42/200\n",
            "191/191 [==============================] - 0s 345us/step - loss: 0.2459 - val_loss: 0.0560\n",
            "Epoch 43/200\n",
            "191/191 [==============================] - 0s 509us/step - loss: 0.2564 - val_loss: 0.0873\n",
            "Epoch 44/200\n",
            "191/191 [==============================] - 0s 358us/step - loss: 0.2465 - val_loss: 0.0754\n",
            "Epoch 45/200\n",
            "191/191 [==============================] - 0s 341us/step - loss: 0.2246 - val_loss: 0.0449\n",
            "Epoch 46/200\n",
            "191/191 [==============================] - 0s 353us/step - loss: 0.2334 - val_loss: 0.0397\n",
            "Epoch 47/200\n",
            "191/191 [==============================] - 0s 343us/step - loss: 0.1953 - val_loss: 0.0386\n",
            "Epoch 48/200\n",
            "191/191 [==============================] - 0s 423us/step - loss: 0.2275 - val_loss: 0.0387\n",
            "Epoch 49/200\n",
            "191/191 [==============================] - 0s 383us/step - loss: 0.2208 - val_loss: 0.0372\n",
            "Epoch 50/200\n",
            "191/191 [==============================] - 0s 352us/step - loss: 0.1909 - val_loss: 0.0374\n",
            "Epoch 51/200\n",
            "191/191 [==============================] - 0s 354us/step - loss: 0.1935 - val_loss: 0.0457\n",
            "Epoch 52/200\n",
            "191/191 [==============================] - 0s 363us/step - loss: 0.2124 - val_loss: 0.0478\n",
            "Epoch 53/200\n",
            "191/191 [==============================] - 0s 356us/step - loss: 0.1872 - val_loss: 0.0433\n",
            "Epoch 54/200\n",
            "191/191 [==============================] - 0s 373us/step - loss: 0.2167 - val_loss: 0.0479\n",
            "Epoch 55/200\n",
            "191/191 [==============================] - 0s 401us/step - loss: 0.2005 - val_loss: 0.0428\n",
            "Epoch 56/200\n",
            "191/191 [==============================] - 0s 390us/step - loss: 0.1886 - val_loss: 0.0454\n",
            "Epoch 57/200\n",
            "191/191 [==============================] - 0s 382us/step - loss: 0.1931 - val_loss: 0.0450\n",
            "Epoch 58/200\n",
            "191/191 [==============================] - 0s 329us/step - loss: 0.1666 - val_loss: 0.0401\n",
            "Epoch 59/200\n",
            "191/191 [==============================] - 0s 364us/step - loss: 0.1739 - val_loss: 0.0414\n",
            "Epoch 60/200\n",
            "191/191 [==============================] - 0s 360us/step - loss: 0.1625 - val_loss: 0.0479\n",
            "Epoch 61/200\n",
            "191/191 [==============================] - 0s 326us/step - loss: 0.1962 - val_loss: 0.0511\n",
            "Epoch 62/200\n",
            "191/191 [==============================] - 0s 354us/step - loss: 0.1756 - val_loss: 0.0362\n",
            "Epoch 63/200\n",
            "191/191 [==============================] - 0s 338us/step - loss: 0.1718 - val_loss: 0.0319\n",
            "Epoch 64/200\n",
            "191/191 [==============================] - 0s 364us/step - loss: 0.1669 - val_loss: 0.0345\n",
            "Epoch 65/200\n",
            "191/191 [==============================] - 0s 399us/step - loss: 0.1648 - val_loss: 0.0323\n",
            "Epoch 66/200\n",
            "191/191 [==============================] - 0s 332us/step - loss: 0.1606 - val_loss: 0.0341\n",
            "Epoch 67/200\n",
            "191/191 [==============================] - 0s 327us/step - loss: 0.1473 - val_loss: 0.0348\n",
            "Epoch 68/200\n",
            "191/191 [==============================] - 0s 385us/step - loss: 0.1757 - val_loss: 0.0365\n",
            "Epoch 69/200\n",
            "191/191 [==============================] - 0s 395us/step - loss: 0.1440 - val_loss: 0.0389\n",
            "Epoch 70/200\n",
            "191/191 [==============================] - 0s 361us/step - loss: 0.1427 - val_loss: 0.0441\n",
            "Epoch 71/200\n",
            "191/191 [==============================] - 0s 344us/step - loss: 0.1462 - val_loss: 0.0323\n",
            "Epoch 72/200\n",
            "191/191 [==============================] - 0s 361us/step - loss: 0.1532 - val_loss: 0.0353\n",
            "Epoch 73/200\n",
            "191/191 [==============================] - 0s 366us/step - loss: 0.1318 - val_loss: 0.0364\n",
            "Epoch 74/200\n",
            "191/191 [==============================] - 0s 361us/step - loss: 0.1524 - val_loss: 0.0459\n",
            "Epoch 75/200\n",
            "191/191 [==============================] - 0s 356us/step - loss: 0.1551 - val_loss: 0.0426\n",
            "Epoch 76/200\n",
            "191/191 [==============================] - 0s 337us/step - loss: 0.1371 - val_loss: 0.0417\n",
            "Epoch 77/200\n",
            "191/191 [==============================] - 0s 379us/step - loss: 0.1246 - val_loss: 0.0353\n",
            "Epoch 78/200\n",
            "191/191 [==============================] - 0s 403us/step - loss: 0.1233 - val_loss: 0.0304\n",
            "Epoch 79/200\n",
            "191/191 [==============================] - 0s 449us/step - loss: 0.1185 - val_loss: 0.0261\n",
            "Epoch 80/200\n",
            "191/191 [==============================] - 0s 367us/step - loss: 0.1241 - val_loss: 0.0297\n",
            "Epoch 81/200\n",
            "191/191 [==============================] - 0s 397us/step - loss: 0.1193 - val_loss: 0.0279\n",
            "Epoch 82/200\n",
            "191/191 [==============================] - 0s 379us/step - loss: 0.1227 - val_loss: 0.0250\n",
            "Epoch 83/200\n",
            "191/191 [==============================] - 0s 354us/step - loss: 0.1072 - val_loss: 0.0273\n",
            "Epoch 84/200\n",
            "191/191 [==============================] - 0s 348us/step - loss: 0.1119 - val_loss: 0.0295\n",
            "Epoch 85/200\n",
            "191/191 [==============================] - 0s 330us/step - loss: 0.1255 - val_loss: 0.0280\n",
            "Epoch 86/200\n",
            "191/191 [==============================] - 0s 366us/step - loss: 0.1141 - val_loss: 0.0276\n",
            "Epoch 87/200\n",
            "191/191 [==============================] - 0s 350us/step - loss: 0.1145 - val_loss: 0.0292\n",
            "Epoch 88/200\n",
            "191/191 [==============================] - 0s 405us/step - loss: 0.0958 - val_loss: 0.0335\n",
            "Epoch 89/200\n",
            "191/191 [==============================] - 0s 340us/step - loss: 0.1108 - val_loss: 0.0386\n",
            "Epoch 90/200\n",
            "191/191 [==============================] - 0s 381us/step - loss: 0.1164 - val_loss: 0.0398\n",
            "Epoch 91/200\n",
            "191/191 [==============================] - 0s 353us/step - loss: 0.1125 - val_loss: 0.0388\n",
            "Epoch 92/200\n",
            "191/191 [==============================] - 0s 384us/step - loss: 0.0928 - val_loss: 0.0390\n",
            "Epoch 93/200\n",
            "191/191 [==============================] - 0s 371us/step - loss: 0.1016 - val_loss: 0.0334\n",
            "Epoch 94/200\n",
            "191/191 [==============================] - 0s 378us/step - loss: 0.1002 - val_loss: 0.0305\n",
            "Epoch 95/200\n",
            "191/191 [==============================] - 0s 344us/step - loss: 0.0967 - val_loss: 0.0267\n",
            "Epoch 96/200\n",
            "191/191 [==============================] - 0s 346us/step - loss: 0.0985 - val_loss: 0.0258\n",
            "Epoch 97/200\n",
            "191/191 [==============================] - 0s 381us/step - loss: 0.1036 - val_loss: 0.0287\n",
            "Epoch 98/200\n",
            "191/191 [==============================] - 0s 318us/step - loss: 0.0910 - val_loss: 0.0320\n",
            "Epoch 99/200\n",
            "191/191 [==============================] - 0s 439us/step - loss: 0.0879 - val_loss: 0.0308\n",
            "Epoch 100/200\n",
            "191/191 [==============================] - 0s 417us/step - loss: 0.0912 - val_loss: 0.0255\n",
            "Epoch 101/200\n",
            "191/191 [==============================] - 0s 409us/step - loss: 0.0919 - val_loss: 0.0219\n",
            "Epoch 102/200\n",
            "191/191 [==============================] - 0s 403us/step - loss: 0.0943 - val_loss: 0.0217\n",
            "Epoch 103/200\n",
            "191/191 [==============================] - 0s 372us/step - loss: 0.0768 - val_loss: 0.0232\n",
            "Epoch 104/200\n",
            "191/191 [==============================] - 0s 335us/step - loss: 0.0833 - val_loss: 0.0260\n",
            "Epoch 105/200\n",
            "191/191 [==============================] - 0s 381us/step - loss: 0.0787 - val_loss: 0.0264\n",
            "Epoch 106/200\n",
            "191/191 [==============================] - 0s 386us/step - loss: 0.0785 - val_loss: 0.0239\n",
            "Epoch 107/200\n",
            "191/191 [==============================] - 0s 365us/step - loss: 0.0888 - val_loss: 0.0238\n",
            "Epoch 108/200\n",
            "191/191 [==============================] - 0s 402us/step - loss: 0.0931 - val_loss: 0.0244\n",
            "Epoch 109/200\n",
            "191/191 [==============================] - 0s 362us/step - loss: 0.0763 - val_loss: 0.0246\n",
            "Epoch 110/200\n",
            "191/191 [==============================] - 0s 333us/step - loss: 0.0698 - val_loss: 0.0269\n",
            "Epoch 111/200\n",
            "191/191 [==============================] - 0s 406us/step - loss: 0.0762 - val_loss: 0.0258\n",
            "Epoch 112/200\n",
            "191/191 [==============================] - 0s 389us/step - loss: 0.0719 - val_loss: 0.0247\n",
            "Epoch 113/200\n",
            "191/191 [==============================] - 0s 366us/step - loss: 0.0806 - val_loss: 0.0234\n",
            "Epoch 114/200\n",
            "191/191 [==============================] - 0s 349us/step - loss: 0.0784 - val_loss: 0.0230\n",
            "Epoch 115/200\n",
            "191/191 [==============================] - 0s 362us/step - loss: 0.0698 - val_loss: 0.0234\n",
            "Epoch 116/200\n",
            "191/191 [==============================] - 0s 360us/step - loss: 0.0780 - val_loss: 0.0244\n",
            "Epoch 117/200\n",
            "191/191 [==============================] - 0s 336us/step - loss: 0.0666 - val_loss: 0.0252\n",
            "Epoch 118/200\n",
            "191/191 [==============================] - 0s 350us/step - loss: 0.0703 - val_loss: 0.0243\n",
            "Epoch 119/200\n",
            "191/191 [==============================] - 0s 360us/step - loss: 0.0664 - val_loss: 0.0242\n",
            "Epoch 120/200\n",
            "191/191 [==============================] - 0s 391us/step - loss: 0.0692 - val_loss: 0.0232\n",
            "Epoch 121/200\n",
            "191/191 [==============================] - 0s 366us/step - loss: 0.0726 - val_loss: 0.0220\n",
            "Epoch 122/200\n",
            "191/191 [==============================] - 0s 378us/step - loss: 0.0650 - val_loss: 0.0227\n",
            "Epoch 123/200\n",
            "191/191 [==============================] - 0s 361us/step - loss: 0.0687 - val_loss: 0.0233\n",
            "Epoch 124/200\n",
            "191/191 [==============================] - 0s 394us/step - loss: 0.0647 - val_loss: 0.0228\n",
            "Epoch 125/200\n",
            "191/191 [==============================] - 0s 327us/step - loss: 0.0629 - val_loss: 0.0228\n",
            "Epoch 126/200\n",
            "191/191 [==============================] - 0s 320us/step - loss: 0.0607 - val_loss: 0.0244\n",
            "Epoch 127/200\n",
            "191/191 [==============================] - 0s 333us/step - loss: 0.0585 - val_loss: 0.0255\n",
            "Epoch 128/200\n",
            "191/191 [==============================] - 0s 378us/step - loss: 0.0560 - val_loss: 0.0243\n",
            "Epoch 129/200\n",
            "191/191 [==============================] - 0s 326us/step - loss: 0.0600 - val_loss: 0.0233\n",
            "Epoch 130/200\n",
            "191/191 [==============================] - 0s 356us/step - loss: 0.0536 - val_loss: 0.0236\n",
            "Epoch 131/200\n",
            "191/191 [==============================] - 0s 375us/step - loss: 0.0605 - val_loss: 0.0250\n",
            "Epoch 132/200\n",
            "191/191 [==============================] - 0s 413us/step - loss: 0.0568 - val_loss: 0.0257\n",
            "Epoch 133/200\n",
            "191/191 [==============================] - 0s 409us/step - loss: 0.0607 - val_loss: 0.0247\n",
            "Epoch 134/200\n",
            "191/191 [==============================] - 0s 348us/step - loss: 0.0523 - val_loss: 0.0242\n",
            "Epoch 135/200\n",
            "191/191 [==============================] - 0s 322us/step - loss: 0.0579 - val_loss: 0.0234\n",
            "Epoch 136/200\n",
            "191/191 [==============================] - 0s 359us/step - loss: 0.0529 - val_loss: 0.0233\n",
            "Epoch 137/200\n",
            "191/191 [==============================] - 0s 405us/step - loss: 0.0591 - val_loss: 0.0234\n",
            "Epoch 138/200\n",
            "191/191 [==============================] - 0s 385us/step - loss: 0.0554 - val_loss: 0.0234\n",
            "Epoch 139/200\n",
            "191/191 [==============================] - 0s 358us/step - loss: 0.0524 - val_loss: 0.0228\n",
            "Epoch 140/200\n",
            "191/191 [==============================] - 0s 378us/step - loss: 0.0533 - val_loss: 0.0227\n",
            "Epoch 141/200\n",
            "191/191 [==============================] - 0s 332us/step - loss: 0.0510 - val_loss: 0.0222\n",
            "Epoch 142/200\n",
            "191/191 [==============================] - 0s 329us/step - loss: 0.0540 - val_loss: 0.0220\n",
            "Epoch 143/200\n",
            "191/191 [==============================] - 0s 361us/step - loss: 0.0531 - val_loss: 0.0226\n",
            "Epoch 144/200\n",
            "191/191 [==============================] - 0s 364us/step - loss: 0.0539 - val_loss: 0.0227\n",
            "Epoch 145/200\n",
            "191/191 [==============================] - 0s 382us/step - loss: 0.0506 - val_loss: 0.0222\n",
            "Epoch 146/200\n",
            "191/191 [==============================] - 0s 398us/step - loss: 0.0459 - val_loss: 0.0217\n",
            "Epoch 147/200\n",
            "191/191 [==============================] - 0s 405us/step - loss: 0.0448 - val_loss: 0.0221\n",
            "Epoch 148/200\n",
            "191/191 [==============================] - 0s 344us/step - loss: 0.0468 - val_loss: 0.0225\n",
            "Epoch 149/200\n",
            "191/191 [==============================] - 0s 344us/step - loss: 0.0416 - val_loss: 0.0230\n",
            "Epoch 150/200\n",
            "191/191 [==============================] - 0s 351us/step - loss: 0.0433 - val_loss: 0.0224\n",
            "Epoch 151/200\n",
            "191/191 [==============================] - 0s 325us/step - loss: 0.0464 - val_loss: 0.0223\n",
            "Epoch 152/200\n",
            "191/191 [==============================] - 0s 319us/step - loss: 0.0430 - val_loss: 0.0227\n",
            "Epoch 153/200\n",
            "191/191 [==============================] - 0s 393us/step - loss: 0.0460 - val_loss: 0.0224\n",
            "Epoch 154/200\n",
            "191/191 [==============================] - 0s 416us/step - loss: 0.0423 - val_loss: 0.0227\n",
            "Epoch 155/200\n",
            "191/191 [==============================] - 0s 381us/step - loss: 0.0442 - val_loss: 0.0228\n",
            "Epoch 156/200\n",
            "191/191 [==============================] - 0s 347us/step - loss: 0.0464 - val_loss: 0.0228\n",
            "Epoch 157/200\n",
            "191/191 [==============================] - 0s 363us/step - loss: 0.0419 - val_loss: 0.0229\n",
            "Epoch 158/200\n",
            "191/191 [==============================] - 0s 368us/step - loss: 0.0381 - val_loss: 0.0225\n",
            "Epoch 159/200\n",
            "191/191 [==============================] - 0s 388us/step - loss: 0.0429 - val_loss: 0.0219\n",
            "Epoch 160/200\n",
            "191/191 [==============================] - 0s 362us/step - loss: 0.0401 - val_loss: 0.0216\n",
            "Epoch 161/200\n",
            "191/191 [==============================] - 0s 379us/step - loss: 0.0417 - val_loss: 0.0220\n",
            "Epoch 162/200\n",
            "191/191 [==============================] - 0s 382us/step - loss: 0.0392 - val_loss: 0.0236\n",
            "Epoch 163/200\n",
            "191/191 [==============================] - 0s 343us/step - loss: 0.0389 - val_loss: 0.0233\n",
            "Epoch 164/200\n",
            "191/191 [==============================] - 0s 368us/step - loss: 0.0415 - val_loss: 0.0225\n",
            "Epoch 165/200\n",
            "191/191 [==============================] - 0s 369us/step - loss: 0.0439 - val_loss: 0.0225\n",
            "Epoch 166/200\n",
            "191/191 [==============================] - 0s 368us/step - loss: 0.0434 - val_loss: 0.0229\n",
            "Epoch 167/200\n",
            "191/191 [==============================] - 0s 403us/step - loss: 0.0414 - val_loss: 0.0225\n",
            "Epoch 168/200\n",
            "191/191 [==============================] - 0s 332us/step - loss: 0.0389 - val_loss: 0.0225\n",
            "Epoch 169/200\n",
            "191/191 [==============================] - 0s 349us/step - loss: 0.0359 - val_loss: 0.0230\n",
            "Epoch 170/200\n",
            "191/191 [==============================] - 0s 342us/step - loss: 0.0387 - val_loss: 0.0228\n",
            "Epoch 171/200\n",
            "191/191 [==============================] - 0s 355us/step - loss: 0.0379 - val_loss: 0.0219\n",
            "Epoch 172/200\n",
            "191/191 [==============================] - 0s 322us/step - loss: 0.0377 - val_loss: 0.0222\n",
            "Epoch 173/200\n",
            "191/191 [==============================] - 0s 349us/step - loss: 0.0357 - val_loss: 0.0229\n",
            "Epoch 174/200\n",
            "191/191 [==============================] - 0s 368us/step - loss: 0.0374 - val_loss: 0.0229\n",
            "Epoch 175/200\n",
            "191/191 [==============================] - 0s 385us/step - loss: 0.0357 - val_loss: 0.0225\n",
            "Epoch 176/200\n",
            "191/191 [==============================] - 0s 333us/step - loss: 0.0321 - val_loss: 0.0227\n",
            "Epoch 177/200\n",
            "191/191 [==============================] - 0s 331us/step - loss: 0.0396 - val_loss: 0.0221\n",
            "Epoch 178/200\n",
            "191/191 [==============================] - 0s 372us/step - loss: 0.0363 - val_loss: 0.0217\n",
            "Epoch 179/200\n",
            "191/191 [==============================] - 0s 317us/step - loss: 0.0334 - val_loss: 0.0215\n",
            "Epoch 180/200\n",
            "191/191 [==============================] - 0s 327us/step - loss: 0.0326 - val_loss: 0.0222\n",
            "Epoch 181/200\n",
            "191/191 [==============================] - 0s 313us/step - loss: 0.0318 - val_loss: 0.0226\n",
            "Epoch 182/200\n",
            "191/191 [==============================] - 0s 417us/step - loss: 0.0319 - val_loss: 0.0226\n",
            "Epoch 183/200\n",
            "191/191 [==============================] - 0s 540us/step - loss: 0.0382 - val_loss: 0.0223\n",
            "Epoch 184/200\n",
            "191/191 [==============================] - 0s 380us/step - loss: 0.0345 - val_loss: 0.0220\n",
            "Epoch 185/200\n",
            "191/191 [==============================] - 0s 370us/step - loss: 0.0307 - val_loss: 0.0216\n",
            "Epoch 186/200\n",
            "191/191 [==============================] - 0s 368us/step - loss: 0.0280 - val_loss: 0.0216\n",
            "Epoch 187/200\n",
            "191/191 [==============================] - 0s 355us/step - loss: 0.0334 - val_loss: 0.0217\n",
            "Epoch 188/200\n",
            "191/191 [==============================] - 0s 338us/step - loss: 0.0332 - val_loss: 0.0218\n",
            "Epoch 189/200\n",
            "191/191 [==============================] - 0s 324us/step - loss: 0.0314 - val_loss: 0.0214\n",
            "Epoch 190/200\n",
            "191/191 [==============================] - 0s 375us/step - loss: 0.0340 - val_loss: 0.0213\n",
            "Epoch 191/200\n",
            "191/191 [==============================] - 0s 317us/step - loss: 0.0311 - val_loss: 0.0216\n",
            "Epoch 192/200\n",
            "191/191 [==============================] - 0s 405us/step - loss: 0.0322 - val_loss: 0.0220\n",
            "Epoch 193/200\n",
            "191/191 [==============================] - 0s 397us/step - loss: 0.0280 - val_loss: 0.0224\n",
            "Epoch 194/200\n",
            "191/191 [==============================] - 0s 350us/step - loss: 0.0298 - val_loss: 0.0218\n",
            "Epoch 195/200\n",
            "191/191 [==============================] - 0s 414us/step - loss: 0.0308 - val_loss: 0.0215\n",
            "Epoch 196/200\n",
            "191/191 [==============================] - 0s 392us/step - loss: 0.0332 - val_loss: 0.0216\n",
            "Epoch 197/200\n",
            "191/191 [==============================] - 0s 397us/step - loss: 0.0289 - val_loss: 0.0215\n",
            "Epoch 198/200\n",
            "191/191 [==============================] - 0s 410us/step - loss: 0.0318 - val_loss: 0.0217\n",
            "Epoch 199/200\n",
            "191/191 [==============================] - 0s 349us/step - loss: 0.0279 - val_loss: 0.0215\n",
            "Epoch 200/200\n",
            "191/191 [==============================] - 0s 375us/step - loss: 0.0260 - val_loss: 0.0212\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f70ea209ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4PnMfQG65Ch",
        "colab_type": "code",
        "outputId": "b6301389-db02-46a4-d331-7248db93700b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# demonstrate prediction\n",
        "x_input = dataXScaler[2:30]\n",
        "x_input = x_input.reshape((1, len(x_input), 4))\n",
        "print(scaler.inverse_transform(dataXScaler)[2:30])\n",
        "yhat = model_gru.predict(x_input, verbose=0)\n",
        "print(scaler.inverse_transform(yhat))\n",
        "print('expected: ', scaler.inverse_transform(dataYScaler)[30])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]]\n",
            "[[77.85289 77.10733 73.91023 73.91752]]\n",
            "expected:  [72. 69. 65. 53.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk9W_GSXtwRG",
        "colab_type": "code",
        "outputId": "7aa8a702-db72-497d-b73a-2f793558bb9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# choose a number of time steps\n",
        "n_steps = None\n",
        "# convert into input/output\n",
        "i=1\n",
        "n=1\n",
        "x_input = np.array([dataXScaler[0]])\n",
        "\n",
        "x_input = x_input.reshape((1, len(x_input), n_features))\n",
        "while i<len(dataXScaler):\n",
        "  # demonstrate prediction\n",
        "  print('Input: ')\n",
        "  for input in x_input:\n",
        "    print(scaler.inverse_transform(input))\n",
        "  print('---------------')\n",
        "  yhat = model_gru.predict(x_input, verbose=1)\n",
        "  print('Predicted Output: ', scaler.inverse_transform(yhat))\n",
        "  print('expected: ', scaler.inverse_transform(dataYScaler)[i])\n",
        "  print('\\n\\n')\n",
        "\n",
        "  yhat = yhat.reshape((1, len(yhat), n_features))\n",
        "  x_input = np.concatenate([x_input, yhat], axis=1)\n",
        "  i += 1\n",
        "  if i>20:\n",
        "    break"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            "[[74. 70. 65. 58.]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 5ms/step\n",
            "Predicted Output:  [[74.44904 69.60308 62.69999 57.07506]]\n",
            "expected:  [74. 70. 65. 58.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[76.25694  72.169205 64.44614  62.029022]]\n",
            "expected:  [74. 70. 65. 58.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[76.527115 73.45797  64.80749  62.385166]]\n",
            "expected:  [75. 70. 58. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[76.3469   74.01289  64.802635 61.148895]]\n",
            "expected:  [75. 70. 58. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[76.24492  74.3193   65.01809  60.415554]]\n",
            "expected:  [75. 70. 60. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]\n",
            " [76.24491966 74.31930399 65.01808882 60.41555583]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[76.29366  74.5374   65.46101  60.329548]]\n",
            "expected:  [75. 70. 60. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]\n",
            " [76.24491966 74.31930399 65.01808882 60.41555583]\n",
            " [76.29366398 74.53739119 65.46101809 60.32954669]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[76.43967  74.722244 66.05531  60.768253]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]\n",
            " [76.24491966 74.31930399 65.01808882 60.41555583]\n",
            " [76.29366398 74.53739119 65.46101809 60.32954669]\n",
            " [76.43966734 74.72224045 66.05530691 60.7682519 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 5ms/step\n",
            "Predicted Output:  [[76.5923  74.87338 66.66608 61.5149 ]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]\n",
            " [76.24491966 74.31930399 65.01808882 60.41555583]\n",
            " [76.29366398 74.53739119 65.46101809 60.32954669]\n",
            " [76.43966734 74.72224045 66.05530691 60.7682519 ]\n",
            " [76.59230435 74.87338185 66.66607952 61.51490271]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[76.71603  75.00351  67.240685 62.45663 ]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]\n",
            " [76.24491966 74.31930399 65.01808882 60.41555583]\n",
            " [76.29366398 74.53739119 65.46101809 60.32954669]\n",
            " [76.43966734 74.72224045 66.05530691 60.7682519 ]\n",
            " [76.59230435 74.87338185 66.66607952 61.51490271]\n",
            " [76.71603036 75.00350869 67.24068308 62.45662999]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 6ms/step\n",
            "Predicted Output:  [[76.8041   75.125    67.755295 63.50266 ]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]\n",
            " [76.24491966 74.31930399 65.01808882 60.41555583]\n",
            " [76.29366398 74.53739119 65.46101809 60.32954669]\n",
            " [76.43966734 74.72224045 66.05530691 60.7682519 ]\n",
            " [76.59230435 74.87338185 66.66607952 61.51490271]\n",
            " [76.71603036 75.00350869 67.24068308 62.45662999]\n",
            " [76.80409908 75.12500417 67.75529242 63.50265813]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[76.85974  75.245415 68.19119  64.55108 ]]\n",
            "expected:  [77. 70. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]\n",
            " [76.24491966 74.31930399 65.01808882 60.41555583]\n",
            " [76.29366398 74.53739119 65.46101809 60.32954669]\n",
            " [76.43966734 74.72224045 66.05530691 60.7682519 ]\n",
            " [76.59230435 74.87338185 66.66607952 61.51490271]\n",
            " [76.71603036 75.00350869 67.24068308 62.45662999]\n",
            " [76.80409908 75.12500417 67.75529242 63.50265813]\n",
            " [76.85974419 75.24540961 68.19118786 64.55108166]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 5ms/step\n",
            "Predicted Output:  [[76.876015 75.3521   68.495445 65.441284]]\n",
            "expected:  [77. 70. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]\n",
            " [76.24491966 74.31930399 65.01808882 60.41555583]\n",
            " [76.29366398 74.53739119 65.46101809 60.32954669]\n",
            " [76.43966734 74.72224045 66.05530691 60.7682519 ]\n",
            " [76.59230435 74.87338185 66.66607952 61.51490271]\n",
            " [76.71603036 75.00350869 67.24068308 62.45662999]\n",
            " [76.80409908 75.12500417 67.75529242 63.50265813]\n",
            " [76.85974419 75.24540961 68.19118786 64.55108166]\n",
            " [76.87601233 75.35210037 68.49544764 65.44128573]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "Predicted Output:  [[76.845985 75.42441  68.621056 66.02654 ]]\n",
            "expected:  [77. 69. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]\n",
            " [76.24491966 74.31930399 65.01808882 60.41555583]\n",
            " [76.29366398 74.53739119 65.46101809 60.32954669]\n",
            " [76.43966734 74.72224045 66.05530691 60.7682519 ]\n",
            " [76.59230435 74.87338185 66.66607952 61.51490271]\n",
            " [76.71603036 75.00350869 67.24068308 62.45662999]\n",
            " [76.80409908 75.12500417 67.75529242 63.50265813]\n",
            " [76.85974419 75.24540961 68.19118786 64.55108166]\n",
            " [76.87601233 75.35210037 68.49544764 65.44128573]\n",
            " [76.84598553 75.42440629 68.62105656 66.0265435 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[76.77847 75.45416 68.58115 66.28294]]\n",
            "expected:  [77. 69. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]\n",
            " [76.24491966 74.31930399 65.01808882 60.41555583]\n",
            " [76.29366398 74.53739119 65.46101809 60.32954669]\n",
            " [76.43966734 74.72224045 66.05530691 60.7682519 ]\n",
            " [76.59230435 74.87338185 66.66607952 61.51490271]\n",
            " [76.71603036 75.00350869 67.24068308 62.45662999]\n",
            " [76.80409908 75.12500417 67.75529242 63.50265813]\n",
            " [76.85974419 75.24540961 68.19118786 64.55108166]\n",
            " [76.87601233 75.35210037 68.49544764 65.44128573]\n",
            " [76.84598553 75.42440629 68.62105656 66.0265435 ]\n",
            " [76.77846789 75.45416164 68.58114815 66.28294361]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[76.695076 75.45099  68.44035  66.29796 ]]\n",
            "expected:  [75. 67. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]\n",
            " [76.24491966 74.31930399 65.01808882 60.41555583]\n",
            " [76.29366398 74.53739119 65.46101809 60.32954669]\n",
            " [76.43966734 74.72224045 66.05530691 60.7682519 ]\n",
            " [76.59230435 74.87338185 66.66607952 61.51490271]\n",
            " [76.71603036 75.00350869 67.24068308 62.45662999]\n",
            " [76.80409908 75.12500417 67.75529242 63.50265813]\n",
            " [76.85974419 75.24540961 68.19118786 64.55108166]\n",
            " [76.87601233 75.35210037 68.49544764 65.44128573]\n",
            " [76.84598553 75.42440629 68.62105656 66.0265435 ]\n",
            " [76.77846789 75.45416164 68.58114815 66.28294361]\n",
            " [76.69507909 75.45098698 68.44035149 66.2979604 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 5ms/step\n",
            "Predicted Output:  [[76.61456  75.430115 68.26451  66.183624]]\n",
            "expected:  [75. 67. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]\n",
            " [76.24491966 74.31930399 65.01808882 60.41555583]\n",
            " [76.29366398 74.53739119 65.46101809 60.32954669]\n",
            " [76.43966734 74.72224045 66.05530691 60.7682519 ]\n",
            " [76.59230435 74.87338185 66.66607952 61.51490271]\n",
            " [76.71603036 75.00350869 67.24068308 62.45662999]\n",
            " [76.80409908 75.12500417 67.75529242 63.50265813]\n",
            " [76.85974419 75.24540961 68.19118786 64.55108166]\n",
            " [76.87601233 75.35210037 68.49544764 65.44128573]\n",
            " [76.84598553 75.42440629 68.62105656 66.0265435 ]\n",
            " [76.77846789 75.45416164 68.58114815 66.28294361]\n",
            " [76.69507909 75.45098698 68.44035149 66.2979604 ]\n",
            " [76.61456454 75.43010974 68.26451302 66.18361998]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[76.54617  75.402855 68.09363  66.01961 ]]\n",
            "expected:  [75. 69. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]\n",
            " [76.24491966 74.31930399 65.01808882 60.41555583]\n",
            " [76.29366398 74.53739119 65.46101809 60.32954669]\n",
            " [76.43966734 74.72224045 66.05530691 60.7682519 ]\n",
            " [76.59230435 74.87338185 66.66607952 61.51490271]\n",
            " [76.71603036 75.00350869 67.24068308 62.45662999]\n",
            " [76.80409908 75.12500417 67.75529242 63.50265813]\n",
            " [76.85974419 75.24540961 68.19118786 64.55108166]\n",
            " [76.87601233 75.35210037 68.49544764 65.44128573]\n",
            " [76.84598553 75.42440629 68.62105656 66.0265435 ]\n",
            " [76.77846789 75.45416164 68.58114815 66.28294361]\n",
            " [76.69507909 75.45098698 68.44035149 66.2979604 ]\n",
            " [76.61456454 75.43010974 68.26451302 66.18361998]\n",
            " [76.54617786 75.40285528 68.09362555 66.01961267]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[76.4915   75.37495  67.94275  65.846794]]\n",
            "expected:  [75. 69. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]\n",
            " [76.24491966 74.31930399 65.01808882 60.41555583]\n",
            " [76.29366398 74.53739119 65.46101809 60.32954669]\n",
            " [76.43966734 74.72224045 66.05530691 60.7682519 ]\n",
            " [76.59230435 74.87338185 66.66607952 61.51490271]\n",
            " [76.71603036 75.00350869 67.24068308 62.45662999]\n",
            " [76.80409908 75.12500417 67.75529242 63.50265813]\n",
            " [76.85974419 75.24540961 68.19118786 64.55108166]\n",
            " [76.87601233 75.35210037 68.49544764 65.44128573]\n",
            " [76.84598553 75.42440629 68.62105656 66.0265435 ]\n",
            " [76.77846789 75.45416164 68.58114815 66.28294361]\n",
            " [76.69507909 75.45098698 68.44035149 66.2979604 ]\n",
            " [76.61456454 75.43010974 68.26451302 66.18361998]\n",
            " [76.54617786 75.40285528 68.09362555 66.01961267]\n",
            " [76.49150479 75.37494957 67.94274807 65.84679592]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 6ms/step\n",
            "Predicted Output:  [[76.44941 75.34898 67.81628 65.6836 ]]\n",
            "expected:  [74. 70. 65. 46.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [74.44903803 69.60308254 62.69998932 57.07506287]\n",
            " [76.25694132 72.16920662 64.44613743 62.02902424]\n",
            " [76.5271138  73.45796943 64.8074832  62.38516545]\n",
            " [76.34690154 74.01289272 64.8026309  61.14889622]\n",
            " [76.24491966 74.31930399 65.01808882 60.41555583]\n",
            " [76.29366398 74.53739119 65.46101809 60.32954669]\n",
            " [76.43966734 74.72224045 66.05530691 60.7682519 ]\n",
            " [76.59230435 74.87338185 66.66607952 61.51490271]\n",
            " [76.71603036 75.00350869 67.24068308 62.45662999]\n",
            " [76.80409908 75.12500417 67.75529242 63.50265813]\n",
            " [76.85974419 75.24540961 68.19118786 64.55108166]\n",
            " [76.87601233 75.35210037 68.49544764 65.44128573]\n",
            " [76.84598553 75.42440629 68.62105656 66.0265435 ]\n",
            " [76.77846789 75.45416164 68.58114815 66.28294361]\n",
            " [76.69507909 75.45098698 68.44035149 66.2979604 ]\n",
            " [76.61456454 75.43010974 68.26451302 66.18361998]\n",
            " [76.54617786 75.40285528 68.09362555 66.01961267]\n",
            " [76.49150479 75.37494957 67.94274807 65.84679592]\n",
            " [76.44941181 75.34898472 67.81627512 65.68360329]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 6ms/step\n",
            "Predicted Output:  [[76.418   75.32617 67.71422 65.53884]]\n",
            "expected:  [74. 70. 65. 46.]\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TATEl_RF8IpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}