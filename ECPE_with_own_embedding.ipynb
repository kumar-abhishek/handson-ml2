{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECPE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMOXVYjKbhtTxdgTuRJXA3n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar-abhishek/handson-ml2/blob/master/ECPE_with_own_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGiWL8CG97xr",
        "colab_type": "text"
      },
      "source": [
        "Algorithm\n",
        "\n",
        "1. Take the document, split into clauses\n",
        "2. Find embeddings of the clauses\n",
        "3. Feed embeddings of clauses into a Bi-LSTM layer(word-level), followed by attention layer \n",
        "4. Output of previous layer gets copied into 2 components.\n",
        "5. 1 component is for emotion extraction and is a Bi-LSTM layer(clause-level)\n",
        "6. 2nd compoent is for cause extraction and is a Bi-LSTM layer(clause-level)\n",
        "7. Loss Lp of the whole model is the weighted sum of two components:\n",
        "Lp=n*Le+(1-n)*Lc\n",
        "where n is a hyper-param\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW2iw-d0JCe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fbe16199-fc08-41ae-b823-115422668d75"
      },
      "source": [
        "!pip install tensorflow==2.0.0\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 36kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.27.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.34.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.18.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.9.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 60.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (46.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=74182586624c53fa3faea26c413a71f6afeaa737cf2e685afbe272f1a5b74460\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: gast, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.2\n",
            "    Uninstalling tensorflow-1.15.2:\n",
            "      Successfully uninstalled tensorflow-1.15.2\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-2.2.0rc1 tensorflow-estimator-2.2.0rc0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7ZKapWzJx47",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "eaca08bf-69fd-4dd7-fb2b-d48940a3de66"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "%tensorflow_version\n"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently selected TF version: 2.x\n",
            "Available versions:\n",
            "* 1.x\n",
            "* 2.x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_h12sNp3T2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1dRpjtec_sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emotion_seeds = set([\"ashamed\", \"delighted\", \"pleased\", \"concerned\", \"delight\", \"happy\", \"embarrassed\", \"furious\", \"nervous\", \n",
        "                     \"miffed\", \"angry\", \"mad\", \"anger\", \"excitement\", \"horror\", \"resentful\", \"astonished\", \"revulsion\", \n",
        "                     \"frightened\", \"cross\", \"sad\", \"down\", \"astonishment\", \"miserable\", \"worried\", \"sorrow\", \"overjoyed\",\n",
        "                     \"dismay\", \"grief\", \"annoyance\", \"alarmed\", \"astounded\", \"anguish\", \"despair\", \"infuriated\", \n",
        "                     \"embarrassment\", \"peeved\", \"amused\", \"disgruntled\", \"indignant\", \"thrilled\", \"anxious\", \"excited\",\n",
        "                     \"exasperation\", \"petrified\", \"heartbroken\", \"saddened\", \"depressed\", \"dismayed\", \"frustrated\", \"fedup\", \"livid\",\n",
        "                     \"revulsion\", \"bewildered\", \"flabbergasted\", \"happier\", \"ecstatic\", \"elation\", \"exhilarated\", \"exhilaration\",\n",
        "                     \"glee\", \"gleeful\", \"crestfallen\", \"sadness\", \"amusement\", \"dejected\", \"desolate\", \"despondency\", \"horrors\",\n",
        "                     \"agitated\", \"disquiet\", \"horrified\", \"exasperated\", \"irked\", \"disgruntlement\", \"sickened\", \"revolted\",\n",
        "                     \"devastated\", \"heartbreak\", \"inconsolable\", \"bewilderment\", \"nonplussed\", \"puzzlement\", \"disquieted\",\n",
        "                     \"glum\", \"downcast\", \"griefstricken\", \"startled\", \"disgusted\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dCwi_DhBrBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input is sentence, output is emotion cause pairs\n",
        "# Determine clauses by splitting on punctuation.\n",
        "\n",
        "# preprocess\n",
        "\n",
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def remove_nonascii(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "  return w\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = remove_nonascii(w)  \n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM7Kub3NJDyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_cause(text):\n",
        "  cur_cause=''\n",
        "  try:\n",
        "    cur_cause = re.findall('<cause>(.*?)<\\\\\\cause>', text)[0]\n",
        "    # Remove tags from line\n",
        "    text=re.sub('<cause>', '', text)\n",
        "    text=re.sub('<\\\\\\cause>', '', text)\n",
        "  except:\n",
        "    pass\n",
        "  #print('here:', text)\n",
        "  return (cur_cause, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSCLxRvA7mbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_filter_clauses(all_clauses):\n",
        "  cause = ''\n",
        "  clauses=[]\n",
        "  for clause in all_clauses:\n",
        "    e_cause, e_text = extract_cause(clause)\n",
        "    if e_cause!='':\n",
        "      cause = remove_nonascii(e_cause)\n",
        "    clauses.append(remove_nonascii(e_text))\n",
        "  return cause, clauses\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8b9UvTqebR6",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBMZz5Le2f71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "path_to_file = \"data.txt\"  \n",
        "# 1. Remove any accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [document, emotion, cause, clauses list]\n",
        "document=[]\n",
        "emotion=[]\n",
        "cause=[]\n",
        "clause=[]\n",
        "def create_dataset(path, num_examples):\n",
        "  document.clear()\n",
        "  emotion.clear()\n",
        "  cause.clear()\n",
        "  clause.clear()\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  for i, line in enumerate(lines[:num_examples]):\n",
        "    cur_emotion = re.findall('<(.*?)>', line)[0]\n",
        "    # removing emotion tag in document\n",
        "    text_without_emotion=line[2+len(cur_emotion):len(line)-len(cur_emotion)-3]\n",
        "    #document.append(text_without_emotion)\n",
        "    emotion.append(cur_emotion)\n",
        "\n",
        "    # Determine clauses by splitting on punctuation.\n",
        "    all_clauses = re.split(\"[.,!;:\\\"]+\", text_without_emotion)\n",
        "    filter_cause, filter_clauses = clean_filter_clauses(all_clauses)\n",
        "    cause.append(filter_cause)\n",
        "    clause.append([filter_clauses])\n",
        "    doc = extract_cause(text_without_emotion)[1]\n",
        "    # clean up document\n",
        "    clean_doc = preprocess_sentence(doc)\n",
        "    document.append(clean_doc)\n",
        "\n",
        "    # clean up clauses\n",
        "    # TODO\n",
        "  return [document, emotion, cause, clause]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve40pjba24tJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "f72e589e-97e8-4ed7-a337-019af2e96d97"
      },
      "source": [
        "document, emotion, cause, clause_list = create_dataset(path_to_file, 5)\n",
        "print(len(document))\n",
        "for i in range(5):\n",
        "  print(document[i])\n",
        "  print(emotion[i])\n",
        "  print(cause[i])\n",
        "  print(clause_list[i])\n",
        "  print('\\n--------\\n')"
      ],
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "<start> i suppose i am happy , being so tiny it means i am able to surprise people with what is generally seen as my confident and outgoing personality . <end>\n",
            "happy\n",
            "being so tiny\n",
            "[['i suppose i am happy', 'being so tiny', 'it means i am able to surprise people with what is generally seen as my confident and outgoing personality', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> lennox has always truly wanted to fight for the world title and was happy , because he was taking the tough route . <end>\n",
            "happy\n",
            "because he was taking the tough route\n",
            "[['lennox has always truly wanted to fight for the world title and was happy', 'because he was taking the tough route', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> he was a professional musician now , still sensitive and happy , doing something he loved . <end>\n",
            "happy\n",
            "doing something he loved\n",
            "[['he was a professional musician now', 'still sensitive and happy', 'doing something he loved', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> holmes is happy , because , he has the freedom of the house when we are out . <end>\n",
            "happy\n",
            "he has the freedom of the house when we are out\n",
            "[['holmes is happy', 'because', 'he has the freedom of the house when we are out', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> i had problems with tutors trying to encourage me to diversify my work and experiment with other styles , but i was quite happy , with the direction my work was heading so i stubbornly stuck to it . <end>\n",
            "happy\n",
            "with the direction my work was heading\n",
            "[['i had problems with tutors trying to encourage me to diversify my work and experiment with other styles', 'but i was quite happy', 'with the direction my work was heading so i stubbornly stuck to it', '']]\n",
            "\n",
            "--------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnB45wnBgip8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50d08124-ae0e-4e69-bc98-9791f64f2ace"
      },
      "source": [
        "X=document\n",
        "#X=[tf.constant(sentence) for sentence in document]\n",
        "#print(X[0], X[0].shape, (X[0].numpy()))\n",
        "y=emotion\n",
        "print(y)"
      ],
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['happy', 'happy', 'happy', 'happy', 'happy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBVHLFB0Ky5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_emotion=keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"xxxxxxx\")\n",
        "tokenizer_emotion.fit_on_texts(emotion_seeds)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iAnFAxmxtVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer=keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"xxxxxxx\")\n",
        "tokenizer_emotion=keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"xxxxxxx\")\n",
        "def use_tokenizer():\n",
        "  tokenizer.fit_on_texts(X)\n",
        "  tokenizer_emotion.fit_on_texts(emotion_seeds)\n",
        "  X_dict=tokenizer.word_index\n",
        "\n",
        "  X_seq=tokenizer.texts_to_sequences(X)\n",
        "  X_padded_seq=pad_sequences(X_seq,padding='post',maxlen=40) \n",
        "  print(X_padded_seq[:3], X_padded_seq.shape, type(X_padded_seq))\n",
        "  print(X_padded_seq.shape)\n",
        "  \n",
        "  return X_padded_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJki4OW_x-vi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "40a691a4-48ab-4885-be17-54450b25a4f7"
      },
      "source": [
        "X_padded_seq = use_tokenizer()"
      ],
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3  2 20  2 13  4 21 14 22 15 23  2 13 24  5 25 26  9 27 16 28 29 30 12\n",
            "  31 10 32 33  6  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3 34 17 35 36 37  5 38 39  7 40 41 10  8  4 18 11  8 42  7 43 44  6  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3 11  8 45 46 47 48 49 50 10  4 51 52 11 53  6  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]] (5, 40) <class 'numpy.ndarray'>\n",
            "(5, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkIi0Z7zgFoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=tokenizer_emotion.texts_to_sequences(y)\n",
        "y=np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT1VS2u-gF9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Flatten\n",
        "\n",
        "TFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\n",
        "os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR\n",
        "\n",
        "text_model = tf.keras.Sequential([\n",
        "                                  #tf.keras.layers.Embedding(input_length=40,input_dim=100,output_dim=50, input_shape=[None]),\n",
        "                                  tf.keras.layers.Embedding(input_length=40,input_dim=10000,output_dim=50),\n",
        "    Flatten(),\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    Dense(len(tokenizer_emotion.word_index), activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr5fsVxVgLPn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "d7c03c9f-98a1-4829-cdf7-bd84ee3f91ae"
      },
      "source": [
        "text_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "text_model.summary()\n"
      ],
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_61\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 40, 50)            500000    \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 2000)              0         \n",
            "_________________________________________________________________\n",
            "dense_114 (Dense)            (None, 128)               256128    \n",
            "_________________________________________________________________\n",
            "dense_115 (Dense)            (None, 89)                11481     \n",
            "=================================================================\n",
            "Total params: 767,609\n",
            "Trainable params: 767,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPpNWjtpgQka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "52be9963-00b9-4dfe-9b47-6b545d5f9c63"
      },
      "source": [
        "print(X_padded_seq.shape, y.shape)\n",
        "text_model.fit(X_padded_seq, y, epochs=5)"
      ],
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 40) (5, 1)\n",
            "Train on 5 samples\n",
            "Epoch 1/5\n",
            "5/5 [==============================] - 0s 59ms/sample - loss: 4.5128 - accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 0s 1ms/sample - loss: 4.3439 - accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 0s 1ms/sample - loss: 4.2064 - accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 0s 1ms/sample - loss: 4.0677 - accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 0s 1ms/sample - loss: 3.9130 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6efe362ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 345
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF191PQCxbdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "87079411-a692-4be9-9fbc-0ddc84a8a4a3"
      },
      "source": [
        "test_input=['he was sad']\n",
        "test_X_seq=tokenizer.texts_to_sequences(test_input)\n",
        "print(test_X_seq)\n",
        "\n",
        "test_X_padded_seq=pad_sequences(test_X_seq,padding='post',maxlen=40)\n",
        "print(test_X_padded_seq)\n",
        "\n",
        "output = text_model.predict_classes(test_X_padded_seq)\n",
        "print(output)\n",
        "for word, index in tokenizer_emotion.word_index.items():\n",
        "  if index==output:\n",
        "    print(word, index, output)\n",
        "    break"
      ],
      "execution_count": 349,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[11, 8, 1]]\n",
            "[[11  8  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
            "[74]\n",
            "happy 74 [74]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XGR9Ukf2GJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zP2vDsB2GMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10Imo1ef2GN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN2pgZyTyqrK",
        "colab_type": "text"
      },
      "source": [
        "Questions:\n",
        "1. Do we need seeding the model?\n",
        "2. HW:\n",
        " a. Use pretrained embeddings\n",
        " **b. use functional apis**\n",
        " c. Limit output to the emotions set(say 10)\n",
        " d. Look at other sources of data(emotion-cause or emotion-entailments)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxoQxxIyEASK",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}