{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BachChorales_HandsOnChapter-15.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar-abhishek/handson-ml2/blob/master/Normalized_BachChorales_HandsOnChapter_15_fix1_MSE_fix2_normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fmXPTqmLxe9",
        "colab_type": "text"
      },
      "source": [
        "**Q10[part-1]. Download the Bach chorales dataset and unzip it. It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long, and each time step contains 4 integers, where each integer corresponds to a note’s index on a piano (except for the value 0, which means that no note is played). Train a model—recurrent, convolutional, or both—that can predict the next time step (four notes), given a sequence of time steps from a chorale. **bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k9Phc0e7EFM",
        "colab_type": "code",
        "outputId": "c52c3791-0de4-4b14-ff54-e42344a18a37",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7d178ceb-1819-4cfd-800d-653669609b6e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7d178ceb-1819-4cfd-800d-653669609b6e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving chorale_229.csv to chorale_229.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfrFpSvRUFUD",
        "colab_type": "code",
        "outputId": "842192c2-2740-426b-c8c3-f6ced06e6cef",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "uploaded_validation = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8ff49e8a-515e-4764-b78b-727cdb88af80\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8ff49e8a-515e-4764-b78b-727cdb88af80\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving chorale_000.csv to chorale_000.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpnbslo39PR_",
        "colab_type": "code",
        "outputId": "8b9a80d5-e643-4af4-c334-c9b2ceb0c07e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "X_train_000 = pd.read_csv('chorale_000.csv')\n",
        "print(X_train_000.head(10))\n",
        "\n",
        "X_valid_229 = pd.read_csv('chorale_229.csv')\n",
        "print(X_valid_229.head(10))\n",
        "\n",
        "X_train_000 = X_train_000.to_numpy()\n",
        "X_valid_229 = X_valid_229.to_numpy()\n",
        "\n",
        "dataX=X_train_000[:-1]\n",
        "dataY=X_train_000[1:]\n",
        "dataValidationX=X_valid_229[:-1]\n",
        "dataValidationY=X_valid_229[1:]\n",
        "\n",
        "print(dataX[0:10])\n",
        "print(dataY[0:10])\n",
        "print(type(dataX))\n",
        "\n",
        "scaler = MinMaxScaler( feature_range=(0, 1) )\n",
        "dataXScaler = scaler.fit_transform(dataX)\n",
        "dataYScaler = scaler.fit_transform(dataY)\n",
        "print(dataXScaler[0:10])\n",
        "print(dataYScaler[0:10])\n",
        "\n",
        "inversedX = scaler.inverse_transform(dataXScaler[0:10])\n",
        "print('inversed:')\n",
        "print(inversedX[0:10])\n",
        "print(scaler.inverse_transform(dataXScaler)[2:30])\n",
        "\n",
        "scalerValidation = MinMaxScaler( feature_range=(0, 1) )\n",
        "dataValidationX = scalerValidation.fit_transform(dataValidationX)\n",
        "dataValidationY = scalerValidation.fit_transform(dataValidationY)\n",
        "print(dataValidationX[0:5])\n",
        "print(dataValidationY[0:5])\n",
        "\n",
        "\n",
        "print(scaler.inverse_transform(dataXScaler)[2:30])\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   note0  note1  note2  note3\n",
            "0     74     70     65     58\n",
            "1     74     70     65     58\n",
            "2     74     70     65     58\n",
            "3     74     70     65     58\n",
            "4     75     70     58     55\n",
            "5     75     70     58     55\n",
            "6     75     70     60     55\n",
            "7     75     70     60     55\n",
            "8     77     69     62     50\n",
            "9     77     69     62     50\n",
            "   note0  note1  note2  note3\n",
            "0     72     67     60     48\n",
            "1     72     67     60     48\n",
            "2     72     67     60     48\n",
            "3     72     67     60     48\n",
            "4     72     67     64     48\n",
            "5     72     67     64     48\n",
            "6     72     67     64     50\n",
            "7     72     67     64     50\n",
            "8     72     67     64     52\n",
            "9     72     67     64     52\n",
            "[[74 70 65 58]\n",
            " [74 70 65 58]\n",
            " [74 70 65 58]\n",
            " [74 70 65 58]\n",
            " [75 70 58 55]\n",
            " [75 70 58 55]\n",
            " [75 70 60 55]\n",
            " [75 70 60 55]\n",
            " [77 69 62 50]\n",
            " [77 69 62 50]]\n",
            "[[74 70 65 58]\n",
            " [74 70 65 58]\n",
            " [74 70 65 58]\n",
            " [75 70 58 55]\n",
            " [75 70 58 55]\n",
            " [75 70 60 55]\n",
            " [75 70 60 55]\n",
            " [77 69 62 50]\n",
            " [77 69 62 50]\n",
            " [77 69 62 50]]\n",
            "<class 'numpy.ndarray'>\n",
            "[[0.44444444 0.77777778 1.         0.88235294]\n",
            " [0.44444444 0.77777778 1.         0.88235294]\n",
            " [0.44444444 0.77777778 1.         0.88235294]\n",
            " [0.44444444 0.77777778 1.         0.88235294]\n",
            " [0.55555556 0.77777778 0.41666667 0.70588235]\n",
            " [0.55555556 0.77777778 0.41666667 0.70588235]\n",
            " [0.55555556 0.77777778 0.58333333 0.70588235]\n",
            " [0.55555556 0.77777778 0.58333333 0.70588235]\n",
            " [0.77777778 0.66666667 0.75       0.41176471]\n",
            " [0.77777778 0.66666667 0.75       0.41176471]]\n",
            "[[0.44444444 0.77777778 1.         0.88235294]\n",
            " [0.44444444 0.77777778 1.         0.88235294]\n",
            " [0.44444444 0.77777778 1.         0.88235294]\n",
            " [0.55555556 0.77777778 0.41666667 0.70588235]\n",
            " [0.55555556 0.77777778 0.41666667 0.70588235]\n",
            " [0.55555556 0.77777778 0.58333333 0.70588235]\n",
            " [0.55555556 0.77777778 0.58333333 0.70588235]\n",
            " [0.77777778 0.66666667 0.75       0.41176471]\n",
            " [0.77777778 0.66666667 0.75       0.41176471]\n",
            " [0.77777778 0.66666667 0.75       0.41176471]]\n",
            "inversed:\n",
            "[[74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]]\n",
            "[[74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]]\n",
            "[[0.41666667 0.375      0.41666667 0.2       ]\n",
            " [0.41666667 0.375      0.41666667 0.2       ]\n",
            " [0.41666667 0.375      0.41666667 0.2       ]\n",
            " [0.41666667 0.375      0.41666667 0.2       ]\n",
            " [0.41666667 0.375      0.75       0.2       ]]\n",
            "[[0.41666667 0.375      0.41666667 0.2       ]\n",
            " [0.41666667 0.375      0.41666667 0.2       ]\n",
            " [0.41666667 0.375      0.41666667 0.2       ]\n",
            " [0.41666667 0.375      0.75       0.2       ]\n",
            " [0.41666667 0.375      0.75       0.2       ]]\n",
            "[[74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RwyZ5FX-t2l",
        "colab_type": "code",
        "outputId": "8de3443c-ebc3-4178-f5c8-0433182a8f2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# reference: https://machinelearningmastery.com/how-to-use-the-timeseriesgenerator-for-time-series-forecasting-in-keras/\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "# make length as a hyperparameter and find it's optimal value via GridSearchCV?\n",
        "generator = TimeseriesGenerator(dataXScaler, dataYScaler, length=3, batch_size=32)\n",
        "validation_generator = TimeseriesGenerator(dataValidationX, dataValidationY, length=3, batch_size=32)\n",
        "print(generator[0])\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([[[0.44444444, 0.77777778, 1.        , 0.88235294],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.88235294],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.88235294]],\n",
            "\n",
            "       [[0.44444444, 0.77777778, 1.        , 0.88235294],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.88235294],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.88235294]],\n",
            "\n",
            "       [[0.44444444, 0.77777778, 1.        , 0.88235294],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.88235294],\n",
            "        [0.55555556, 0.77777778, 0.41666667, 0.70588235]],\n",
            "\n",
            "       [[0.44444444, 0.77777778, 1.        , 0.88235294],\n",
            "        [0.55555556, 0.77777778, 0.41666667, 0.70588235],\n",
            "        [0.55555556, 0.77777778, 0.41666667, 0.70588235]],\n",
            "\n",
            "       [[0.55555556, 0.77777778, 0.41666667, 0.70588235],\n",
            "        [0.55555556, 0.77777778, 0.41666667, 0.70588235],\n",
            "        [0.55555556, 0.77777778, 0.58333333, 0.70588235]],\n",
            "\n",
            "       [[0.55555556, 0.77777778, 0.41666667, 0.70588235],\n",
            "        [0.55555556, 0.77777778, 0.58333333, 0.70588235],\n",
            "        [0.55555556, 0.77777778, 0.58333333, 0.70588235]],\n",
            "\n",
            "       [[0.55555556, 0.77777778, 0.58333333, 0.70588235],\n",
            "        [0.55555556, 0.77777778, 0.58333333, 0.70588235],\n",
            "        [0.77777778, 0.66666667, 0.75      , 0.41176471]],\n",
            "\n",
            "       [[0.55555556, 0.77777778, 0.58333333, 0.70588235],\n",
            "        [0.77777778, 0.66666667, 0.75      , 0.41176471],\n",
            "        [0.77777778, 0.66666667, 0.75      , 0.41176471]],\n",
            "\n",
            "       [[0.77777778, 0.66666667, 0.75      , 0.41176471],\n",
            "        [0.77777778, 0.66666667, 0.75      , 0.41176471],\n",
            "        [0.77777778, 0.66666667, 0.75      , 0.41176471]],\n",
            "\n",
            "       [[0.77777778, 0.66666667, 0.75      , 0.41176471],\n",
            "        [0.77777778, 0.66666667, 0.75      , 0.41176471],\n",
            "        [0.77777778, 0.66666667, 0.75      , 0.41176471]],\n",
            "\n",
            "       [[0.77777778, 0.66666667, 0.75      , 0.41176471],\n",
            "        [0.77777778, 0.66666667, 0.75      , 0.41176471],\n",
            "        [0.77777778, 0.77777778, 0.75      , 0.70588235]],\n",
            "\n",
            "       [[0.77777778, 0.66666667, 0.75      , 0.41176471],\n",
            "        [0.77777778, 0.77777778, 0.75      , 0.70588235],\n",
            "        [0.77777778, 0.77777778, 0.75      , 0.70588235]],\n",
            "\n",
            "       [[0.77777778, 0.77777778, 0.75      , 0.70588235],\n",
            "        [0.77777778, 0.77777778, 0.75      , 0.70588235],\n",
            "        [0.77777778, 0.66666667, 0.75      , 0.70588235]],\n",
            "\n",
            "       [[0.77777778, 0.77777778, 0.75      , 0.70588235],\n",
            "        [0.77777778, 0.66666667, 0.75      , 0.70588235],\n",
            "        [0.77777778, 0.66666667, 0.75      , 0.70588235]],\n",
            "\n",
            "       [[0.77777778, 0.66666667, 0.75      , 0.70588235],\n",
            "        [0.77777778, 0.66666667, 0.75      , 0.70588235],\n",
            "        [0.55555556, 0.44444444, 0.83333333, 0.29411765]],\n",
            "\n",
            "       [[0.77777778, 0.66666667, 0.75      , 0.70588235],\n",
            "        [0.55555556, 0.44444444, 0.83333333, 0.29411765],\n",
            "        [0.55555556, 0.44444444, 0.83333333, 0.29411765]],\n",
            "\n",
            "       [[0.55555556, 0.44444444, 0.83333333, 0.29411765],\n",
            "        [0.55555556, 0.44444444, 0.83333333, 0.29411765],\n",
            "        [0.55555556, 0.66666667, 0.83333333, 0.29411765]],\n",
            "\n",
            "       [[0.55555556, 0.44444444, 0.83333333, 0.29411765],\n",
            "        [0.55555556, 0.66666667, 0.83333333, 0.29411765],\n",
            "        [0.55555556, 0.66666667, 0.83333333, 0.29411765]],\n",
            "\n",
            "       [[0.55555556, 0.66666667, 0.83333333, 0.29411765],\n",
            "        [0.55555556, 0.66666667, 0.83333333, 0.29411765],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.17647059]],\n",
            "\n",
            "       [[0.55555556, 0.66666667, 0.83333333, 0.29411765],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.17647059]],\n",
            "\n",
            "       [[0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.17647059]],\n",
            "\n",
            "       [[0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.17647059]],\n",
            "\n",
            "       [[0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529]],\n",
            "\n",
            "       [[0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529]],\n",
            "\n",
            "       [[0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529]],\n",
            "\n",
            "       [[0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529]],\n",
            "\n",
            "       [[0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529]],\n",
            "\n",
            "       [[0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529]],\n",
            "\n",
            "       [[0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529]],\n",
            "\n",
            "       [[0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529]],\n",
            "\n",
            "       [[0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.17647059]],\n",
            "\n",
            "       [[0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "        [0.44444444, 0.77777778, 1.        , 0.17647059]]]), array([[0.55555556, 0.77777778, 0.41666667, 0.70588235],\n",
            "       [0.55555556, 0.77777778, 0.41666667, 0.70588235],\n",
            "       [0.55555556, 0.77777778, 0.58333333, 0.70588235],\n",
            "       [0.55555556, 0.77777778, 0.58333333, 0.70588235],\n",
            "       [0.77777778, 0.66666667, 0.75      , 0.41176471],\n",
            "       [0.77777778, 0.66666667, 0.75      , 0.41176471],\n",
            "       [0.77777778, 0.66666667, 0.75      , 0.41176471],\n",
            "       [0.77777778, 0.66666667, 0.75      , 0.41176471],\n",
            "       [0.77777778, 0.77777778, 0.75      , 0.70588235],\n",
            "       [0.77777778, 0.77777778, 0.75      , 0.70588235],\n",
            "       [0.77777778, 0.66666667, 0.75      , 0.70588235],\n",
            "       [0.77777778, 0.66666667, 0.75      , 0.70588235],\n",
            "       [0.55555556, 0.44444444, 0.83333333, 0.29411765],\n",
            "       [0.55555556, 0.44444444, 0.83333333, 0.29411765],\n",
            "       [0.55555556, 0.66666667, 0.83333333, 0.29411765],\n",
            "       [0.55555556, 0.66666667, 0.83333333, 0.29411765],\n",
            "       [0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "       [0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "       [0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "       [0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "       [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "       [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "       [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "       [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "       [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "       [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "       [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "       [0.22222222, 0.66666667, 1.        , 0.58823529],\n",
            "       [0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "       [0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "       [0.44444444, 0.77777778, 1.        , 0.17647059],\n",
            "       [0.44444444, 0.77777778, 1.        , 0.17647059]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5kzBrOIC6gB",
        "colab_type": "code",
        "outputId": "b2bae579-2f90-4e5b-9122-a7ee77b5a1b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, TimeDistributed\n",
        "# the dataset knows the number of features, e.g. 2\n",
        "n_features = 4\n",
        "\n",
        "# choose a number of time steps\n",
        "n_steps = None\n",
        "\n",
        "model = Sequential()\n",
        "model.add(TimeDistributed(Dense(128), input_shape=(None, n_features)))\n",
        "model.add(LSTM(64, activation='tanh', input_shape=(None, n_features), return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(LSTM(32 , activation = 'tanh'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(n_features, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# fit model\n",
        "model.fit_generator(generator, epochs=500, validation_data=validation_generator)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 3s 421ms/step - loss: 0.1612 - val_loss: 0.1786\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1348 - val_loss: 0.1266\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1284 - val_loss: 0.1248\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1253 - val_loss: 0.1269\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1203 - val_loss: 0.1234\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1225 - val_loss: 0.1211\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1218 - val_loss: 0.1227\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1164 - val_loss: 0.1222\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1202 - val_loss: 0.1196\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1141 - val_loss: 0.1194\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1136 - val_loss: 0.1188\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1152 - val_loss: 0.1194\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1144 - val_loss: 0.1195\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1136 - val_loss: 0.1193\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1119 - val_loss: 0.1218\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1143 - val_loss: 0.1252\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1102 - val_loss: 0.1229\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1117 - val_loss: 0.1234\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1126 - val_loss: 0.1375\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1129 - val_loss: 0.1316\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1090 - val_loss: 0.1222\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1111 - val_loss: 0.1235\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1082 - val_loss: 0.1233\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1115 - val_loss: 0.1257\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1113 - val_loss: 0.1289\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.1079 - val_loss: 0.1250\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1104 - val_loss: 0.1305\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1093 - val_loss: 0.1334\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1126 - val_loss: 0.1292\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1059 - val_loss: 0.1293\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1082 - val_loss: 0.1306\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1059 - val_loss: 0.1222\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1098 - val_loss: 0.1273\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1097 - val_loss: 0.1485\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1104 - val_loss: 0.1390\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1088 - val_loss: 0.1289\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1069 - val_loss: 0.1274\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1067 - val_loss: 0.1256\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1064 - val_loss: 0.1290\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1100 - val_loss: 0.1286\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1075 - val_loss: 0.1315\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1067 - val_loss: 0.1229\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1059 - val_loss: 0.1260\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1071 - val_loss: 0.1343\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1065 - val_loss: 0.1285\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1065 - val_loss: 0.1252\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1067 - val_loss: 0.1241\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1057 - val_loss: 0.1261\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1073 - val_loss: 0.1225\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1047 - val_loss: 0.1289\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1041 - val_loss: 0.1274\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1052 - val_loss: 0.1224\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1018 - val_loss: 0.1245\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1038 - val_loss: 0.1273\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1053 - val_loss: 0.1229\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1046 - val_loss: 0.1195\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1039 - val_loss: 0.1209\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1047 - val_loss: 0.1225\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1052 - val_loss: 0.1240\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1059 - val_loss: 0.1227\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1022 - val_loss: 0.1228\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1023 - val_loss: 0.1279\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1042 - val_loss: 0.1305\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1027 - val_loss: 0.1291\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1043 - val_loss: 0.1239\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1031 - val_loss: 0.1240\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1040 - val_loss: 0.1262\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1030 - val_loss: 0.1266\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1015 - val_loss: 0.1238\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1004 - val_loss: 0.1255\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1021 - val_loss: 0.1243\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1030 - val_loss: 0.1188\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.1032 - val_loss: 0.1248\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1021 - val_loss: 0.1322\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1027 - val_loss: 0.1261\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1029 - val_loss: 0.1307\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1033 - val_loss: 0.1327\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1023 - val_loss: 0.1263\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1020 - val_loss: 0.1230\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1010 - val_loss: 0.1242\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1013 - val_loss: 0.1281\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1020 - val_loss: 0.1299\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1014 - val_loss: 0.1302\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1027 - val_loss: 0.1313\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1012 - val_loss: 0.1332\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1011 - val_loss: 0.1342\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0995 - val_loss: 0.1336\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1003 - val_loss: 0.1286\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.1010 - val_loss: 0.1251\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1012 - val_loss: 0.1290\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1005 - val_loss: 0.1322\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0995 - val_loss: 0.1283\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0990 - val_loss: 0.1255\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1008 - val_loss: 0.1293\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0997 - val_loss: 0.1347\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0999 - val_loss: 0.1302\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0987 - val_loss: 0.1302\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0989 - val_loss: 0.1347\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1011 - val_loss: 0.1335\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.1000 - val_loss: 0.1287\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1012 - val_loss: 0.1269\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0984 - val_loss: 0.1299\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1001 - val_loss: 0.1311\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0994 - val_loss: 0.1324\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0994 - val_loss: 0.1322\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0992 - val_loss: 0.1328\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0987 - val_loss: 0.1326\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0981 - val_loss: 0.1320\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0999 - val_loss: 0.1301\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0991 - val_loss: 0.1296\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0986 - val_loss: 0.1236\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0978 - val_loss: 0.1246\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1009 - val_loss: 0.1315\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0984 - val_loss: 0.1312\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0989 - val_loss: 0.1287\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0994 - val_loss: 0.1284\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0977 - val_loss: 0.1320\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0984 - val_loss: 0.1354\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0979 - val_loss: 0.1340\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0987 - val_loss: 0.1314\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0972 - val_loss: 0.1270\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0972 - val_loss: 0.1248\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0970 - val_loss: 0.1258\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0970 - val_loss: 0.1265\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0991 - val_loss: 0.1234\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 0.0988 - val_loss: 0.1233\n",
            "Epoch 127/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0992 - val_loss: 0.1288\n",
            "Epoch 128/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0967 - val_loss: 0.1296\n",
            "Epoch 129/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0982 - val_loss: 0.1265\n",
            "Epoch 130/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0962 - val_loss: 0.1276\n",
            "Epoch 131/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0984 - val_loss: 0.1276\n",
            "Epoch 132/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0975 - val_loss: 0.1270\n",
            "Epoch 133/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0978 - val_loss: 0.1287\n",
            "Epoch 134/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0974 - val_loss: 0.1297\n",
            "Epoch 135/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0961 - val_loss: 0.1280\n",
            "Epoch 136/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0971 - val_loss: 0.1274\n",
            "Epoch 137/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0969 - val_loss: 0.1295\n",
            "Epoch 138/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0963 - val_loss: 0.1286\n",
            "Epoch 139/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0955 - val_loss: 0.1284\n",
            "Epoch 140/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0966 - val_loss: 0.1274\n",
            "Epoch 141/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0970 - val_loss: 0.1286\n",
            "Epoch 142/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0972 - val_loss: 0.1310\n",
            "Epoch 143/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0973 - val_loss: 0.1268\n",
            "Epoch 144/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0969 - val_loss: 0.1282\n",
            "Epoch 145/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0962 - val_loss: 0.1292\n",
            "Epoch 146/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0969 - val_loss: 0.1290\n",
            "Epoch 147/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0965 - val_loss: 0.1293\n",
            "Epoch 148/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0959 - val_loss: 0.1280\n",
            "Epoch 149/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0965 - val_loss: 0.1281\n",
            "Epoch 150/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0970 - val_loss: 0.1284\n",
            "Epoch 151/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0958 - val_loss: 0.1285\n",
            "Epoch 152/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0973 - val_loss: 0.1306\n",
            "Epoch 153/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0964 - val_loss: 0.1283\n",
            "Epoch 154/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0972 - val_loss: 0.1259\n",
            "Epoch 155/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0955 - val_loss: 0.1254\n",
            "Epoch 156/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0952 - val_loss: 0.1265\n",
            "Epoch 157/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0956 - val_loss: 0.1256\n",
            "Epoch 158/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0951 - val_loss: 0.1276\n",
            "Epoch 159/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0951 - val_loss: 0.1287\n",
            "Epoch 160/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0961 - val_loss: 0.1260\n",
            "Epoch 161/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0960 - val_loss: 0.1256\n",
            "Epoch 162/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0953 - val_loss: 0.1244\n",
            "Epoch 163/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0948 - val_loss: 0.1242\n",
            "Epoch 164/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0963 - val_loss: 0.1236\n",
            "Epoch 165/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0959 - val_loss: 0.1260\n",
            "Epoch 166/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0954 - val_loss: 0.1256\n",
            "Epoch 167/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0954 - val_loss: 0.1240\n",
            "Epoch 168/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0950 - val_loss: 0.1254\n",
            "Epoch 169/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0949 - val_loss: 0.1249\n",
            "Epoch 170/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0961 - val_loss: 0.1231\n",
            "Epoch 171/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0949 - val_loss: 0.1244\n",
            "Epoch 172/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0943 - val_loss: 0.1246\n",
            "Epoch 173/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0952 - val_loss: 0.1255\n",
            "Epoch 174/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0952 - val_loss: 0.1280\n",
            "Epoch 175/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0956 - val_loss: 0.1290\n",
            "Epoch 176/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0959 - val_loss: 0.1277\n",
            "Epoch 177/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0941 - val_loss: 0.1247\n",
            "Epoch 178/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0952 - val_loss: 0.1256\n",
            "Epoch 179/500\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0948 - val_loss: 0.1239\n",
            "Epoch 180/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0942 - val_loss: 0.1217\n",
            "Epoch 181/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0947 - val_loss: 0.1238\n",
            "Epoch 182/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0941 - val_loss: 0.1268\n",
            "Epoch 183/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0945 - val_loss: 0.1260\n",
            "Epoch 184/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0944 - val_loss: 0.1237\n",
            "Epoch 185/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0946 - val_loss: 0.1253\n",
            "Epoch 186/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0950 - val_loss: 0.1281\n",
            "Epoch 187/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0944 - val_loss: 0.1282\n",
            "Epoch 188/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0945 - val_loss: 0.1254\n",
            "Epoch 189/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0947 - val_loss: 0.1282\n",
            "Epoch 190/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0941 - val_loss: 0.1328\n",
            "Epoch 191/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0953 - val_loss: 0.1286\n",
            "Epoch 192/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0946 - val_loss: 0.1261\n",
            "Epoch 193/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0939 - val_loss: 0.1256\n",
            "Epoch 194/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0946 - val_loss: 0.1252\n",
            "Epoch 195/500\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0940 - val_loss: 0.1246\n",
            "Epoch 196/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0953 - val_loss: 0.1265\n",
            "Epoch 197/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0946 - val_loss: 0.1323\n",
            "Epoch 198/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0946 - val_loss: 0.1287\n",
            "Epoch 199/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0938 - val_loss: 0.1256\n",
            "Epoch 200/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0935 - val_loss: 0.1246\n",
            "Epoch 201/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0946 - val_loss: 0.1251\n",
            "Epoch 202/500\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0956 - val_loss: 0.1283\n",
            "Epoch 203/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0945 - val_loss: 0.1275\n",
            "Epoch 204/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0945 - val_loss: 0.1309\n",
            "Epoch 205/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0940 - val_loss: 0.1313\n",
            "Epoch 206/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0944 - val_loss: 0.1258\n",
            "Epoch 207/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0940 - val_loss: 0.1263\n",
            "Epoch 208/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0950 - val_loss: 0.1277\n",
            "Epoch 209/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0941 - val_loss: 0.1254\n",
            "Epoch 210/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0944 - val_loss: 0.1266\n",
            "Epoch 211/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0946 - val_loss: 0.1278\n",
            "Epoch 212/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0936 - val_loss: 0.1252\n",
            "Epoch 213/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0933 - val_loss: 0.1249\n",
            "Epoch 214/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0937 - val_loss: 0.1290\n",
            "Epoch 215/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0945 - val_loss: 0.1265\n",
            "Epoch 216/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0937 - val_loss: 0.1261\n",
            "Epoch 217/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0946 - val_loss: 0.1299\n",
            "Epoch 218/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0937 - val_loss: 0.1302\n",
            "Epoch 219/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0931 - val_loss: 0.1281\n",
            "Epoch 220/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0929 - val_loss: 0.1286\n",
            "Epoch 221/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0933 - val_loss: 0.1277\n",
            "Epoch 222/500\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.0930 - val_loss: 0.1276\n",
            "Epoch 223/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.0938 - val_loss: 0.1274\n",
            "Epoch 224/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0933 - val_loss: 0.1292\n",
            "Epoch 225/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0933 - val_loss: 0.1295\n",
            "Epoch 226/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0947 - val_loss: 0.1268\n",
            "Epoch 227/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0955 - val_loss: 0.1301\n",
            "Epoch 228/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0937 - val_loss: 0.1330\n",
            "Epoch 229/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0941 - val_loss: 0.1311\n",
            "Epoch 230/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0940 - val_loss: 0.1293\n",
            "Epoch 231/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0935 - val_loss: 0.1305\n",
            "Epoch 232/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0937 - val_loss: 0.1300\n",
            "Epoch 233/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0939 - val_loss: 0.1289\n",
            "Epoch 234/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0932 - val_loss: 0.1292\n",
            "Epoch 235/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0933 - val_loss: 0.1297\n",
            "Epoch 236/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0934 - val_loss: 0.1282\n",
            "Epoch 237/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0936 - val_loss: 0.1279\n",
            "Epoch 238/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0936 - val_loss: 0.1291\n",
            "Epoch 239/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0942 - val_loss: 0.1320\n",
            "Epoch 240/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0933 - val_loss: 0.1226\n",
            "Epoch 241/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0937 - val_loss: 0.1215\n",
            "Epoch 242/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0936 - val_loss: 0.1218\n",
            "Epoch 243/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0930 - val_loss: 0.1227\n",
            "Epoch 244/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0939 - val_loss: 0.1272\n",
            "Epoch 245/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0933 - val_loss: 0.1315\n",
            "Epoch 246/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0930 - val_loss: 0.1296\n",
            "Epoch 247/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0931 - val_loss: 0.1262\n",
            "Epoch 248/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0930 - val_loss: 0.1294\n",
            "Epoch 249/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0927 - val_loss: 0.1298\n",
            "Epoch 250/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0929 - val_loss: 0.1295\n",
            "Epoch 251/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0928 - val_loss: 0.1312\n",
            "Epoch 252/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0926 - val_loss: 0.1294\n",
            "Epoch 253/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0933 - val_loss: 0.1259\n",
            "Epoch 254/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0938 - val_loss: 0.1258\n",
            "Epoch 255/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0925 - val_loss: 0.1255\n",
            "Epoch 256/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0931 - val_loss: 0.1271\n",
            "Epoch 257/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0920 - val_loss: 0.1279\n",
            "Epoch 258/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0933 - val_loss: 0.1273\n",
            "Epoch 259/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0927 - val_loss: 0.1288\n",
            "Epoch 260/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0930 - val_loss: 0.1314\n",
            "Epoch 261/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0926 - val_loss: 0.1302\n",
            "Epoch 262/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0928 - val_loss: 0.1282\n",
            "Epoch 263/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0921 - val_loss: 0.1291\n",
            "Epoch 264/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0932 - val_loss: 0.1313\n",
            "Epoch 265/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0931 - val_loss: 0.1319\n",
            "Epoch 266/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0931 - val_loss: 0.1304\n",
            "Epoch 267/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0935 - val_loss: 0.1315\n",
            "Epoch 268/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0933 - val_loss: 0.1325\n",
            "Epoch 269/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0927 - val_loss: 0.1336\n",
            "Epoch 270/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0927 - val_loss: 0.1312\n",
            "Epoch 271/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0926 - val_loss: 0.1305\n",
            "Epoch 272/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0924 - val_loss: 0.1302\n",
            "Epoch 273/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0925 - val_loss: 0.1279\n",
            "Epoch 274/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0937 - val_loss: 0.1286\n",
            "Epoch 275/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0932 - val_loss: 0.1308\n",
            "Epoch 276/500\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 0.0931 - val_loss: 0.1285\n",
            "Epoch 277/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0934 - val_loss: 0.1283\n",
            "Epoch 278/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0932 - val_loss: 0.1355\n",
            "Epoch 279/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0934 - val_loss: 0.1329\n",
            "Epoch 280/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0924 - val_loss: 0.1284\n",
            "Epoch 281/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0929 - val_loss: 0.1319\n",
            "Epoch 282/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0929 - val_loss: 0.1338\n",
            "Epoch 283/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0926 - val_loss: 0.1317\n",
            "Epoch 284/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0924 - val_loss: 0.1312\n",
            "Epoch 285/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0924 - val_loss: 0.1340\n",
            "Epoch 286/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0922 - val_loss: 0.1347\n",
            "Epoch 287/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0923 - val_loss: 0.1308\n",
            "Epoch 288/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0923 - val_loss: 0.1312\n",
            "Epoch 289/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0922 - val_loss: 0.1340\n",
            "Epoch 290/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0922 - val_loss: 0.1312\n",
            "Epoch 291/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0930 - val_loss: 0.1279\n",
            "Epoch 292/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0921 - val_loss: 0.1305\n",
            "Epoch 293/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0929 - val_loss: 0.1306\n",
            "Epoch 294/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0925 - val_loss: 0.1299\n",
            "Epoch 295/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0926 - val_loss: 0.1310\n",
            "Epoch 296/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0931 - val_loss: 0.1315\n",
            "Epoch 297/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0929 - val_loss: 0.1301\n",
            "Epoch 298/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0924 - val_loss: 0.1317\n",
            "Epoch 299/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0927 - val_loss: 0.1338\n",
            "Epoch 300/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0921 - val_loss: 0.1326\n",
            "Epoch 301/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0927 - val_loss: 0.1311\n",
            "Epoch 302/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0924 - val_loss: 0.1315\n",
            "Epoch 303/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0929 - val_loss: 0.1310\n",
            "Epoch 304/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0935 - val_loss: 0.1319\n",
            "Epoch 305/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0922 - val_loss: 0.1328\n",
            "Epoch 306/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0925 - val_loss: 0.1316\n",
            "Epoch 307/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0926 - val_loss: 0.1314\n",
            "Epoch 308/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0924 - val_loss: 0.1330\n",
            "Epoch 309/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0932 - val_loss: 0.1322\n",
            "Epoch 310/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0926 - val_loss: 0.1290\n",
            "Epoch 311/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0933 - val_loss: 0.1290\n",
            "Epoch 312/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0925 - val_loss: 0.1315\n",
            "Epoch 313/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0926 - val_loss: 0.1285\n",
            "Epoch 314/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0926 - val_loss: 0.1256\n",
            "Epoch 315/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0921 - val_loss: 0.1279\n",
            "Epoch 316/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0919 - val_loss: 0.1305\n",
            "Epoch 317/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0923 - val_loss: 0.1281\n",
            "Epoch 318/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0922 - val_loss: 0.1268\n",
            "Epoch 319/500\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0921 - val_loss: 0.1289\n",
            "Epoch 320/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0917 - val_loss: 0.1313\n",
            "Epoch 321/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0919 - val_loss: 0.1295\n",
            "Epoch 322/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0928 - val_loss: 0.1288\n",
            "Epoch 323/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0921 - val_loss: 0.1290\n",
            "Epoch 324/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0920 - val_loss: 0.1270\n",
            "Epoch 325/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0916 - val_loss: 0.1260\n",
            "Epoch 326/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0921 - val_loss: 0.1263\n",
            "Epoch 327/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0925 - val_loss: 0.1283\n",
            "Epoch 328/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0922 - val_loss: 0.1266\n",
            "Epoch 329/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0926 - val_loss: 0.1280\n",
            "Epoch 330/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0921 - val_loss: 0.1328\n",
            "Epoch 331/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0922 - val_loss: 0.1311\n",
            "Epoch 332/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0923 - val_loss: 0.1296\n",
            "Epoch 333/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0917 - val_loss: 0.1321\n",
            "Epoch 334/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0920 - val_loss: 0.1313\n",
            "Epoch 335/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0918 - val_loss: 0.1327\n",
            "Epoch 336/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0918 - val_loss: 0.1314\n",
            "Epoch 337/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0921 - val_loss: 0.1309\n",
            "Epoch 338/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0918 - val_loss: 0.1313\n",
            "Epoch 339/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0913 - val_loss: 0.1313\n",
            "Epoch 340/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0926 - val_loss: 0.1307\n",
            "Epoch 341/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0924 - val_loss: 0.1296\n",
            "Epoch 342/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0918 - val_loss: 0.1305\n",
            "Epoch 343/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0922 - val_loss: 0.1299\n",
            "Epoch 344/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0918 - val_loss: 0.1289\n",
            "Epoch 345/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0918 - val_loss: 0.1298\n",
            "Epoch 346/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0924 - val_loss: 0.1278\n",
            "Epoch 347/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0917 - val_loss: 0.1277\n",
            "Epoch 348/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0915 - val_loss: 0.1271\n",
            "Epoch 349/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0918 - val_loss: 0.1270\n",
            "Epoch 350/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0924 - val_loss: 0.1290\n",
            "Epoch 351/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0921 - val_loss: 0.1295\n",
            "Epoch 352/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0918 - val_loss: 0.1282\n",
            "Epoch 353/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0921 - val_loss: 0.1266\n",
            "Epoch 354/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0924 - val_loss: 0.1259\n",
            "Epoch 355/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0920 - val_loss: 0.1278\n",
            "Epoch 356/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0916 - val_loss: 0.1330\n",
            "Epoch 357/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0913 - val_loss: 0.1321\n",
            "Epoch 358/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0927 - val_loss: 0.1276\n",
            "Epoch 359/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0921 - val_loss: 0.1279\n",
            "Epoch 360/500\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.0919 - val_loss: 0.1284\n",
            "Epoch 361/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0920 - val_loss: 0.1273\n",
            "Epoch 362/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0921 - val_loss: 0.1314\n",
            "Epoch 363/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0922 - val_loss: 0.1318\n",
            "Epoch 364/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0920 - val_loss: 0.1291\n",
            "Epoch 365/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0921 - val_loss: 0.1272\n",
            "Epoch 366/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0923 - val_loss: 0.1292\n",
            "Epoch 367/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0917 - val_loss: 0.1281\n",
            "Epoch 368/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0915 - val_loss: 0.1267\n",
            "Epoch 369/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0911 - val_loss: 0.1258\n",
            "Epoch 370/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0920 - val_loss: 0.1271\n",
            "Epoch 371/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0914 - val_loss: 0.1275\n",
            "Epoch 372/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0914 - val_loss: 0.1289\n",
            "Epoch 373/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0915 - val_loss: 0.1293\n",
            "Epoch 374/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0919 - val_loss: 0.1284\n",
            "Epoch 375/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0918 - val_loss: 0.1275\n",
            "Epoch 376/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0917 - val_loss: 0.1273\n",
            "Epoch 377/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0913 - val_loss: 0.1282\n",
            "Epoch 378/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0914 - val_loss: 0.1285\n",
            "Epoch 379/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0920 - val_loss: 0.1258\n",
            "Epoch 380/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0917 - val_loss: 0.1295\n",
            "Epoch 381/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0922 - val_loss: 0.1325\n",
            "Epoch 382/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0912 - val_loss: 0.1277\n",
            "Epoch 383/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0918 - val_loss: 0.1273\n",
            "Epoch 384/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0912 - val_loss: 0.1266\n",
            "Epoch 385/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0914 - val_loss: 0.1267\n",
            "Epoch 386/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0908 - val_loss: 0.1294\n",
            "Epoch 387/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0909 - val_loss: 0.1308\n",
            "Epoch 388/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0915 - val_loss: 0.1294\n",
            "Epoch 389/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0914 - val_loss: 0.1261\n",
            "Epoch 390/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0909 - val_loss: 0.1268\n",
            "Epoch 391/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0916 - val_loss: 0.1305\n",
            "Epoch 392/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0920 - val_loss: 0.1299\n",
            "Epoch 393/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0924 - val_loss: 0.1279\n",
            "Epoch 394/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0922 - val_loss: 0.1304\n",
            "Epoch 395/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0917 - val_loss: 0.1301\n",
            "Epoch 396/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0914 - val_loss: 0.1270\n",
            "Epoch 397/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0910 - val_loss: 0.1247\n",
            "Epoch 398/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0924 - val_loss: 0.1280\n",
            "Epoch 399/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0916 - val_loss: 0.1313\n",
            "Epoch 400/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0915 - val_loss: 0.1298\n",
            "Epoch 401/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0913 - val_loss: 0.1261\n",
            "Epoch 402/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0913 - val_loss: 0.1281\n",
            "Epoch 403/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0911 - val_loss: 0.1295\n",
            "Epoch 404/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0920 - val_loss: 0.1285\n",
            "Epoch 405/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0919 - val_loss: 0.1299\n",
            "Epoch 406/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0911 - val_loss: 0.1307\n",
            "Epoch 407/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0921 - val_loss: 0.1309\n",
            "Epoch 408/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0916 - val_loss: 0.1299\n",
            "Epoch 409/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0910 - val_loss: 0.1314\n",
            "Epoch 410/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0920 - val_loss: 0.1312\n",
            "Epoch 411/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0913 - val_loss: 0.1300\n",
            "Epoch 412/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0910 - val_loss: 0.1292\n",
            "Epoch 413/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0910 - val_loss: 0.1308\n",
            "Epoch 414/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0914 - val_loss: 0.1322\n",
            "Epoch 415/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0919 - val_loss: 0.1307\n",
            "Epoch 416/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0920 - val_loss: 0.1295\n",
            "Epoch 417/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0911 - val_loss: 0.1299\n",
            "Epoch 418/500\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.0916 - val_loss: 0.1292\n",
            "Epoch 419/500\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.0913 - val_loss: 0.1275\n",
            "Epoch 420/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0917 - val_loss: 0.1299\n",
            "Epoch 421/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0913 - val_loss: 0.1355\n",
            "Epoch 422/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0918 - val_loss: 0.1332\n",
            "Epoch 423/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0907 - val_loss: 0.1285\n",
            "Epoch 424/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0918 - val_loss: 0.1292\n",
            "Epoch 425/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0912 - val_loss: 0.1319\n",
            "Epoch 426/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0913 - val_loss: 0.1292\n",
            "Epoch 427/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0912 - val_loss: 0.1273\n",
            "Epoch 428/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0914 - val_loss: 0.1268\n",
            "Epoch 429/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0911 - val_loss: 0.1302\n",
            "Epoch 430/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0912 - val_loss: 0.1354\n",
            "Epoch 431/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0917 - val_loss: 0.1319\n",
            "Epoch 432/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0913 - val_loss: 0.1297\n",
            "Epoch 433/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0922 - val_loss: 0.1292\n",
            "Epoch 434/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0911 - val_loss: 0.1299\n",
            "Epoch 435/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0911 - val_loss: 0.1294\n",
            "Epoch 436/500\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 0.0913 - val_loss: 0.1300\n",
            "Epoch 437/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0912 - val_loss: 0.1302\n",
            "Epoch 438/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0912 - val_loss: 0.1282\n",
            "Epoch 439/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0915 - val_loss: 0.1246\n",
            "Epoch 440/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0914 - val_loss: 0.1252\n",
            "Epoch 441/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0914 - val_loss: 0.1265\n",
            "Epoch 442/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0916 - val_loss: 0.1272\n",
            "Epoch 443/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0909 - val_loss: 0.1262\n",
            "Epoch 444/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0912 - val_loss: 0.1286\n",
            "Epoch 445/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0914 - val_loss: 0.1298\n",
            "Epoch 446/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0911 - val_loss: 0.1281\n",
            "Epoch 447/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0906 - val_loss: 0.1262\n",
            "Epoch 448/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0913 - val_loss: 0.1261\n",
            "Epoch 449/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0908 - val_loss: 0.1257\n",
            "Epoch 450/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0910 - val_loss: 0.1253\n",
            "Epoch 451/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0912 - val_loss: 0.1265\n",
            "Epoch 452/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0916 - val_loss: 0.1285\n",
            "Epoch 453/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0913 - val_loss: 0.1287\n",
            "Epoch 454/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0911 - val_loss: 0.1276\n",
            "Epoch 455/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0921 - val_loss: 0.1291\n",
            "Epoch 456/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0912 - val_loss: 0.1284\n",
            "Epoch 457/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0912 - val_loss: 0.1256\n",
            "Epoch 458/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0908 - val_loss: 0.1256\n",
            "Epoch 459/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0916 - val_loss: 0.1263\n",
            "Epoch 460/500\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 0.0912 - val_loss: 0.1293\n",
            "Epoch 461/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0911 - val_loss: 0.1299\n",
            "Epoch 462/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0914 - val_loss: 0.1273\n",
            "Epoch 463/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0910 - val_loss: 0.1285\n",
            "Epoch 464/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0911 - val_loss: 0.1304\n",
            "Epoch 465/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0907 - val_loss: 0.1291\n",
            "Epoch 466/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0903 - val_loss: 0.1300\n",
            "Epoch 467/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0910 - val_loss: 0.1320\n",
            "Epoch 468/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0904 - val_loss: 0.1309\n",
            "Epoch 469/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0908 - val_loss: 0.1308\n",
            "Epoch 470/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0912 - val_loss: 0.1309\n",
            "Epoch 471/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0907 - val_loss: 0.1318\n",
            "Epoch 472/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0911 - val_loss: 0.1287\n",
            "Epoch 473/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0906 - val_loss: 0.1276\n",
            "Epoch 474/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0912 - val_loss: 0.1277\n",
            "Epoch 475/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0911 - val_loss: 0.1307\n",
            "Epoch 476/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0908 - val_loss: 0.1305\n",
            "Epoch 477/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0911 - val_loss: 0.1291\n",
            "Epoch 478/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0908 - val_loss: 0.1302\n",
            "Epoch 479/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0915 - val_loss: 0.1301\n",
            "Epoch 480/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0915 - val_loss: 0.1310\n",
            "Epoch 481/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0911 - val_loss: 0.1303\n",
            "Epoch 482/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0907 - val_loss: 0.1305\n",
            "Epoch 483/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0902 - val_loss: 0.1304\n",
            "Epoch 484/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0909 - val_loss: 0.1318\n",
            "Epoch 485/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0905 - val_loss: 0.1320\n",
            "Epoch 486/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0908 - val_loss: 0.1292\n",
            "Epoch 487/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0911 - val_loss: 0.1298\n",
            "Epoch 488/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0908 - val_loss: 0.1313\n",
            "Epoch 489/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0909 - val_loss: 0.1302\n",
            "Epoch 490/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0907 - val_loss: 0.1317\n",
            "Epoch 491/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0908 - val_loss: 0.1320\n",
            "Epoch 492/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0911 - val_loss: 0.1323\n",
            "Epoch 493/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0916 - val_loss: 0.1308\n",
            "Epoch 494/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0909 - val_loss: 0.1302\n",
            "Epoch 495/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0906 - val_loss: 0.1305\n",
            "Epoch 496/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 0.0904 - val_loss: 0.1292\n",
            "Epoch 497/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.0904 - val_loss: 0.1278\n",
            "Epoch 498/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.0907 - val_loss: 0.1287\n",
            "Epoch 499/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 0.0910 - val_loss: 0.1300\n",
            "Epoch 500/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.0911 - val_loss: 0.1301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe2269a82b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJHLBiJEVd8r",
        "colab_type": "code",
        "outputId": "537db7f4-11cd-4e22-f380-1335b1fa8097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# demonstrate prediction\n",
        "print(scaler.inverse_transform(dataXScaler)[2:30])\n",
        "\n",
        "x_input = dataXScaler[2:30]\n",
        "x_input = x_input.reshape((1, len(x_input), 4))\n",
        "print(scaler.inverse_transform(dataXScaler)[2:30])\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "print(scaler.inverse_transform(yhat))\n",
        "print('expected: ', scaler.inverse_transform(dataYScaler)[30])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]]\n",
            "[[74. 70. 65. 58.]\n",
            " [74. 70. 65. 58.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 58. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [75. 70. 60. 55.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 69. 62. 50.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 70. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [77. 69. 62. 55.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 67. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [75. 69. 63. 48.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [74. 70. 65. 46.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]\n",
            " [72. 69. 65. 53.]]\n",
            "[[71.882805 66.64405  54.83745  46.95733 ]]\n",
            "expected:  [72. 69. 65. 53.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpvggFUajEDg",
        "colab_type": "text"
      },
      "source": [
        "**Q10[part-2] Then use this model to generate Bach-like music, one note at a time: you can do this by giving the model the start of a chorale and asking it to predict the next time step, then appending these time steps to the input sequence and asking the model for the next note, and so on. Also make sure to check out Google’s Coconet model, which was used for a nice Google doodle about Bach.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do-eQEQTQaJj",
        "colab_type": "code",
        "outputId": "f2367385-c537-430e-f344-22307d47c9e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# choose a number of time steps\n",
        "n_steps = None\n",
        "# convert into input/output\n",
        "i=1\n",
        "n=1\n",
        "x_input = np.array([dataXScaler[0]])\n",
        "\n",
        "x_input = x_input.reshape((1, len(x_input), n_features))\n",
        "while i<len(dataXScaler):\n",
        "  # demonstrate prediction\n",
        "  print('Input: ')\n",
        "  for input in x_input:\n",
        "    print(scaler.inverse_transform(input))\n",
        "  print('---------------')\n",
        "  yhat = model.predict(x_input, verbose=1)\n",
        "  print('Predicted Output: ', scaler.inverse_transform(yhat))\n",
        "  print('expected: ', scaler.inverse_transform(dataYScaler)[i])\n",
        "  print('\\n\\n')\n",
        "\n",
        "  yhat = yhat.reshape((1, len(yhat), n_features))\n",
        "  x_input = np.concatenate([x_input, yhat], axis=1)\n",
        "  i += 1\n",
        "  if i>20:\n",
        "    break"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            "[[74. 70. 65. 58.]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[71.65973  69.55494  53.853416 43.2744  ]]\n",
            "expected:  [74. 70. 65. 58.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[71.34236 68.22179 53.20636 47.30871]]\n",
            "expected:  [74. 70. 65. 58.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[70.95206 66.34743 53.17611 51.62926]]\n",
            "expected:  [75. 70. 58. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[70.98377  66.420784 53.559643 50.88745 ]]\n",
            "expected:  [75. 70. 58. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicted Output:  [[71.25659  66.7      53.981842 49.246613]]\n",
            "expected:  [75. 70. 60. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]\n",
            " [71.25659226 66.69999495 53.98184317 49.246613  ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicted Output:  [[71.45928  66.742615 54.00107  48.75601 ]]\n",
            "expected:  [75. 70. 60. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]\n",
            " [71.25659226 66.69999495 53.98184317 49.246613  ]\n",
            " [71.45928384 66.74261418 54.00107247 48.75600713]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicted Output:  [[71.66646  66.812645 54.086166 48.111843]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]\n",
            " [71.25659226 66.69999495 53.98184317 49.246613  ]\n",
            " [71.45928384 66.74261418 54.00107247 48.75600713]\n",
            " [71.66645552 66.81264746 54.08616984 48.11184341]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[71.852516 66.889984 54.226967 47.414864]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]\n",
            " [71.25659226 66.69999495 53.98184317 49.246613  ]\n",
            " [71.45928384 66.74261418 54.00107247 48.75600713]\n",
            " [71.66645552 66.81264746 54.08616984 48.11184341]\n",
            " [71.85251097 66.88998199 54.22696722 47.41486454]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicted Output:  [[71.99175  66.90008  54.404537 46.881226]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]\n",
            " [71.25659226 66.69999495 53.98184317 49.246613  ]\n",
            " [71.45928384 66.74261418 54.00107247 48.75600713]\n",
            " [71.66645552 66.81264746 54.08616984 48.11184341]\n",
            " [71.85251097 66.88998199 54.22696722 47.41486454]\n",
            " [71.99175598 66.90007702 54.40453562 46.8812232 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[72.097855 66.82489  54.55175  46.614277]]\n",
            "expected:  [77. 69. 62. 50.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]\n",
            " [71.25659226 66.69999495 53.98184317 49.246613  ]\n",
            " [71.45928384 66.74261418 54.00107247 48.75600713]\n",
            " [71.66645552 66.81264746 54.08616984 48.11184341]\n",
            " [71.85251097 66.88998199 54.22696722 47.41486454]\n",
            " [71.99175598 66.90007702 54.40453562 46.8812232 ]\n",
            " [72.09785599 66.82489014 54.55174983 46.61427891]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[72.19141  66.583954 54.831425 46.496468]]\n",
            "expected:  [77. 70. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]\n",
            " [71.25659226 66.69999495 53.98184317 49.246613  ]\n",
            " [71.45928384 66.74261418 54.00107247 48.75600713]\n",
            " [71.66645552 66.81264746 54.08616984 48.11184341]\n",
            " [71.85251097 66.88998199 54.22696722 47.41486454]\n",
            " [71.99175598 66.90007702 54.40453562 46.8812232 ]\n",
            " [72.09785599 66.82489014 54.55174983 46.61427891]\n",
            " [72.1914054  66.58395481 54.83142358 46.49646826]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[72.212654 66.100945 55.44858  46.49437 ]]\n",
            "expected:  [77. 70. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]\n",
            " [71.25659226 66.69999495 53.98184317 49.246613  ]\n",
            " [71.45928384 66.74261418 54.00107247 48.75600713]\n",
            " [71.66645552 66.81264746 54.08616984 48.11184341]\n",
            " [71.85251097 66.88998199 54.22696722 47.41486454]\n",
            " [71.99175598 66.90007702 54.40453562 46.8812232 ]\n",
            " [72.09785599 66.82489014 54.55174983 46.61427891]\n",
            " [72.1914054  66.58395481 54.83142358 46.49646826]\n",
            " [72.21265614 66.10094798 55.44857985 46.49436976]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[72.17097  65.48241  56.37505  46.428993]]\n",
            "expected:  [77. 69. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]\n",
            " [71.25659226 66.69999495 53.98184317 49.246613  ]\n",
            " [71.45928384 66.74261418 54.00107247 48.75600713]\n",
            " [71.66645552 66.81264746 54.08616984 48.11184341]\n",
            " [71.85251097 66.88998199 54.22696722 47.41486454]\n",
            " [71.99175598 66.90007702 54.40453562 46.8812232 ]\n",
            " [72.09785599 66.82489014 54.55174983 46.61427891]\n",
            " [72.1914054  66.58395481 54.83142358 46.49646826]\n",
            " [72.21265614 66.10094798 55.44857985 46.49436976]\n",
            " [72.17096281 65.48240161 56.37504971 46.42899199]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[72.16536  65.128006 56.814663 46.486202]]\n",
            "expected:  [77. 69. 62. 55.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]\n",
            " [71.25659226 66.69999495 53.98184317 49.246613  ]\n",
            " [71.45928384 66.74261418 54.00107247 48.75600713]\n",
            " [71.66645552 66.81264746 54.08616984 48.11184341]\n",
            " [71.85251097 66.88998199 54.22696722 47.41486454]\n",
            " [71.99175598 66.90007702 54.40453562 46.8812232 ]\n",
            " [72.09785599 66.82489014 54.55174983 46.61427891]\n",
            " [72.1914054  66.58395481 54.83142358 46.49646826]\n",
            " [72.21265614 66.10094798 55.44857985 46.49436976]\n",
            " [72.17096281 65.48240161 56.37504971 46.42899199]\n",
            " [72.16536236 65.12800522 56.81466055 46.48620248]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[72.181076 65.01665  56.987057 46.422634]]\n",
            "expected:  [75. 67. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]\n",
            " [71.25659226 66.69999495 53.98184317 49.246613  ]\n",
            " [71.45928384 66.74261418 54.00107247 48.75600713]\n",
            " [71.66645552 66.81264746 54.08616984 48.11184341]\n",
            " [71.85251097 66.88998199 54.22696722 47.41486454]\n",
            " [71.99175598 66.90007702 54.40453562 46.8812232 ]\n",
            " [72.09785599 66.82489014 54.55174983 46.61427891]\n",
            " [72.1914054  66.58395481 54.83142358 46.49646826]\n",
            " [72.21265614 66.10094798 55.44857985 46.49436976]\n",
            " [72.17096281 65.48240161 56.37504971 46.42899199]\n",
            " [72.16536236 65.12800522 56.81466055 46.48620248]\n",
            " [72.1810797  65.0166446  56.98705685 46.42263442]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[72.203926 64.973625 57.05096  46.370213]]\n",
            "expected:  [75. 67. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]\n",
            " [71.25659226 66.69999495 53.98184317 49.246613  ]\n",
            " [71.45928384 66.74261418 54.00107247 48.75600713]\n",
            " [71.66645552 66.81264746 54.08616984 48.11184341]\n",
            " [71.85251097 66.88998199 54.22696722 47.41486454]\n",
            " [71.99175598 66.90007702 54.40453562 46.8812232 ]\n",
            " [72.09785599 66.82489014 54.55174983 46.61427891]\n",
            " [72.1914054  66.58395481 54.83142358 46.49646826]\n",
            " [72.21265614 66.10094798 55.44857985 46.49436976]\n",
            " [72.17096281 65.48240161 56.37504971 46.42899199]\n",
            " [72.16536236 65.12800522 56.81466055 46.48620248]\n",
            " [72.1810797  65.0166446  56.98705685 46.42263442]\n",
            " [72.20392528 64.97362357 57.05096281 46.37021066]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[72.24595  64.94416  57.072453 46.316055]]\n",
            "expected:  [75. 69. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]\n",
            " [71.25659226 66.69999495 53.98184317 49.246613  ]\n",
            " [71.45928384 66.74261418 54.00107247 48.75600713]\n",
            " [71.66645552 66.81264746 54.08616984 48.11184341]\n",
            " [71.85251097 66.88998199 54.22696722 47.41486454]\n",
            " [71.99175598 66.90007702 54.40453562 46.8812232 ]\n",
            " [72.09785599 66.82489014 54.55174983 46.61427891]\n",
            " [72.1914054  66.58395481 54.83142358 46.49646826]\n",
            " [72.21265614 66.10094798 55.44857985 46.49436976]\n",
            " [72.17096281 65.48240161 56.37504971 46.42899199]\n",
            " [72.16536236 65.12800522 56.81466055 46.48620248]\n",
            " [72.1810797  65.0166446  56.98705685 46.42263442]\n",
            " [72.20392528 64.97362357 57.05096281 46.37021066]\n",
            " [72.24594584 64.94415803 57.0724498  46.31605521]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[72.20717  64.993034 57.02536  46.36368 ]]\n",
            "expected:  [75. 69. 63. 48.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]\n",
            " [71.25659226 66.69999495 53.98184317 49.246613  ]\n",
            " [71.45928384 66.74261418 54.00107247 48.75600713]\n",
            " [71.66645552 66.81264746 54.08616984 48.11184341]\n",
            " [71.85251097 66.88998199 54.22696722 47.41486454]\n",
            " [71.99175598 66.90007702 54.40453562 46.8812232 ]\n",
            " [72.09785599 66.82489014 54.55174983 46.61427891]\n",
            " [72.1914054  66.58395481 54.83142358 46.49646826]\n",
            " [72.21265614 66.10094798 55.44857985 46.49436976]\n",
            " [72.17096281 65.48240161 56.37504971 46.42899199]\n",
            " [72.16536236 65.12800522 56.81466055 46.48620248]\n",
            " [72.1810797  65.0166446  56.98705685 46.42263442]\n",
            " [72.20392528 64.97362357 57.05096281 46.37021066]\n",
            " [72.24594584 64.94415803 57.0724498  46.31605521]\n",
            " [72.2071725  64.99303378 57.0253613  46.36368236]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[72.07592  65.13565  56.924965 46.48445 ]]\n",
            "expected:  [74. 70. 65. 46.]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[74.         70.         65.         58.        ]\n",
            " [71.65972653 69.55493957 53.85341865 43.27439876]\n",
            " [71.34236649 68.22178406 53.20635723 47.30870888]\n",
            " [70.95205525 66.34743011 53.17610913 51.62926197]\n",
            " [70.98377258 66.4207845  53.5596447  50.88745159]\n",
            " [71.25659226 66.69999495 53.98184317 49.246613  ]\n",
            " [71.45928384 66.74261418 54.00107247 48.75600713]\n",
            " [71.66645552 66.81264746 54.08616984 48.11184341]\n",
            " [71.85251097 66.88998199 54.22696722 47.41486454]\n",
            " [71.99175598 66.90007702 54.40453562 46.8812232 ]\n",
            " [72.09785599 66.82489014 54.55174983 46.61427891]\n",
            " [72.1914054  66.58395481 54.83142358 46.49646826]\n",
            " [72.21265614 66.10094798 55.44857985 46.49436976]\n",
            " [72.17096281 65.48240161 56.37504971 46.42899199]\n",
            " [72.16536236 65.12800522 56.81466055 46.48620248]\n",
            " [72.1810797  65.0166446  56.98705685 46.42263442]\n",
            " [72.20392528 64.97362357 57.05096281 46.37021066]\n",
            " [72.24594584 64.94415803 57.0724498  46.31605521]\n",
            " [72.2071725  64.99303378 57.0253613  46.36368236]\n",
            " [72.07591927 65.13564764 56.9249655  46.48445001]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[71.92222  65.31458  56.78239  46.638763]]\n",
            "expected:  [74. 70. 65. 46.]\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjgMqOxYXRvf",
        "colab_type": "code",
        "outputId": "966c1522-e8ee-43d7-f182-b0b39b99cc18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Dense, SimpleRNN\n",
        "\n",
        "# Using SimpleRNN \n",
        "print(n_features)\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(TimeDistributed(Dense(128), input_shape=(None, n_features)))\n",
        "model_rnn.add(SimpleRNN(100, input_shape=[None, n_features], return_sequences=True ))\n",
        "model_rnn.add(SimpleRNN(100))\n",
        "model_rnn.add(Dense(n_features, activation='softmax'))\n",
        "\n",
        "model_rnn.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "model_rnn.fit_generator(generator, epochs=500, validation_data=validation_generator)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "Epoch 1/500\n",
            "6/6 [==============================] - 2s 410ms/step - loss: 2.8488 - val_loss: 2.7542\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 2.6689 - val_loss: 2.6543\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6477 - val_loss: 2.6706\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6340 - val_loss: 2.6723\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6356 - val_loss: 2.6434\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6209 - val_loss: 2.6438\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6028 - val_loss: 2.6420\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6022 - val_loss: 2.6250\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6090 - val_loss: 2.6255\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5977 - val_loss: 2.6263\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5938 - val_loss: 2.6195\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5889 - val_loss: 2.6214\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5863 - val_loss: 2.6190\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5851 - val_loss: 2.6162\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5858 - val_loss: 2.6320\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.5849 - val_loss: 2.6204\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5833 - val_loss: 2.6198\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5819 - val_loss: 2.6292\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5805 - val_loss: 2.6189\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5847 - val_loss: 2.6216\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5800 - val_loss: 2.6113\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5812 - val_loss: 2.6223\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5781 - val_loss: 2.6180\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5783 - val_loss: 2.6276\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5775 - val_loss: 2.6173\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5778 - val_loss: 2.6380\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5810 - val_loss: 2.6118\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5864 - val_loss: 2.6294\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5796 - val_loss: 2.6126\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5773 - val_loss: 2.6285\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5816 - val_loss: 2.6174\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5786 - val_loss: 2.6098\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5746 - val_loss: 2.6197\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5755 - val_loss: 2.6112\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5723 - val_loss: 2.6246\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5701 - val_loss: 2.6138\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5714 - val_loss: 2.6256\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5759 - val_loss: 2.6181\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5744 - val_loss: 2.6145\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5741 - val_loss: 2.6183\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5748 - val_loss: 2.6303\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5754 - val_loss: 2.6203\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5776 - val_loss: 2.6351\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5807 - val_loss: 2.6155\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5787 - val_loss: 2.6277\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5727 - val_loss: 2.6204\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5692 - val_loss: 2.6185\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5678 - val_loss: 2.6280\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5662 - val_loss: 2.6191\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5665 - val_loss: 2.6267\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5683 - val_loss: 2.6233\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5684 - val_loss: 2.6277\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5692 - val_loss: 2.6388\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5707 - val_loss: 2.6464\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5803 - val_loss: 2.6569\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5753 - val_loss: 2.6550\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5882 - val_loss: 2.6784\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5947 - val_loss: 2.6380\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5994 - val_loss: 2.6313\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5896 - val_loss: 2.6277\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5808 - val_loss: 2.6320\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5682 - val_loss: 2.6412\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5681 - val_loss: 2.6281\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5658 - val_loss: 2.6256\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5630 - val_loss: 2.6388\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5610 - val_loss: 2.6358\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5616 - val_loss: 2.6244\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5616 - val_loss: 2.6440\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5643 - val_loss: 2.6393\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5591 - val_loss: 2.6459\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5596 - val_loss: 2.6334\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5576 - val_loss: 2.6459\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5582 - val_loss: 2.6370\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5581 - val_loss: 2.6488\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5573 - val_loss: 2.6501\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5593 - val_loss: 2.6422\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5581 - val_loss: 2.6392\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5569 - val_loss: 2.6639\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5554 - val_loss: 2.6478\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5531 - val_loss: 2.6506\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5557 - val_loss: 2.6549\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5542 - val_loss: 2.6655\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5536 - val_loss: 2.6490\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5540 - val_loss: 2.6593\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5547 - val_loss: 2.6572\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5553 - val_loss: 2.6584\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5528 - val_loss: 2.6602\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5563 - val_loss: 2.6629\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5594 - val_loss: 2.6713\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5551 - val_loss: 2.6710\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5576 - val_loss: 2.6701\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5530 - val_loss: 2.6702\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5577 - val_loss: 2.6711\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5560 - val_loss: 2.6701\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5574 - val_loss: 2.6681\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5570 - val_loss: 2.6546\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5523 - val_loss: 2.6805\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5509 - val_loss: 2.6669\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5483 - val_loss: 2.6667\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5534 - val_loss: 2.6698\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5541 - val_loss: 2.6755\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5539 - val_loss: 2.6641\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5611 - val_loss: 2.6920\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5614 - val_loss: 2.6785\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5674 - val_loss: 2.6618\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5567 - val_loss: 2.6864\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5624 - val_loss: 2.6846\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5527 - val_loss: 2.6665\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5548 - val_loss: 2.6588\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5547 - val_loss: 2.6983\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5508 - val_loss: 2.6628\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5497 - val_loss: 2.6718\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.5503 - val_loss: 2.6738\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5461 - val_loss: 2.6559\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5489 - val_loss: 2.6791\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5469 - val_loss: 2.6861\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5462 - val_loss: 2.6609\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5452 - val_loss: 2.6770\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5449 - val_loss: 2.6853\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5443 - val_loss: 2.6739\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5451 - val_loss: 2.6789\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5445 - val_loss: 2.6930\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5432 - val_loss: 2.6745\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5453 - val_loss: 2.6771\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5434 - val_loss: 2.6926\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5411 - val_loss: 2.6743\n",
            "Epoch 127/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5433 - val_loss: 2.6803\n",
            "Epoch 128/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5414 - val_loss: 2.6903\n",
            "Epoch 129/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5436 - val_loss: 2.6755\n",
            "Epoch 130/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5440 - val_loss: 2.6831\n",
            "Epoch 131/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5428 - val_loss: 2.6921\n",
            "Epoch 132/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5431 - val_loss: 2.6634\n",
            "Epoch 133/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5441 - val_loss: 2.7191\n",
            "Epoch 134/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5481 - val_loss: 2.6605\n",
            "Epoch 135/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5486 - val_loss: 2.6836\n",
            "Epoch 136/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5496 - val_loss: 2.7516\n",
            "Epoch 137/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5491 - val_loss: 2.6539\n",
            "Epoch 138/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5507 - val_loss: 2.6918\n",
            "Epoch 139/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5455 - val_loss: 2.7112\n",
            "Epoch 140/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5486 - val_loss: 2.6741\n",
            "Epoch 141/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5455 - val_loss: 2.6926\n",
            "Epoch 142/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5447 - val_loss: 2.6955\n",
            "Epoch 143/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5447 - val_loss: 2.6868\n",
            "Epoch 144/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5485 - val_loss: 2.7029\n",
            "Epoch 145/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5444 - val_loss: 2.6896\n",
            "Epoch 146/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5417 - val_loss: 2.6851\n",
            "Epoch 147/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5546 - val_loss: 2.6941\n",
            "Epoch 148/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5595 - val_loss: 2.7673\n",
            "Epoch 149/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5517 - val_loss: 2.6743\n",
            "Epoch 150/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5528 - val_loss: 2.6935\n",
            "Epoch 151/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5504 - val_loss: 2.6878\n",
            "Epoch 152/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5570 - val_loss: 2.7306\n",
            "Epoch 153/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5503 - val_loss: 2.6776\n",
            "Epoch 154/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5476 - val_loss: 2.7016\n",
            "Epoch 155/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5515 - val_loss: 2.6955\n",
            "Epoch 156/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5501 - val_loss: 2.6949\n",
            "Epoch 157/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5485 - val_loss: 2.7041\n",
            "Epoch 158/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5478 - val_loss: 2.6935\n",
            "Epoch 159/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5452 - val_loss: 2.6874\n",
            "Epoch 160/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5416 - val_loss: 2.6997\n",
            "Epoch 161/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5421 - val_loss: 2.7015\n",
            "Epoch 162/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5401 - val_loss: 2.6825\n",
            "Epoch 163/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5388 - val_loss: 2.6842\n",
            "Epoch 164/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5390 - val_loss: 2.7125\n",
            "Epoch 165/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5370 - val_loss: 2.6886\n",
            "Epoch 166/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5376 - val_loss: 2.6967\n",
            "Epoch 167/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5358 - val_loss: 2.7022\n",
            "Epoch 168/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5373 - val_loss: 2.7036\n",
            "Epoch 169/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5429 - val_loss: 2.6996\n",
            "Epoch 170/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5415 - val_loss: 2.6998\n",
            "Epoch 171/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5448 - val_loss: 2.7031\n",
            "Epoch 172/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5415 - val_loss: 2.7034\n",
            "Epoch 173/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5357 - val_loss: 2.7060\n",
            "Epoch 174/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5365 - val_loss: 2.7052\n",
            "Epoch 175/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5355 - val_loss: 2.7095\n",
            "Epoch 176/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5344 - val_loss: 2.6922\n",
            "Epoch 177/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5344 - val_loss: 2.7241\n",
            "Epoch 178/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5355 - val_loss: 2.6930\n",
            "Epoch 179/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5355 - val_loss: 2.7224\n",
            "Epoch 180/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5346 - val_loss: 2.7085\n",
            "Epoch 181/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5343 - val_loss: 2.7112\n",
            "Epoch 182/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5356 - val_loss: 2.7068\n",
            "Epoch 183/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5371 - val_loss: 2.7217\n",
            "Epoch 184/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5370 - val_loss: 2.7082\n",
            "Epoch 185/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5369 - val_loss: 2.7390\n",
            "Epoch 186/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5407 - val_loss: 2.7195\n",
            "Epoch 187/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5359 - val_loss: 2.7153\n",
            "Epoch 188/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.5368 - val_loss: 2.6924\n",
            "Epoch 189/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5381 - val_loss: 2.7448\n",
            "Epoch 190/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5334 - val_loss: 2.7077\n",
            "Epoch 191/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5343 - val_loss: 2.7263\n",
            "Epoch 192/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5335 - val_loss: 2.7217\n",
            "Epoch 193/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5323 - val_loss: 2.7200\n",
            "Epoch 194/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5326 - val_loss: 2.7201\n",
            "Epoch 195/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5306 - val_loss: 2.7299\n",
            "Epoch 196/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5327 - val_loss: 2.7215\n",
            "Epoch 197/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5320 - val_loss: 2.7290\n",
            "Epoch 198/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5310 - val_loss: 2.7378\n",
            "Epoch 199/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5326 - val_loss: 2.7320\n",
            "Epoch 200/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5309 - val_loss: 2.7337\n",
            "Epoch 201/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5314 - val_loss: 2.7359\n",
            "Epoch 202/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5331 - val_loss: 2.7414\n",
            "Epoch 203/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5318 - val_loss: 2.7194\n",
            "Epoch 204/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5374 - val_loss: 2.7437\n",
            "Epoch 205/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5347 - val_loss: 2.7438\n",
            "Epoch 206/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5331 - val_loss: 2.7337\n",
            "Epoch 207/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5328 - val_loss: 2.7517\n",
            "Epoch 208/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5381 - val_loss: 2.7288\n",
            "Epoch 209/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5374 - val_loss: 2.7410\n",
            "Epoch 210/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5323 - val_loss: 2.7573\n",
            "Epoch 211/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5314 - val_loss: 2.7285\n",
            "Epoch 212/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5295 - val_loss: 2.7552\n",
            "Epoch 213/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5297 - val_loss: 2.7385\n",
            "Epoch 214/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5302 - val_loss: 2.7466\n",
            "Epoch 215/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5290 - val_loss: 2.7358\n",
            "Epoch 216/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5350 - val_loss: 2.7778\n",
            "Epoch 217/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5303 - val_loss: 2.7172\n",
            "Epoch 218/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5324 - val_loss: 2.7708\n",
            "Epoch 219/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5337 - val_loss: 2.7308\n",
            "Epoch 220/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5298 - val_loss: 2.7844\n",
            "Epoch 221/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5332 - val_loss: 2.7114\n",
            "Epoch 222/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5376 - val_loss: 2.8047\n",
            "Epoch 223/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5379 - val_loss: 2.7295\n",
            "Epoch 224/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5444 - val_loss: 2.7937\n",
            "Epoch 225/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5462 - val_loss: 2.7677\n",
            "Epoch 226/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5393 - val_loss: 2.7470\n",
            "Epoch 227/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5347 - val_loss: 2.7525\n",
            "Epoch 228/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5361 - val_loss: 2.7735\n",
            "Epoch 229/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5325 - val_loss: 2.7665\n",
            "Epoch 230/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5360 - val_loss: 2.7045\n",
            "Epoch 231/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5354 - val_loss: 2.8255\n",
            "Epoch 232/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5325 - val_loss: 2.7503\n",
            "Epoch 233/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5371 - val_loss: 2.8007\n",
            "Epoch 234/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5342 - val_loss: 2.7311\n",
            "Epoch 235/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5337 - val_loss: 2.7555\n",
            "Epoch 236/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5340 - val_loss: 2.7659\n",
            "Epoch 237/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5280 - val_loss: 2.7691\n",
            "Epoch 238/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5295 - val_loss: 2.7556\n",
            "Epoch 239/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5305 - val_loss: 2.7761\n",
            "Epoch 240/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5274 - val_loss: 2.7638\n",
            "Epoch 241/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5289 - val_loss: 2.7701\n",
            "Epoch 242/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5292 - val_loss: 2.7462\n",
            "Epoch 243/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5281 - val_loss: 2.7682\n",
            "Epoch 244/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5258 - val_loss: 2.7801\n",
            "Epoch 245/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5266 - val_loss: 2.7773\n",
            "Epoch 246/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5270 - val_loss: 2.7885\n",
            "Epoch 247/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5319 - val_loss: 2.7766\n",
            "Epoch 248/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5360 - val_loss: 2.7770\n",
            "Epoch 249/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5336 - val_loss: 2.7788\n",
            "Epoch 250/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5316 - val_loss: 2.7884\n",
            "Epoch 251/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5309 - val_loss: 2.7780\n",
            "Epoch 252/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5315 - val_loss: 2.7700\n",
            "Epoch 253/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5288 - val_loss: 2.8060\n",
            "Epoch 254/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5343 - val_loss: 2.7826\n",
            "Epoch 255/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5267 - val_loss: 2.7732\n",
            "Epoch 256/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5284 - val_loss: 2.7869\n",
            "Epoch 257/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5258 - val_loss: 2.7660\n",
            "Epoch 258/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5258 - val_loss: 2.7971\n",
            "Epoch 259/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5250 - val_loss: 2.7769\n",
            "Epoch 260/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5273 - val_loss: 2.8072\n",
            "Epoch 261/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5255 - val_loss: 2.7725\n",
            "Epoch 262/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5312 - val_loss: 2.7950\n",
            "Epoch 263/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5314 - val_loss: 2.7665\n",
            "Epoch 264/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5405 - val_loss: 2.7957\n",
            "Epoch 265/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5432 - val_loss: 2.7554\n",
            "Epoch 266/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5449 - val_loss: 2.7619\n",
            "Epoch 267/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5425 - val_loss: 2.8197\n",
            "Epoch 268/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5337 - val_loss: 2.7382\n",
            "Epoch 269/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5353 - val_loss: 2.7755\n",
            "Epoch 270/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5278 - val_loss: 2.7459\n",
            "Epoch 271/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5272 - val_loss: 2.8125\n",
            "Epoch 272/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5283 - val_loss: 2.7899\n",
            "Epoch 273/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5256 - val_loss: 2.7723\n",
            "Epoch 274/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5235 - val_loss: 2.8066\n",
            "Epoch 275/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5232 - val_loss: 2.7674\n",
            "Epoch 276/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5235 - val_loss: 2.8112\n",
            "Epoch 277/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5225 - val_loss: 2.7753\n",
            "Epoch 278/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5244 - val_loss: 2.8056\n",
            "Epoch 279/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5249 - val_loss: 2.8021\n",
            "Epoch 280/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5236 - val_loss: 2.8241\n",
            "Epoch 281/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5214 - val_loss: 2.8068\n",
            "Epoch 282/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5216 - val_loss: 2.8161\n",
            "Epoch 283/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5234 - val_loss: 2.8008\n",
            "Epoch 284/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5213 - val_loss: 2.8291\n",
            "Epoch 285/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5229 - val_loss: 2.8069\n",
            "Epoch 286/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5237 - val_loss: 2.8132\n",
            "Epoch 287/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5232 - val_loss: 2.8255\n",
            "Epoch 288/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5230 - val_loss: 2.8188\n",
            "Epoch 289/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5248 - val_loss: 2.8169\n",
            "Epoch 290/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5226 - val_loss: 2.8345\n",
            "Epoch 291/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5212 - val_loss: 2.8184\n",
            "Epoch 292/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5207 - val_loss: 2.8232\n",
            "Epoch 293/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5200 - val_loss: 2.8333\n",
            "Epoch 294/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5216 - val_loss: 2.8176\n",
            "Epoch 295/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5240 - val_loss: 2.8393\n",
            "Epoch 296/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5234 - val_loss: 2.8405\n",
            "Epoch 297/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5220 - val_loss: 2.8228\n",
            "Epoch 298/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5238 - val_loss: 2.8260\n",
            "Epoch 299/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5265 - val_loss: 2.8686\n",
            "Epoch 300/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5219 - val_loss: 2.8360\n",
            "Epoch 301/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5239 - val_loss: 2.8179\n",
            "Epoch 302/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5217 - val_loss: 2.8455\n",
            "Epoch 303/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5211 - val_loss: 2.8525\n",
            "Epoch 304/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5220 - val_loss: 2.8376\n",
            "Epoch 305/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5211 - val_loss: 2.8281\n",
            "Epoch 306/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5222 - val_loss: 2.8363\n",
            "Epoch 307/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5222 - val_loss: 2.8549\n",
            "Epoch 308/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5237 - val_loss: 2.8418\n",
            "Epoch 309/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5204 - val_loss: 2.8306\n",
            "Epoch 310/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5217 - val_loss: 2.8562\n",
            "Epoch 311/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5205 - val_loss: 2.8341\n",
            "Epoch 312/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5192 - val_loss: 2.8492\n",
            "Epoch 313/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5190 - val_loss: 2.8492\n",
            "Epoch 314/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5211 - val_loss: 2.8600\n",
            "Epoch 315/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5204 - val_loss: 2.8433\n",
            "Epoch 316/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5221 - val_loss: 2.8801\n",
            "Epoch 317/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5206 - val_loss: 2.8418\n",
            "Epoch 318/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5195 - val_loss: 2.8574\n",
            "Epoch 319/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5200 - val_loss: 2.8576\n",
            "Epoch 320/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5198 - val_loss: 2.8720\n",
            "Epoch 321/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5217 - val_loss: 2.8367\n",
            "Epoch 322/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5219 - val_loss: 2.8942\n",
            "Epoch 323/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5216 - val_loss: 2.8436\n",
            "Epoch 324/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5214 - val_loss: 2.8620\n",
            "Epoch 325/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5193 - val_loss: 2.8735\n",
            "Epoch 326/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5184 - val_loss: 2.8471\n",
            "Epoch 327/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5177 - val_loss: 2.8888\n",
            "Epoch 328/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5189 - val_loss: 2.8625\n",
            "Epoch 329/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5195 - val_loss: 2.8769\n",
            "Epoch 330/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5201 - val_loss: 2.8499\n",
            "Epoch 331/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5227 - val_loss: 2.9174\n",
            "Epoch 332/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5194 - val_loss: 2.8644\n",
            "Epoch 333/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5210 - val_loss: 2.8948\n",
            "Epoch 334/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5214 - val_loss: 2.8497\n",
            "Epoch 335/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5226 - val_loss: 2.9083\n",
            "Epoch 336/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5218 - val_loss: 2.8589\n",
            "Epoch 337/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5205 - val_loss: 2.8454\n",
            "Epoch 338/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5203 - val_loss: 2.8611\n",
            "Epoch 339/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5184 - val_loss: 2.9135\n",
            "Epoch 340/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5162 - val_loss: 2.8559\n",
            "Epoch 341/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5171 - val_loss: 2.9127\n",
            "Epoch 342/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5179 - val_loss: 2.8682\n",
            "Epoch 343/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5201 - val_loss: 2.9046\n",
            "Epoch 344/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5189 - val_loss: 2.8503\n",
            "Epoch 345/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5186 - val_loss: 2.8860\n",
            "Epoch 346/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5172 - val_loss: 2.8785\n",
            "Epoch 347/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5165 - val_loss: 2.8847\n",
            "Epoch 348/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5156 - val_loss: 2.8850\n",
            "Epoch 349/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5150 - val_loss: 2.8961\n",
            "Epoch 350/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5166 - val_loss: 2.8952\n",
            "Epoch 351/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5158 - val_loss: 2.8961\n",
            "Epoch 352/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5216 - val_loss: 2.9045\n",
            "Epoch 353/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5205 - val_loss: 2.8510\n",
            "Epoch 354/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5172 - val_loss: 2.9189\n",
            "Epoch 355/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5162 - val_loss: 2.9006\n",
            "Epoch 356/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5167 - val_loss: 2.9373\n",
            "Epoch 357/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5207 - val_loss: 2.8502\n",
            "Epoch 358/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5224 - val_loss: 2.9609\n",
            "Epoch 359/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5186 - val_loss: 2.8792\n",
            "Epoch 360/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5205 - val_loss: 2.8941\n",
            "Epoch 361/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5200 - val_loss: 2.9048\n",
            "Epoch 362/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5168 - val_loss: 2.8939\n",
            "Epoch 363/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5170 - val_loss: 2.9248\n",
            "Epoch 364/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5171 - val_loss: 2.8778\n",
            "Epoch 365/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5157 - val_loss: 2.9665\n",
            "Epoch 366/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5182 - val_loss: 2.8666\n",
            "Epoch 367/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5190 - val_loss: 2.9416\n",
            "Epoch 368/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5180 - val_loss: 2.8919\n",
            "Epoch 369/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5202 - val_loss: 2.9754\n",
            "Epoch 370/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5239 - val_loss: 2.8383\n",
            "Epoch 371/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5340 - val_loss: 2.8896\n",
            "Epoch 372/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5343 - val_loss: 2.8520\n",
            "Epoch 373/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5266 - val_loss: 2.8453\n",
            "Epoch 374/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5213 - val_loss: 2.9177\n",
            "Epoch 375/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5243 - val_loss: 2.8884\n",
            "Epoch 376/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5253 - val_loss: 2.9301\n",
            "Epoch 377/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5181 - val_loss: 2.9441\n",
            "Epoch 378/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5231 - val_loss: 2.9053\n",
            "Epoch 379/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5284 - val_loss: 2.9041\n",
            "Epoch 380/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5259 - val_loss: 2.9675\n",
            "Epoch 381/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5254 - val_loss: 2.9062\n",
            "Epoch 382/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5227 - val_loss: 2.8959\n",
            "Epoch 383/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5215 - val_loss: 2.9224\n",
            "Epoch 384/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5179 - val_loss: 2.9226\n",
            "Epoch 385/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5171 - val_loss: 2.9118\n",
            "Epoch 386/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5142 - val_loss: 2.8964\n",
            "Epoch 387/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5144 - val_loss: 2.9110\n",
            "Epoch 388/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5156 - val_loss: 2.9546\n",
            "Epoch 389/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5130 - val_loss: 2.9043\n",
            "Epoch 390/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5142 - val_loss: 2.9471\n",
            "Epoch 391/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5135 - val_loss: 2.9073\n",
            "Epoch 392/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5143 - val_loss: 2.9639\n",
            "Epoch 393/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5132 - val_loss: 2.9104\n",
            "Epoch 394/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5159 - val_loss: 2.9499\n",
            "Epoch 395/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5165 - val_loss: 2.9340\n",
            "Epoch 396/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5180 - val_loss: 2.9470\n",
            "Epoch 397/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5202 - val_loss: 2.9293\n",
            "Epoch 398/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5155 - val_loss: 2.9193\n",
            "Epoch 399/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5156 - val_loss: 2.9448\n",
            "Epoch 400/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5135 - val_loss: 2.9366\n",
            "Epoch 401/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5141 - val_loss: 2.9416\n",
            "Epoch 402/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5123 - val_loss: 2.9579\n",
            "Epoch 403/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5137 - val_loss: 2.9458\n",
            "Epoch 404/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5129 - val_loss: 2.9495\n",
            "Epoch 405/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5123 - val_loss: 2.9606\n",
            "Epoch 406/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5124 - val_loss: 2.9476\n",
            "Epoch 407/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5134 - val_loss: 2.9519\n",
            "Epoch 408/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5139 - val_loss: 2.9621\n",
            "Epoch 409/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5120 - val_loss: 2.9491\n",
            "Epoch 410/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5143 - val_loss: 2.9756\n",
            "Epoch 411/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5135 - val_loss: 2.9223\n",
            "Epoch 412/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5142 - val_loss: 2.9806\n",
            "Epoch 413/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5132 - val_loss: 2.9392\n",
            "Epoch 414/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5144 - val_loss: 2.9949\n",
            "Epoch 415/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5131 - val_loss: 2.9506\n",
            "Epoch 416/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5165 - val_loss: 2.9857\n",
            "Epoch 417/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5159 - val_loss: 2.9181\n",
            "Epoch 418/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5169 - val_loss: 3.0455\n",
            "Epoch 419/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5181 - val_loss: 2.8423\n",
            "Epoch 420/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.5202 - val_loss: 3.0219\n",
            "Epoch 421/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5212 - val_loss: 2.8762\n",
            "Epoch 422/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5223 - val_loss: 2.9175\n",
            "Epoch 423/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5181 - val_loss: 2.9445\n",
            "Epoch 424/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5189 - val_loss: 2.9544\n",
            "Epoch 425/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5141 - val_loss: 2.9422\n",
            "Epoch 426/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5157 - val_loss: 2.9199\n",
            "Epoch 427/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5129 - val_loss: 2.9689\n",
            "Epoch 428/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5125 - val_loss: 2.9456\n",
            "Epoch 429/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5125 - val_loss: 2.9639\n",
            "Epoch 430/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5126 - val_loss: 2.9452\n",
            "Epoch 431/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5118 - val_loss: 2.9831\n",
            "Epoch 432/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5123 - val_loss: 2.9388\n",
            "Epoch 433/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5113 - val_loss: 2.9822\n",
            "Epoch 434/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5128 - val_loss: 2.9625\n",
            "Epoch 435/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5125 - val_loss: 2.9909\n",
            "Epoch 436/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5141 - val_loss: 2.9713\n",
            "Epoch 437/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5143 - val_loss: 2.9605\n",
            "Epoch 438/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5154 - val_loss: 2.9978\n",
            "Epoch 439/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5153 - val_loss: 2.9623\n",
            "Epoch 440/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5174 - val_loss: 2.9902\n",
            "Epoch 441/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5190 - val_loss: 2.9783\n",
            "Epoch 442/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.5184 - val_loss: 2.9901\n",
            "Epoch 443/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5153 - val_loss: 3.0002\n",
            "Epoch 444/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5170 - val_loss: 2.9547\n",
            "Epoch 445/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5147 - val_loss: 2.9707\n",
            "Epoch 446/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5169 - val_loss: 2.9859\n",
            "Epoch 447/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5184 - val_loss: 2.9582\n",
            "Epoch 448/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5155 - val_loss: 3.0100\n",
            "Epoch 449/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5137 - val_loss: 2.9474\n",
            "Epoch 450/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.5140 - val_loss: 3.0020\n",
            "Epoch 451/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5123 - val_loss: 2.9562\n",
            "Epoch 452/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5140 - val_loss: 3.0213\n",
            "Epoch 453/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5137 - val_loss: 2.9339\n",
            "Epoch 454/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5147 - val_loss: 2.9965\n",
            "Epoch 455/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5131 - val_loss: 2.9673\n",
            "Epoch 456/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5147 - val_loss: 2.9969\n",
            "Epoch 457/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5129 - val_loss: 2.9840\n",
            "Epoch 458/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5121 - val_loss: 2.9851\n",
            "Epoch 459/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5138 - val_loss: 3.0136\n",
            "Epoch 460/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5131 - val_loss: 2.9718\n",
            "Epoch 461/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5142 - val_loss: 3.0057\n",
            "Epoch 462/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5111 - val_loss: 2.9943\n",
            "Epoch 463/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5113 - val_loss: 2.9875\n",
            "Epoch 464/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5110 - val_loss: 2.9882\n",
            "Epoch 465/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5111 - val_loss: 3.0093\n",
            "Epoch 466/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5106 - val_loss: 2.9946\n",
            "Epoch 467/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5116 - val_loss: 2.9711\n",
            "Epoch 468/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5133 - val_loss: 3.0214\n",
            "Epoch 469/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5112 - val_loss: 3.0087\n",
            "Epoch 470/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5103 - val_loss: 3.0109\n",
            "Epoch 471/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5129 - val_loss: 3.0111\n",
            "Epoch 472/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5143 - val_loss: 3.0251\n",
            "Epoch 473/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5121 - val_loss: 2.9916\n",
            "Epoch 474/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5147 - val_loss: 3.0377\n",
            "Epoch 475/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5133 - val_loss: 2.9972\n",
            "Epoch 476/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5122 - val_loss: 3.0214\n",
            "Epoch 477/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5130 - val_loss: 3.0079\n",
            "Epoch 478/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5114 - val_loss: 3.0355\n",
            "Epoch 479/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5112 - val_loss: 2.9804\n",
            "Epoch 480/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5110 - val_loss: 3.0359\n",
            "Epoch 481/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5102 - val_loss: 3.0174\n",
            "Epoch 482/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5112 - val_loss: 3.0204\n",
            "Epoch 483/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5114 - val_loss: 3.0219\n",
            "Epoch 484/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5103 - val_loss: 3.0194\n",
            "Epoch 485/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5138 - val_loss: 2.9921\n",
            "Epoch 486/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5117 - val_loss: 3.0707\n",
            "Epoch 487/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5137 - val_loss: 2.9371\n",
            "Epoch 488/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5139 - val_loss: 3.0357\n",
            "Epoch 489/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5136 - val_loss: 2.9830\n",
            "Epoch 490/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5125 - val_loss: 3.0671\n",
            "Epoch 491/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5150 - val_loss: 2.9886\n",
            "Epoch 492/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5126 - val_loss: 3.0199\n",
            "Epoch 493/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5150 - val_loss: 3.0047\n",
            "Epoch 494/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5163 - val_loss: 3.0164\n",
            "Epoch 495/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5154 - val_loss: 3.0068\n",
            "Epoch 496/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5124 - val_loss: 2.9966\n",
            "Epoch 497/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5117 - val_loss: 3.0612\n",
            "Epoch 498/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5132 - val_loss: 3.0003\n",
            "Epoch 499/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5119 - val_loss: 3.0433\n",
            "Epoch 500/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5126 - val_loss: 3.0373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faec1ab4a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b03c83ae-dce3-4206-8e98-26476d26f424",
        "id": "JTCPTgyb5q0s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# demonstrate prediction\n",
        "x_input = dataXScaler[2:30]\n",
        "x_input = x_input.reshape((1, len(x_input), 4))\n",
        "print(scaler.inverse_transform(dataXScaler)[2:30])\n",
        "yhat = model_rnn.predict(x_input, verbose=0)\n",
        "print(scaler.inverse_transform(yhat))\n",
        "print('expected: ', scaler.inverse_transform(dataYScaler)[30])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [72.33333333 70.22222222 67.         58.23529412]\n",
            " [73.66666667 70.22222222 60.         55.58823529]\n",
            " [73.66666667 70.22222222 60.         55.58823529]\n",
            " [73.66666667 70.22222222 62.         55.58823529]\n",
            " [73.66666667 70.22222222 62.         55.58823529]\n",
            " [76.33333333 69.33333333 64.         51.17647059]\n",
            " [76.33333333 69.33333333 64.         51.17647059]\n",
            " [76.33333333 69.33333333 64.         51.17647059]\n",
            " [76.33333333 69.33333333 64.         51.17647059]\n",
            " [76.33333333 70.22222222 64.         55.58823529]\n",
            " [76.33333333 70.22222222 64.         55.58823529]\n",
            " [76.33333333 69.33333333 64.         55.58823529]\n",
            " [76.33333333 69.33333333 64.         55.58823529]\n",
            " [73.66666667 67.55555556 65.         49.41176471]\n",
            " [73.66666667 67.55555556 65.         49.41176471]\n",
            " [73.66666667 69.33333333 65.         49.41176471]\n",
            " [73.66666667 69.33333333 65.         49.41176471]\n",
            " [72.33333333 70.22222222 67.         47.64705882]\n",
            " [72.33333333 70.22222222 67.         47.64705882]\n",
            " [72.33333333 70.22222222 67.         47.64705882]\n",
            " [72.33333333 70.22222222 67.         47.64705882]\n",
            " [69.66666667 69.33333333 67.         53.82352941]\n",
            " [69.66666667 69.33333333 67.         53.82352941]\n",
            " [69.66666667 69.33333333 67.         53.82352941]\n",
            " [69.66666667 69.33333333 67.         53.82352941]\n",
            " [69.66666667 69.33333333 67.         53.82352941]\n",
            " [69.66666667 69.33333333 67.         53.82352941]]\n",
            "[[68.33061 66.38948 59.57888 48.13285]]\n",
            "expected:  [69.66666667 69.33333333 67.         53.82352941]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbHx1t9X6nb4",
        "colab_type": "code",
        "outputId": "1c1268f7-23af-45e7-eaeb-bb9b0bdf4cf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# choose a number of time steps\n",
        "n_steps = None\n",
        "# convert into input/output\n",
        "i=1\n",
        "n=1\n",
        "x_input = np.array([dataXScaler[0]])\n",
        "\n",
        "x_input = x_input.reshape((1, len(x_input), n_features))\n",
        "while i<len(dataXScaler):\n",
        "  # demonstrate prediction\n",
        "  print('Input: ')\n",
        "  for input in x_input:\n",
        "    print(scaler.inverse_transform(input))\n",
        "  print('---------------')\n",
        "  yhat = model.predict(x_input, verbose=1)\n",
        "  print('Predicted Output: ', scaler.inverse_transform(yhat))\n",
        "  print('expected: ', scaler.inverse_transform(dataYScaler)[i])\n",
        "  print('\\n\\n')\n",
        "\n",
        "  yhat = yhat.reshape((1, len(yhat), n_features))\n",
        "  x_input = np.concatenate([x_input, yhat], axis=1)\n",
        "  i += 1\n",
        "  if i>20:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]]\n",
            "---------------\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Predicted Output:  [[75.26765  65.903206 55.87507  45.003094]]\n",
            "expected:  [72.33333333 70.22222222 67.         58.23529412]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[78.15341 64.03464 55.79461 45.00003]]\n",
            "expected:  [72.33333333 70.22222222 67.         58.23529412]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicted Output:  [[75.54515  64.27181  58.038734 45.010487]]\n",
            "expected:  [73.66666667 70.22222222 60.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicted Output:  [[71.08536  65.97478  57.158997 48.49185 ]]\n",
            "expected:  [73.66666667 70.22222222 60.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicted Output:  [[70.36781  65.637566 58.040543 48.91912 ]]\n",
            "expected:  [73.66666667 70.22222222 62.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]\n",
            " [70.36781025 65.6375643  58.04054534 48.91912222]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicted Output:  [[70.02989  65.698685 58.57705  48.556293]]\n",
            "expected:  [73.66666667 70.22222222 62.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]\n",
            " [70.36781025 65.6375643  58.04054534 48.91912222]\n",
            " [70.02989054 65.69868386 58.57704687 48.55629534]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[69.76989  65.751465 58.597515 48.756752]]\n",
            "expected:  [76.33333333 69.33333333 64.         51.17647059]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]\n",
            " [70.36781025 65.6375643  58.04054534 48.91912222]\n",
            " [70.02989054 65.69868386 58.57704687 48.55629534]\n",
            " [69.76988775 65.75146353 58.59751499 48.75675291]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[69.68616  65.78672  58.39553  49.047787]]\n",
            "expected:  [76.33333333 69.33333333 64.         51.17647059]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]\n",
            " [70.36781025 65.6375643  58.04054534 48.91912222]\n",
            " [70.02989054 65.69868386 58.57704687 48.55629534]\n",
            " [69.76988775 65.75146353 58.59751499 48.75675291]\n",
            " [69.6861549  65.78672051 58.39553463 49.04778644]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[69.64665  65.80456  58.225502 49.27626 ]]\n",
            "expected:  [76.33333333 69.33333333 64.         51.17647059]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]\n",
            " [70.36781025 65.6375643  58.04054534 48.91912222]\n",
            " [70.02989054 65.69868386 58.57704687 48.55629534]\n",
            " [69.76988775 65.75146353 58.59751499 48.75675291]\n",
            " [69.6861549  65.78672051 58.39553463 49.04778644]\n",
            " [69.64665133 65.80455911 58.22550225 49.27625924]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicted Output:  [[69.62285  65.813995 58.110176 49.43248 ]]\n",
            "expected:  [76.33333333 69.33333333 64.         51.17647059]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]\n",
            " [70.36781025 65.6375643  58.04054534 48.91912222]\n",
            " [70.02989054 65.69868386 58.57704687 48.55629534]\n",
            " [69.76988775 65.75146353 58.59751499 48.75675291]\n",
            " [69.6861549  65.78672051 58.39553463 49.04778644]\n",
            " [69.64665133 65.80455911 58.22550225 49.27625924]\n",
            " [69.62285012 65.81399488 58.11017478 49.43247959]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[69.60918  65.818954 58.04165  49.52594 ]]\n",
            "expected:  [76.33333333 70.22222222 64.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]\n",
            " [70.36781025 65.6375643  58.04054534 48.91912222]\n",
            " [70.02989054 65.69868386 58.57704687 48.55629534]\n",
            " [69.76988775 65.75146353 58.59751499 48.75675291]\n",
            " [69.6861549  65.78672051 58.39553463 49.04778644]\n",
            " [69.64665133 65.80455911 58.22550225 49.27625924]\n",
            " [69.62285012 65.81399488 58.11017478 49.43247959]\n",
            " [69.60917479 65.81895161 58.04164433 49.52594161]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[69.60971 65.82308 57.99785 49.57227]]\n",
            "expected:  [76.33333333 70.22222222 64.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]\n",
            " [70.36781025 65.6375643  58.04054534 48.91912222]\n",
            " [70.02989054 65.69868386 58.57704687 48.55629534]\n",
            " [69.76988775 65.75146353 58.59751499 48.75675291]\n",
            " [69.6861549  65.78672051 58.39553463 49.04778644]\n",
            " [69.64665133 65.80455911 58.22550225 49.27625924]\n",
            " [69.62285012 65.81399488 58.11017478 49.43247959]\n",
            " [69.60917479 65.81895161 58.04164433 49.52594161]\n",
            " [69.60970569 65.82308435 57.99785298 49.57226828]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[69.649315 65.83282  57.94197  49.574356]]\n",
            "expected:  [76.33333333 69.33333333 64.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]\n",
            " [70.36781025 65.6375643  58.04054534 48.91912222]\n",
            " [70.02989054 65.69868386 58.57704687 48.55629534]\n",
            " [69.76988775 65.75146353 58.59751499 48.75675291]\n",
            " [69.6861549  65.78672051 58.39553463 49.04778644]\n",
            " [69.64665133 65.80455911 58.22550225 49.27625924]\n",
            " [69.62285012 65.81399488 58.11017478 49.43247959]\n",
            " [69.60917479 65.81895161 58.04164433 49.52594161]\n",
            " [69.60970569 65.82308435 57.99785298 49.57226828]\n",
            " [69.64931691 65.83281898 57.94196969 49.57435682]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[69.68615  65.841064 57.898865 49.566734]]\n",
            "expected:  [76.33333333 69.33333333 64.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]\n",
            " [70.36781025 65.6375643  58.04054534 48.91912222]\n",
            " [70.02989054 65.69868386 58.57704687 48.55629534]\n",
            " [69.76988775 65.75146353 58.59751499 48.75675291]\n",
            " [69.6861549  65.78672051 58.39553463 49.04778644]\n",
            " [69.64665133 65.80455911 58.22550225 49.27625924]\n",
            " [69.62285012 65.81399488 58.11017478 49.43247959]\n",
            " [69.60917479 65.81895161 58.04164433 49.52594161]\n",
            " [69.60970569 65.82308435 57.99785298 49.57226828]\n",
            " [69.64931691 65.83281898 57.94196969 49.57435682]\n",
            " [69.68615043 65.84106374 57.89886588 49.56673577]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[69.715836 65.84743  57.868134 49.556103]]\n",
            "expected:  [73.66666667 67.55555556 65.         49.41176471]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]\n",
            " [70.36781025 65.6375643  58.04054534 48.91912222]\n",
            " [70.02989054 65.69868386 58.57704687 48.55629534]\n",
            " [69.76988775 65.75146353 58.59751499 48.75675291]\n",
            " [69.6861549  65.78672051 58.39553463 49.04778644]\n",
            " [69.64665133 65.80455911 58.22550225 49.27625924]\n",
            " [69.62285012 65.81399488 58.11017478 49.43247959]\n",
            " [69.60917479 65.81895161 58.04164433 49.52594161]\n",
            " [69.60970569 65.82308435 57.99785298 49.57226828]\n",
            " [69.64931691 65.83281898 57.94196969 49.57435682]\n",
            " [69.68615043 65.84106374 57.89886588 49.56673577]\n",
            " [69.71583962 65.84742785 57.8681342  49.55610529]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[69.739105 65.8523   57.84669  49.54469 ]]\n",
            "expected:  [73.66666667 67.55555556 65.         49.41176471]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]\n",
            " [70.36781025 65.6375643  58.04054534 48.91912222]\n",
            " [70.02989054 65.69868386 58.57704687 48.55629534]\n",
            " [69.76988775 65.75146353 58.59751499 48.75675291]\n",
            " [69.6861549  65.78672051 58.39553463 49.04778644]\n",
            " [69.64665133 65.80455911 58.22550225 49.27625924]\n",
            " [69.62285012 65.81399488 58.11017478 49.43247959]\n",
            " [69.60917479 65.81895161 58.04164433 49.52594161]\n",
            " [69.60970569 65.82308435 57.99785298 49.57226828]\n",
            " [69.64931691 65.83281898 57.94196969 49.57435682]\n",
            " [69.68615043 65.84106374 57.89886588 49.56673577]\n",
            " [69.71583962 65.84742785 57.8681342  49.55610529]\n",
            " [69.73910582 65.85230052 57.84669137 49.5446898 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[69.75688  65.85602  57.831947 49.533924]]\n",
            "expected:  [73.66666667 69.33333333 65.         49.41176471]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]\n",
            " [70.36781025 65.6375643  58.04054534 48.91912222]\n",
            " [70.02989054 65.69868386 58.57704687 48.55629534]\n",
            " [69.76988775 65.75146353 58.59751499 48.75675291]\n",
            " [69.6861549  65.78672051 58.39553463 49.04778644]\n",
            " [69.64665133 65.80455911 58.22550225 49.27625924]\n",
            " [69.62285012 65.81399488 58.11017478 49.43247959]\n",
            " [69.60917479 65.81895161 58.04164433 49.52594161]\n",
            " [69.60970569 65.82308435 57.99785298 49.57226828]\n",
            " [69.64931691 65.83281898 57.94196969 49.57435682]\n",
            " [69.68615043 65.84106374 57.89886588 49.56673577]\n",
            " [69.71583962 65.84742785 57.8681342  49.55610529]\n",
            " [69.73910582 65.85230052 57.84669137 49.5446898 ]\n",
            " [69.75688583 65.85602033 57.83194387 49.53392386]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[69.77006  65.85883  57.82211  49.524483]]\n",
            "expected:  [73.66666667 69.33333333 65.         49.41176471]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]\n",
            " [70.36781025 65.6375643  58.04054534 48.91912222]\n",
            " [70.02989054 65.69868386 58.57704687 48.55629534]\n",
            " [69.76988775 65.75146353 58.59751499 48.75675291]\n",
            " [69.6861549  65.78672051 58.39553463 49.04778644]\n",
            " [69.64665133 65.80455911 58.22550225 49.27625924]\n",
            " [69.62285012 65.81399488 58.11017478 49.43247959]\n",
            " [69.60917479 65.81895161 58.04164433 49.52594161]\n",
            " [69.60970569 65.82308435 57.99785298 49.57226828]\n",
            " [69.64931691 65.83281898 57.94196969 49.57435682]\n",
            " [69.68615043 65.84106374 57.89886588 49.56673577]\n",
            " [69.71583962 65.84742785 57.8681342  49.55610529]\n",
            " [69.73910582 65.85230052 57.84669137 49.5446898 ]\n",
            " [69.75688583 65.85602033 57.83194387 49.53392386]\n",
            " [69.77005351 65.85883224 57.82211161 49.52448249]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[69.77939  65.860916 57.81594  49.51661 ]]\n",
            "expected:  [72.33333333 70.22222222 67.         47.64705882]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [75.26764822 65.90320635 55.87506586 45.00309411]\n",
            " [78.15341401 64.03463399 55.79460961 45.0000323 ]\n",
            " [75.54515028 64.27181584 58.03873682 45.0104869 ]\n",
            " [71.08535445 65.97477663 57.15900016 48.49185035]\n",
            " [70.36781025 65.6375643  58.04054534 48.91912222]\n",
            " [70.02989054 65.69868386 58.57704687 48.55629534]\n",
            " [69.76988775 65.75146353 58.59751499 48.75675291]\n",
            " [69.6861549  65.78672051 58.39553463 49.04778644]\n",
            " [69.64665133 65.80455911 58.22550225 49.27625924]\n",
            " [69.62285012 65.81399488 58.11017478 49.43247959]\n",
            " [69.60917479 65.81895161 58.04164433 49.52594161]\n",
            " [69.60970569 65.82308435 57.99785298 49.57226828]\n",
            " [69.64931691 65.83281898 57.94196969 49.57435682]\n",
            " [69.68615043 65.84106374 57.89886588 49.56673577]\n",
            " [69.71583962 65.84742785 57.8681342  49.55610529]\n",
            " [69.73910582 65.85230052 57.84669137 49.5446898 ]\n",
            " [69.75688583 65.85602033 57.83194387 49.53392386]\n",
            " [69.77005351 65.85883224 57.82211161 49.52448249]\n",
            " [69.77939063 65.86091912 57.8159436  49.51660842]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[69.78559  65.86243  57.812523 49.51031 ]]\n",
            "expected:  [72.33333333 70.22222222 67.         47.64705882]\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx1PEBFFRy1h",
        "colab_type": "code",
        "outputId": "0a639751-a3bf-458b-8611-b36a4ab20b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# using GRU\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GRU, Dense\n",
        "\n",
        "# choose a number of time steps\n",
        "n_steps = None\n",
        "n_features = 4\n",
        "\n",
        "\n",
        "\n",
        "model_gru = Sequential()\n",
        "model_gru.add(TimeDistributed(Dense(128), input_shape=(None, n_features)))\n",
        "model_gru.add(GRU(64, activation='tanh', input_shape=(None, n_features), return_sequences=True))\n",
        "model_gru.add(BatchNormalization())\n",
        "\n",
        "model_gru.add(GRU(32 , activation = 'tanh'))\n",
        "model_gru.add(BatchNormalization())\n",
        "model_gru.add(Dropout(0.2))\n",
        "model_gru.add(Dense(n_features, activation='softmax'))\n",
        "model_gru.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "# fit model\n",
        "model_gru.fit_generator(generator, epochs=500, validation_data=validation_generator)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 4s 746ms/step - loss: 3.2365 - val_loss: 3.7881\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 3.0617 - val_loss: 3.1095\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 2.9522 - val_loss: 2.9572\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.8794 - val_loss: 2.9560\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.8688 - val_loss: 2.8725\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.8729 - val_loss: 2.8114\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.8483 - val_loss: 2.7877\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.8308 - val_loss: 2.8471\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.8720 - val_loss: 2.8296\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.8530 - val_loss: 2.8576\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.8379 - val_loss: 2.8443\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.8123 - val_loss: 2.7617\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 2.8176 - val_loss: 2.7653\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.7860 - val_loss: 2.7749\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.8357 - val_loss: 2.7883\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.8035 - val_loss: 2.7988\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.7882 - val_loss: 2.7120\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.7871 - val_loss: 2.7270\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.8047 - val_loss: 2.8292\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.7787 - val_loss: 2.8148\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.7835 - val_loss: 2.7604\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.7762 - val_loss: 2.7766\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.7538 - val_loss: 2.7910\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.7311 - val_loss: 2.8065\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.7499 - val_loss: 2.8187\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.7408 - val_loss: 2.7746\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.7375 - val_loss: 2.7706\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.7326 - val_loss: 2.7704\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.7015 - val_loss: 2.7875\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.7206 - val_loss: 2.7243\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.7121 - val_loss: 2.7201\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.7035 - val_loss: 2.7352\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.7111 - val_loss: 2.7222\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.7363 - val_loss: 2.7277\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.7047 - val_loss: 2.7488\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.7052 - val_loss: 2.7399\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.7109 - val_loss: 2.7622\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.7005 - val_loss: 2.7305\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6884 - val_loss: 2.7292\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6916 - val_loss: 2.7368\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.7018 - val_loss: 2.7450\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 2.6791 - val_loss: 2.7329\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6856 - val_loss: 2.7175\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6634 - val_loss: 2.7327\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6910 - val_loss: 2.7677\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6651 - val_loss: 2.7556\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6837 - val_loss: 2.7332\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6769 - val_loss: 2.7541\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6640 - val_loss: 2.7404\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.6831 - val_loss: 2.7106\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.6736 - val_loss: 2.7151\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6530 - val_loss: 2.7087\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6785 - val_loss: 2.7258\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6647 - val_loss: 2.7457\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6548 - val_loss: 2.7409\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6671 - val_loss: 2.7343\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6458 - val_loss: 2.7186\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6510 - val_loss: 2.7122\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6657 - val_loss: 2.7055\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6415 - val_loss: 2.6948\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6499 - val_loss: 2.7114\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6503 - val_loss: 2.7230\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6442 - val_loss: 2.7349\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6313 - val_loss: 2.7252\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6469 - val_loss: 2.7217\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6334 - val_loss: 2.7254\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6456 - val_loss: 2.7114\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6592 - val_loss: 2.7069\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6559 - val_loss: 2.7228\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6325 - val_loss: 2.7747\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6381 - val_loss: 2.7708\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 2.6242 - val_loss: 2.7406\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6396 - val_loss: 2.7269\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6328 - val_loss: 2.7174\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6201 - val_loss: 2.7263\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6192 - val_loss: 2.7502\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.6237 - val_loss: 2.7477\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6247 - val_loss: 2.7480\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6281 - val_loss: 2.7274\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6233 - val_loss: 2.7059\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6200 - val_loss: 2.7118\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6186 - val_loss: 2.6993\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6206 - val_loss: 2.6982\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6136 - val_loss: 2.7065\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6184 - val_loss: 2.7049\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6261 - val_loss: 2.7168\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6157 - val_loss: 2.7432\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6130 - val_loss: 2.7290\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6291 - val_loss: 2.7309\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.6122 - val_loss: 2.7579\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.6241 - val_loss: 2.7347\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6221 - val_loss: 2.7626\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6107 - val_loss: 2.7835\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6008 - val_loss: 2.7746\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.6108 - val_loss: 2.7610\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 2.6040 - val_loss: 2.7299\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.6122 - val_loss: 2.7161\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.6091 - val_loss: 2.7332\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5844 - val_loss: 2.7586\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5974 - val_loss: 2.7254\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5956 - val_loss: 2.7284\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5951 - val_loss: 2.7276\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5997 - val_loss: 2.7041\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6002 - val_loss: 2.7088\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5998 - val_loss: 2.7305\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6064 - val_loss: 2.7370\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.6000 - val_loss: 2.7287\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5990 - val_loss: 2.7332\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5959 - val_loss: 2.7448\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5919 - val_loss: 2.7289\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5999 - val_loss: 2.7174\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5982 - val_loss: 2.7264\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5841 - val_loss: 2.7497\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5945 - val_loss: 2.7421\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5879 - val_loss: 2.7463\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5961 - val_loss: 2.7733\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5961 - val_loss: 2.7763\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5793 - val_loss: 2.7463\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5923 - val_loss: 2.7355\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5877 - val_loss: 2.7447\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5882 - val_loss: 2.7468\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5884 - val_loss: 2.7465\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5935 - val_loss: 2.7553\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5899 - val_loss: 2.7641\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5849 - val_loss: 2.7454\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5923 - val_loss: 2.7361\n",
            "Epoch 127/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5866 - val_loss: 2.7469\n",
            "Epoch 128/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5890 - val_loss: 2.7359\n",
            "Epoch 129/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5951 - val_loss: 2.7324\n",
            "Epoch 130/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5899 - val_loss: 2.7657\n",
            "Epoch 131/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5871 - val_loss: 2.7589\n",
            "Epoch 132/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.5815 - val_loss: 2.7288\n",
            "Epoch 133/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5849 - val_loss: 2.7535\n",
            "Epoch 134/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 2.5841 - val_loss: 2.7804\n",
            "Epoch 135/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.5767 - val_loss: 2.7508\n",
            "Epoch 136/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5867 - val_loss: 2.7294\n",
            "Epoch 137/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5820 - val_loss: 2.7414\n",
            "Epoch 138/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5849 - val_loss: 2.7740\n",
            "Epoch 139/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5855 - val_loss: 2.7312\n",
            "Epoch 140/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5814 - val_loss: 2.7081\n",
            "Epoch 141/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5793 - val_loss: 2.7122\n",
            "Epoch 142/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5758 - val_loss: 2.7253\n",
            "Epoch 143/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5803 - val_loss: 2.7138\n",
            "Epoch 144/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5712 - val_loss: 2.7052\n",
            "Epoch 145/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.5826 - val_loss: 2.7143\n",
            "Epoch 146/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5812 - val_loss: 2.7336\n",
            "Epoch 147/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5808 - val_loss: 2.7531\n",
            "Epoch 148/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5855 - val_loss: 2.7507\n",
            "Epoch 149/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5786 - val_loss: 2.7392\n",
            "Epoch 150/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5734 - val_loss: 2.7432\n",
            "Epoch 151/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5752 - val_loss: 2.7446\n",
            "Epoch 152/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5761 - val_loss: 2.7343\n",
            "Epoch 153/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5815 - val_loss: 2.7313\n",
            "Epoch 154/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5789 - val_loss: 2.7469\n",
            "Epoch 155/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5737 - val_loss: 2.7526\n",
            "Epoch 156/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5748 - val_loss: 2.7657\n",
            "Epoch 157/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5762 - val_loss: 2.7716\n",
            "Epoch 158/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5736 - val_loss: 2.7530\n",
            "Epoch 159/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5755 - val_loss: 2.7492\n",
            "Epoch 160/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5699 - val_loss: 2.7678\n",
            "Epoch 161/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5744 - val_loss: 2.7719\n",
            "Epoch 162/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5778 - val_loss: 2.7684\n",
            "Epoch 163/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5746 - val_loss: 2.7583\n",
            "Epoch 164/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5683 - val_loss: 2.7674\n",
            "Epoch 165/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5771 - val_loss: 2.7846\n",
            "Epoch 166/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5696 - val_loss: 2.7773\n",
            "Epoch 167/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5704 - val_loss: 2.7635\n",
            "Epoch 168/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5680 - val_loss: 2.7553\n",
            "Epoch 169/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5711 - val_loss: 2.7548\n",
            "Epoch 170/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5766 - val_loss: 2.7603\n",
            "Epoch 171/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5683 - val_loss: 2.7617\n",
            "Epoch 172/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5753 - val_loss: 2.7565\n",
            "Epoch 173/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5650 - val_loss: 2.7490\n",
            "Epoch 174/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.5720 - val_loss: 2.7587\n",
            "Epoch 175/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5717 - val_loss: 2.7775\n",
            "Epoch 176/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5736 - val_loss: 2.7735\n",
            "Epoch 177/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5681 - val_loss: 2.7613\n",
            "Epoch 178/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5732 - val_loss: 2.7777\n",
            "Epoch 179/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5666 - val_loss: 2.7591\n",
            "Epoch 180/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.5625 - val_loss: 2.7565\n",
            "Epoch 181/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5707 - val_loss: 2.7510\n",
            "Epoch 182/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5698 - val_loss: 2.7552\n",
            "Epoch 183/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.5804 - val_loss: 2.7616\n",
            "Epoch 184/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5681 - val_loss: 2.7580\n",
            "Epoch 185/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5635 - val_loss: 2.7692\n",
            "Epoch 186/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5735 - val_loss: 2.7652\n",
            "Epoch 187/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5705 - val_loss: 2.7563\n",
            "Epoch 188/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5709 - val_loss: 2.7804\n",
            "Epoch 189/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5720 - val_loss: 2.8026\n",
            "Epoch 190/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5780 - val_loss: 2.7785\n",
            "Epoch 191/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5702 - val_loss: 2.7487\n",
            "Epoch 192/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5726 - val_loss: 2.7440\n",
            "Epoch 193/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5699 - val_loss: 2.7568\n",
            "Epoch 194/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5664 - val_loss: 2.7580\n",
            "Epoch 195/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5671 - val_loss: 2.7511\n",
            "Epoch 196/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5678 - val_loss: 2.7529\n",
            "Epoch 197/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5610 - val_loss: 2.7596\n",
            "Epoch 198/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5699 - val_loss: 2.7472\n",
            "Epoch 199/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5674 - val_loss: 2.7456\n",
            "Epoch 200/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5661 - val_loss: 2.7543\n",
            "Epoch 201/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5622 - val_loss: 2.7820\n",
            "Epoch 202/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5640 - val_loss: 2.7858\n",
            "Epoch 203/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5721 - val_loss: 2.7989\n",
            "Epoch 204/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5654 - val_loss: 2.7715\n",
            "Epoch 205/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5597 - val_loss: 2.7631\n",
            "Epoch 206/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5611 - val_loss: 2.7798\n",
            "Epoch 207/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5643 - val_loss: 2.7704\n",
            "Epoch 208/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5638 - val_loss: 2.7624\n",
            "Epoch 209/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5642 - val_loss: 2.7661\n",
            "Epoch 210/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5605 - val_loss: 2.7546\n",
            "Epoch 211/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5648 - val_loss: 2.7561\n",
            "Epoch 212/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5629 - val_loss: 2.7658\n",
            "Epoch 213/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5608 - val_loss: 2.7700\n",
            "Epoch 214/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5579 - val_loss: 2.7614\n",
            "Epoch 215/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 2.5628 - val_loss: 2.7600\n",
            "Epoch 216/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5653 - val_loss: 2.7908\n",
            "Epoch 217/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5627 - val_loss: 2.7585\n",
            "Epoch 218/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5626 - val_loss: 2.7470\n",
            "Epoch 219/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5611 - val_loss: 2.7792\n",
            "Epoch 220/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5657 - val_loss: 2.7763\n",
            "Epoch 221/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.5640 - val_loss: 2.7645\n",
            "Epoch 222/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5643 - val_loss: 2.7698\n",
            "Epoch 223/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5645 - val_loss: 2.7615\n",
            "Epoch 224/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5609 - val_loss: 2.7613\n",
            "Epoch 225/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5622 - val_loss: 2.7684\n",
            "Epoch 226/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5638 - val_loss: 2.7785\n",
            "Epoch 227/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5615 - val_loss: 2.7737\n",
            "Epoch 228/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5672 - val_loss: 2.7566\n",
            "Epoch 229/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 2.5610 - val_loss: 2.7673\n",
            "Epoch 230/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5627 - val_loss: 2.7515\n",
            "Epoch 231/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5651 - val_loss: 2.7559\n",
            "Epoch 232/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5628 - val_loss: 2.7690\n",
            "Epoch 233/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5641 - val_loss: 2.7616\n",
            "Epoch 234/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5634 - val_loss: 2.7672\n",
            "Epoch 235/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5602 - val_loss: 2.7898\n",
            "Epoch 236/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5613 - val_loss: 2.7906\n",
            "Epoch 237/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5592 - val_loss: 2.7734\n",
            "Epoch 238/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5623 - val_loss: 2.7700\n",
            "Epoch 239/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 2.5614 - val_loss: 2.7773\n",
            "Epoch 240/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5586 - val_loss: 2.7791\n",
            "Epoch 241/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5626 - val_loss: 2.7696\n",
            "Epoch 242/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5634 - val_loss: 2.7821\n",
            "Epoch 243/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5604 - val_loss: 2.7651\n",
            "Epoch 244/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5588 - val_loss: 2.7726\n",
            "Epoch 245/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5606 - val_loss: 2.7790\n",
            "Epoch 246/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5626 - val_loss: 2.7730\n",
            "Epoch 247/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5646 - val_loss: 2.7817\n",
            "Epoch 248/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5628 - val_loss: 2.7997\n",
            "Epoch 249/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5591 - val_loss: 2.7738\n",
            "Epoch 250/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5575 - val_loss: 2.7652\n",
            "Epoch 251/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5568 - val_loss: 2.7919\n",
            "Epoch 252/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5636 - val_loss: 2.7762\n",
            "Epoch 253/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5618 - val_loss: 2.7684\n",
            "Epoch 254/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5551 - val_loss: 2.7783\n",
            "Epoch 255/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5617 - val_loss: 2.7645\n",
            "Epoch 256/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5569 - val_loss: 2.7679\n",
            "Epoch 257/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5619 - val_loss: 2.7786\n",
            "Epoch 258/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5598 - val_loss: 2.7845\n",
            "Epoch 259/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5645 - val_loss: 2.7999\n",
            "Epoch 260/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5582 - val_loss: 2.7709\n",
            "Epoch 261/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5559 - val_loss: 2.7590\n",
            "Epoch 262/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5604 - val_loss: 2.7925\n",
            "Epoch 263/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5618 - val_loss: 2.7856\n",
            "Epoch 264/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5554 - val_loss: 2.7810\n",
            "Epoch 265/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5661 - val_loss: 2.8110\n",
            "Epoch 266/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5622 - val_loss: 2.7956\n",
            "Epoch 267/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5587 - val_loss: 2.7730\n",
            "Epoch 268/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5632 - val_loss: 2.7761\n",
            "Epoch 269/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5581 - val_loss: 2.7760\n",
            "Epoch 270/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5577 - val_loss: 2.7820\n",
            "Epoch 271/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5603 - val_loss: 2.7948\n",
            "Epoch 272/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5547 - val_loss: 2.7739\n",
            "Epoch 273/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5549 - val_loss: 2.7738\n",
            "Epoch 274/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5572 - val_loss: 2.7831\n",
            "Epoch 275/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5655 - val_loss: 2.7823\n",
            "Epoch 276/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5557 - val_loss: 2.7921\n",
            "Epoch 277/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5591 - val_loss: 2.7690\n",
            "Epoch 278/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5608 - val_loss: 2.7753\n",
            "Epoch 279/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5570 - val_loss: 2.7879\n",
            "Epoch 280/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5536 - val_loss: 2.7652\n",
            "Epoch 281/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5577 - val_loss: 2.7802\n",
            "Epoch 282/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5603 - val_loss: 2.7675\n",
            "Epoch 283/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5568 - val_loss: 2.7674\n",
            "Epoch 284/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5578 - val_loss: 2.7769\n",
            "Epoch 285/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5561 - val_loss: 2.7735\n",
            "Epoch 286/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5557 - val_loss: 2.7670\n",
            "Epoch 287/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5553 - val_loss: 2.7630\n",
            "Epoch 288/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5517 - val_loss: 2.7816\n",
            "Epoch 289/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5606 - val_loss: 2.7792\n",
            "Epoch 290/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 2.5531 - val_loss: 2.7787\n",
            "Epoch 291/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.5604 - val_loss: 2.7782\n",
            "Epoch 292/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5525 - val_loss: 2.7618\n",
            "Epoch 293/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5601 - val_loss: 2.7629\n",
            "Epoch 294/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5538 - val_loss: 2.7783\n",
            "Epoch 295/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 2.5555 - val_loss: 2.7751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4PnMfQG65Ch",
        "colab_type": "code",
        "outputId": "1232f5f2-6c8c-454a-8c13-b13214f0d5fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# demonstrate prediction\n",
        "x_input = dataXScaler[2:30]\n",
        "x_input = x_input.reshape((1, len(x_input), 4))\n",
        "print(scaler.inverse_transform(dataXScaler)[2:30])\n",
        "yhat = model_gru.predict(x_input, verbose=0)\n",
        "print(scaler.inverse_transform(yhat))\n",
        "print('expected: ', scaler.inverse_transform(dataYScaler)[30])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [72.33333333 70.22222222 67.         58.23529412]\n",
            " [73.66666667 70.22222222 60.         55.58823529]\n",
            " [73.66666667 70.22222222 60.         55.58823529]\n",
            " [73.66666667 70.22222222 62.         55.58823529]\n",
            " [73.66666667 70.22222222 62.         55.58823529]\n",
            " [76.33333333 69.33333333 64.         51.17647059]\n",
            " [76.33333333 69.33333333 64.         51.17647059]\n",
            " [76.33333333 69.33333333 64.         51.17647059]\n",
            " [76.33333333 69.33333333 64.         51.17647059]\n",
            " [76.33333333 70.22222222 64.         55.58823529]\n",
            " [76.33333333 70.22222222 64.         55.58823529]\n",
            " [76.33333333 69.33333333 64.         55.58823529]\n",
            " [76.33333333 69.33333333 64.         55.58823529]\n",
            " [73.66666667 67.55555556 65.         49.41176471]\n",
            " [73.66666667 67.55555556 65.         49.41176471]\n",
            " [73.66666667 69.33333333 65.         49.41176471]\n",
            " [73.66666667 69.33333333 65.         49.41176471]\n",
            " [72.33333333 70.22222222 67.         47.64705882]\n",
            " [72.33333333 70.22222222 67.         47.64705882]\n",
            " [72.33333333 70.22222222 67.         47.64705882]\n",
            " [72.33333333 70.22222222 67.         47.64705882]\n",
            " [69.66666667 69.33333333 67.         53.82352941]\n",
            " [69.66666667 69.33333333 67.         53.82352941]\n",
            " [69.66666667 69.33333333 67.         53.82352941]\n",
            " [69.66666667 69.33333333 67.         53.82352941]\n",
            " [69.66666667 69.33333333 67.         53.82352941]\n",
            " [69.66666667 69.33333333 67.         53.82352941]]\n",
            "[[71.82475 66.25096 57.36835 46.78806]]\n",
            "expected:  [69.66666667 69.33333333 67.         53.82352941]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk9W_GSXtwRG",
        "colab_type": "code",
        "outputId": "4ed09fc7-c586-4f7e-faa4-ec226ed7d7c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# choose a number of time steps\n",
        "n_steps = None\n",
        "# convert into input/output\n",
        "i=1\n",
        "n=1\n",
        "x_input = np.array([dataXScaler[0]])\n",
        "\n",
        "x_input = x_input.reshape((1, len(x_input), n_features))\n",
        "while i<len(dataXScaler):\n",
        "  # demonstrate prediction\n",
        "  print('Input: ')\n",
        "  for input in x_input:\n",
        "    print(scaler.inverse_transform(input))\n",
        "  print('---------------')\n",
        "  yhat = model_gru.predict(x_input, verbose=1)\n",
        "  print('Predicted Output: ', scaler.inverse_transform(yhat))\n",
        "  print('expected: ', scaler.inverse_transform(dataYScaler)[i])\n",
        "  print('\\n\\n')\n",
        "\n",
        "  yhat = yhat.reshape((1, len(yhat), n_features))\n",
        "  x_input = np.concatenate([x_input, yhat], axis=1)\n",
        "  i += 1\n",
        "  if i>20:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[69.76914  66.819084 57.545616 48.070786]]\n",
            "expected:  [72.33333333 70.22222222 67.         58.23529412]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[70.597855 66.4717   57.273823 48.025963]]\n",
            "expected:  [72.33333333 70.22222222 67.         58.23529412]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 6ms/step\n",
            "Predicted Output:  [[71.425766 65.70524  57.289185 48.40898 ]]\n",
            "expected:  [73.66666667 70.22222222 60.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[73.012375 65.17541  56.929264 47.86906 ]]\n",
            "expected:  [73.66666667 70.22222222 60.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[73.877266 64.87273  57.304558 46.88636 ]]\n",
            "expected:  [73.66666667 70.22222222 62.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]\n",
            " [73.87726688 64.87272441 57.30455595 46.88636228]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[73.391914 64.7105   58.381256 46.45134 ]]\n",
            "expected:  [73.66666667 70.22222222 62.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]\n",
            " [73.87726688 64.87272441 57.30455595 46.88636228]\n",
            " [73.39191794 64.71050191 58.3812567  46.45134062]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicted Output:  [[72.99582  64.655594 58.47685  46.929928]]\n",
            "expected:  [76.33333333 69.33333333 64.         51.17647059]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]\n",
            " [73.87726688 64.87272441 57.30455595 46.88636228]\n",
            " [73.39191794 64.71050191 58.3812567  46.45134062]\n",
            " [72.99582005 64.65559393 58.47684634 46.92992806]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicted Output:  [[72.364235 64.68978  58.204468 47.995785]]\n",
            "expected:  [76.33333333 69.33333333 64.         51.17647059]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]\n",
            " [73.87726688 64.87272441 57.30455595 46.88636228]\n",
            " [73.39191794 64.71050191 58.3812567  46.45134062]\n",
            " [72.99582005 64.65559393 58.47684634 46.92992806]\n",
            " [72.36423743 64.68977988 58.20446515 47.99578495]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[71.18284 64.79686 57.96437 49.57188]]\n",
            "expected:  [76.33333333 69.33333333 64.         51.17647059]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]\n",
            " [73.87726688 64.87272441 57.30455595 46.88636228]\n",
            " [73.39191794 64.71050191 58.3812567  46.45134062]\n",
            " [72.99582005 64.65559393 58.47684634 46.92992806]\n",
            " [72.36423743 64.68977988 58.20446515 47.99578495]\n",
            " [71.18284237 64.79685742 57.96436971 49.57187757]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[70.01671  64.90681  57.966328 50.820953]]\n",
            "expected:  [76.33333333 69.33333333 64.         51.17647059]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]\n",
            " [73.87726688 64.87272441 57.30455595 46.88636228]\n",
            " [73.39191794 64.71050191 58.3812567  46.45134062]\n",
            " [72.99582005 64.65559393 58.47684634 46.92992806]\n",
            " [72.36423743 64.68977988 58.20446515 47.99578495]\n",
            " [71.18284237 64.79685742 57.96436971 49.57187757]\n",
            " [70.01670337 64.90680462 57.96632737 50.82095236]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[69.294846 65.02426  58.24356  51.156506]]\n",
            "expected:  [76.33333333 70.22222222 64.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]\n",
            " [73.87726688 64.87272441 57.30455595 46.88636228]\n",
            " [73.39191794 64.71050191 58.3812567  46.45134062]\n",
            " [72.99582005 64.65559393 58.47684634 46.92992806]\n",
            " [72.36423743 64.68977988 58.20446515 47.99578495]\n",
            " [71.18284237 64.79685742 57.96436971 49.57187757]\n",
            " [70.01670337 64.90680462 57.96632737 50.82095236]\n",
            " [69.29484367 65.02426076 58.24356031 51.15650654]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[69.10765  65.18127  58.855682 50.330967]]\n",
            "expected:  [76.33333333 70.22222222 64.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]\n",
            " [73.87726688 64.87272441 57.30455595 46.88636228]\n",
            " [73.39191794 64.71050191 58.3812567  46.45134062]\n",
            " [72.99582005 64.65559393 58.47684634 46.92992806]\n",
            " [72.36423743 64.68977988 58.20446515 47.99578495]\n",
            " [71.18284237 64.79685742 57.96436971 49.57187757]\n",
            " [70.01670337 64.90680462 57.96632737 50.82095236]\n",
            " [69.29484367 65.02426076 58.24356031 51.15650654]\n",
            " [69.10764891 65.18126547 58.85567975 50.33096686]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[69.360954 65.36729  59.85804  48.412598]]\n",
            "expected:  [76.33333333 69.33333333 64.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]\n",
            " [73.87726688 64.87272441 57.30455595 46.88636228]\n",
            " [73.39191794 64.71050191 58.3812567  46.45134062]\n",
            " [72.99582005 64.65559393 58.47684634 46.92992806]\n",
            " [72.36423743 64.68977988 58.20446515 47.99578495]\n",
            " [71.18284237 64.79685742 57.96436971 49.57187757]\n",
            " [70.01670337 64.90680462 57.96632737 50.82095236]\n",
            " [69.29484367 65.02426076 58.24356031 51.15650654]\n",
            " [69.10764891 65.18126547 58.85567975 50.33096686]\n",
            " [69.36095083 65.36728632 59.85804307 48.41259666]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[69.805336 65.344894 60.67514  46.877728]]\n",
            "expected:  [76.33333333 69.33333333 64.         55.58823529]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]\n",
            " [73.87726688 64.87272441 57.30455595 46.88636228]\n",
            " [73.39191794 64.71050191 58.3812567  46.45134062]\n",
            " [72.99582005 64.65559393 58.47684634 46.92992806]\n",
            " [72.36423743 64.68977988 58.20446515 47.99578495]\n",
            " [71.18284237 64.79685742 57.96436971 49.57187757]\n",
            " [70.01670337 64.90680462 57.96632737 50.82095236]\n",
            " [69.29484367 65.02426076 58.24356031 51.15650654]\n",
            " [69.10764891 65.18126547 58.85567975 50.33096686]\n",
            " [69.36095083 65.36728632 59.85804307 48.41259666]\n",
            " [69.80533528 65.34489441 60.67513871 46.87772982]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[70.27622  65.175415 61.011185 46.186832]]\n",
            "expected:  [73.66666667 67.55555556 65.         49.41176471]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]\n",
            " [73.87726688 64.87272441 57.30455595 46.88636228]\n",
            " [73.39191794 64.71050191 58.3812567  46.45134062]\n",
            " [72.99582005 64.65559393 58.47684634 46.92992806]\n",
            " [72.36423743 64.68977988 58.20446515 47.99578495]\n",
            " [71.18284237 64.79685742 57.96436971 49.57187757]\n",
            " [70.01670337 64.90680462 57.96632737 50.82095236]\n",
            " [69.29484367 65.02426076 58.24356031 51.15650654]\n",
            " [69.10764891 65.18126547 58.85567975 50.33096686]\n",
            " [69.36095083 65.36728632 59.85804307 48.41259666]\n",
            " [69.80533528 65.34489441 60.67513871 46.87772982]\n",
            " [70.27622283 65.17541885 61.01118374 46.18683144]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[70.788956 65.03701  60.865425 45.987633]]\n",
            "expected:  [73.66666667 67.55555556 65.         49.41176471]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]\n",
            " [73.87726688 64.87272441 57.30455595 46.88636228]\n",
            " [73.39191794 64.71050191 58.3812567  46.45134062]\n",
            " [72.99582005 64.65559393 58.47684634 46.92992806]\n",
            " [72.36423743 64.68977988 58.20446515 47.99578495]\n",
            " [71.18284237 64.79685742 57.96436971 49.57187757]\n",
            " [70.01670337 64.90680462 57.96632737 50.82095236]\n",
            " [69.29484367 65.02426076 58.24356031 51.15650654]\n",
            " [69.10764891 65.18126547 58.85567975 50.33096686]\n",
            " [69.36095083 65.36728632 59.85804307 48.41259666]\n",
            " [69.80533528 65.34489441 60.67513871 46.87772982]\n",
            " [70.27622283 65.17541885 61.01118374 46.18683144]\n",
            " [70.78894997 65.03701329 60.86542463 45.98763205]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[71.66511  64.944626 60.173687 45.93034 ]]\n",
            "expected:  [73.66666667 69.33333333 65.         49.41176471]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]\n",
            " [73.87726688 64.87272441 57.30455595 46.88636228]\n",
            " [73.39191794 64.71050191 58.3812567  46.45134062]\n",
            " [72.99582005 64.65559393 58.47684634 46.92992806]\n",
            " [72.36423743 64.68977988 58.20446515 47.99578495]\n",
            " [71.18284237 64.79685742 57.96436971 49.57187757]\n",
            " [70.01670337 64.90680462 57.96632737 50.82095236]\n",
            " [69.29484367 65.02426076 58.24356031 51.15650654]\n",
            " [69.10764891 65.18126547 58.85567975 50.33096686]\n",
            " [69.36095083 65.36728632 59.85804307 48.41259666]\n",
            " [69.80533528 65.34489441 60.67513871 46.87772982]\n",
            " [70.27622283 65.17541885 61.01118374 46.18683144]\n",
            " [70.78894997 65.03701329 60.86542463 45.98763205]\n",
            " [71.66510391 64.94462281 60.17368901 45.93034068]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[72.79852  64.83642  59.255646 45.864002]]\n",
            "expected:  [73.66666667 69.33333333 65.         49.41176471]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]\n",
            " [73.87726688 64.87272441 57.30455595 46.88636228]\n",
            " [73.39191794 64.71050191 58.3812567  46.45134062]\n",
            " [72.99582005 64.65559393 58.47684634 46.92992806]\n",
            " [72.36423743 64.68977988 58.20446515 47.99578495]\n",
            " [71.18284237 64.79685742 57.96436971 49.57187757]\n",
            " [70.01670337 64.90680462 57.96632737 50.82095236]\n",
            " [69.29484367 65.02426076 58.24356031 51.15650654]\n",
            " [69.10764891 65.18126547 58.85567975 50.33096686]\n",
            " [69.36095083 65.36728632 59.85804307 48.41259666]\n",
            " [69.80533528 65.34489441 60.67513871 46.87772982]\n",
            " [70.27622283 65.17541885 61.01118374 46.18683144]\n",
            " [70.78894997 65.03701329 60.86542463 45.98763205]\n",
            " [71.66510391 64.94462281 60.17368901 45.93034068]\n",
            " [72.79852426 64.83641768 59.25564682 45.86400306]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicted Output:  [[73.63336  64.75961  58.553707 45.84188 ]]\n",
            "expected:  [72.33333333 70.22222222 67.         47.64705882]\n",
            "\n",
            "\n",
            "\n",
            "Input: \n",
            "[[72.33333333 70.22222222 67.         58.23529412]\n",
            " [69.76913869 66.81908059 57.54561144 48.07078555]\n",
            " [70.59785545 66.47170234 57.27382016 48.02596428]\n",
            " [71.42577112 65.70524013 57.28918582 48.4089797 ]\n",
            " [73.01237679 65.17540693 56.92926484 47.86906019]\n",
            " [73.87726688 64.87272441 57.30455595 46.88636228]\n",
            " [73.39191794 64.71050191 58.3812567  46.45134062]\n",
            " [72.99582005 64.65559393 58.47684634 46.92992806]\n",
            " [72.36423743 64.68977988 58.20446515 47.99578495]\n",
            " [71.18284237 64.79685742 57.96436971 49.57187757]\n",
            " [70.01670337 64.90680462 57.96632737 50.82095236]\n",
            " [69.29484367 65.02426076 58.24356031 51.15650654]\n",
            " [69.10764891 65.18126547 58.85567975 50.33096686]\n",
            " [69.36095083 65.36728632 59.85804307 48.41259666]\n",
            " [69.80533528 65.34489441 60.67513871 46.87772982]\n",
            " [70.27622283 65.17541885 61.01118374 46.18683144]\n",
            " [70.78894997 65.03701329 60.86542463 45.98763205]\n",
            " [71.66510391 64.94462281 60.17368901 45.93034068]\n",
            " [72.79852426 64.83641768 59.25564682 45.86400306]\n",
            " [73.63336611 64.75961381 58.55370915 45.8418802 ]]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 4ms/step\n",
            "Predicted Output:  [[73.801765 64.69251  58.395782 45.954605]]\n",
            "expected:  [72.33333333 70.22222222 67.         47.64705882]\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TATEl_RF8IpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}