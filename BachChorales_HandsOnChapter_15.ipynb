{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BachChorales_HandsOnChapter-15.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar-abhishek/handson-ml2/blob/master/BachChorales_HandsOnChapter_15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pugAaqoQ2miR",
        "colab_type": "code",
        "outputId": "489e7913-d325-489a-94e2-707d3b94fb56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# univariate lstm example\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "# split a univariate sequence into samples\n",
        "def split_sequence(sequence, n_steps):\n",
        "  X, y = list(), list()\n",
        "  for i in range(len(sequence)):\n",
        "    # find the end of this pattern\n",
        "    end_ix = i + n_steps\n",
        "    # check if we are beyond the sequence\n",
        "    if end_ix > len(sequence)-1:\n",
        "     break\n",
        "    # gather input and output parts of the pattern\n",
        "    seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "    X.append(seq_x)\n",
        "    y.append(seq_y)\n",
        "  return array(X), array(y)\n",
        "# define input sequence\n",
        "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
        "# choose a number of time steps\n",
        "n_steps = 3\n",
        "# split into samples\n",
        "X, y = split_sequence(raw_seq, n_steps)\n",
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[10 20 30]\n",
            " [20 30 40]\n",
            " [30 40 50]\n",
            " [40 50 60]\n",
            " [50 60 70]\n",
            " [60 70 80]]\n",
            "[40 50 60 70 80 90]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WVx7XFSRNCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
        "n_features = 1\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features))) model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "# fit model\n",
        "model.fit(X, y, epochs=200, verbose=0)\n",
        "# demonstrate prediction\n",
        "x_input = array([70, 80, 90])\n",
        "x_input = x_input.reshape((1, n_steps, n_features))\n",
        "# demonstrate prediction\n",
        "x_input = array([70, 80, 90])\n",
        "x_input = x_input.reshape((1, n_steps, n_features))\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "                                                                                   \n",
        "print(yhat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6Si9sjBRGoE",
        "colab_type": "code",
        "outputId": "7ff3921b-a9a0-4c9b-b15b-11d29fbd5cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# multivariate output stacked lstm example\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "# split a multivariate sequence into samples\n",
        "def split_sequences(sequences, n_steps):\n",
        "  X, y = list(), list()\n",
        "  for i in range(len(sequences)):\n",
        "    # find the end of this pattern\n",
        "    end_ix = i + n_steps\n",
        "    # check if we are beyond the dataset\n",
        "    if end_ix > len(sequences)-1:\n",
        "     break\n",
        "    # gather input and output parts of the pattern\n",
        "    seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n",
        "    X.append(seq_x)\n",
        "    y.append(seq_y)\n",
        "    return array(X), array(y)\n",
        "# define input sequence\n",
        "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
        "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
        "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
        "# convert to [rows, columns] structure\n",
        "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
        "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
        "out_seq = out_seq.reshape((len(out_seq), 1))\n",
        "# horizontally stack columns\n",
        "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
        "print(dataset)\n",
        "# choose a number of time steps\n",
        "n_steps = 3\n",
        "# convert into input/output\n",
        "X, y = split_sequences(dataset, n_steps)\n",
        "print(X)\n",
        "print(y)\n",
        "print(type(X))\n",
        "# the dataset knows the number of features, e.g. 2\n",
        "n_features = X.shape[2]\n",
        "print(n_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 10  15  25]\n",
            " [ 20  25  45]\n",
            " [ 30  35  65]\n",
            " [ 40  45  85]\n",
            " [ 50  55 105]\n",
            " [ 60  65 125]\n",
            " [ 70  75 145]\n",
            " [ 80  85 165]\n",
            " [ 90  95 185]]\n",
            "[[[10 15 25]\n",
            "  [20 25 45]\n",
            "  [30 35 65]]]\n",
            "[[40 45 85]]\n",
            "<class 'numpy.ndarray'>\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Azwr1ubOCLh",
        "colab_type": "code",
        "outputId": "459612d6-d934-412c-ef64-0a405a0a3f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
        "model.add(LSTM(100, activation='relu')) \n",
        "model.add(Dense(n_features)) \n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "# fit model\n",
        "model.fit(X, y, epochs=1000, verbose=0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7935424e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nndS5_BtODUw",
        "colab_type": "code",
        "outputId": "b707b692-945e-433d-d5eb-2db74ef0de79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# demonstrate prediction\n",
        "x_input = array([[70,75,145], [80,85,165], [90,95,185]])\n",
        "x_input = x_input.reshape((1, n_steps, n_features))\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "print(yhat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[197.93314 223.3317  407.88092]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fmXPTqmLxe9",
        "colab_type": "text"
      },
      "source": [
        "**Q10[part-1]. Download the Bach chorales dataset and unzip it. It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long, and each time step contains 4 integers, where each integer corresponds to a note’s index on a piano (except for the value 0, which means that no note is played). Train a model—recurrent, convolutional, or both—that can predict the next time step (four notes), given a sequence of time steps from a chorale. **bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k9Phc0e7EFM",
        "colab_type": "code",
        "outputId": "868a678a-d201-4658-eef8-a5ad2b7683b7",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fd9c95aa-335b-4260-b871-f5ba85e5c985\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-fd9c95aa-335b-4260-b871-f5ba85e5c985\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving chorale_000.csv to chorale_000.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpnbslo39PR_",
        "colab_type": "code",
        "outputId": "e7fb3c5b-0ca5-4862-fa13-afdae171ef2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "X_train_000 = pd.read_csv('chorale_000.csv')\n",
        "print(X_train_000.head(10))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   note0  note1  note2  note3\n",
            "0     74     70     65     58\n",
            "1     74     70     65     58\n",
            "2     74     70     65     58\n",
            "3     74     70     65     58\n",
            "4     75     70     58     55\n",
            "5     75     70     58     55\n",
            "6     75     70     60     55\n",
            "7     75     70     60     55\n",
            "8     77     69     62     50\n",
            "9     77     69     62     50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLZKNOoI9fBd",
        "colab_type": "code",
        "outputId": "36088d37-32e6-4937-b8f3-858bcde22703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "X_train_000 = X_train_000.to_numpy()\n",
        "dataX=[]\n",
        "dataY=[]\n",
        "for row in range(len(X_train_000)-1):\n",
        "    dataX.append(X_train_000[row])\n",
        "    dataY.append(X_train_000[row+1])\n",
        "print(dataX[0:5])\n",
        "print(dataY[0:5])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([74, 70, 65, 58]), array([74, 70, 65, 58]), array([74, 70, 65, 58]), array([74, 70, 65, 58]), array([75, 70, 58, 55])]\n",
            "[array([74, 70, 65, 58]), array([74, 70, 65, 58]), array([74, 70, 65, 58]), array([75, 70, 58, 55]), array([75, 70, 58, 55])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ziuhgmgj9p7s",
        "colab_type": "code",
        "outputId": "0a7bf102-052a-4a14-f9f3-8106777043f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "\"\"\"X = np.reshape(dataX, (len(dataX), 1, 4))\n",
        "X_train, X_test, y_train,  y_test = train_test_split(X, dataY, test_size=0.33, random_state=42)\n",
        "print(X_train[0:5])\n",
        "print(y_train[0:5])\n",
        "\"\"\"\n",
        "# choose a number of time steps\n",
        "n_steps = 3\n",
        "# convert into input/output\n",
        "i=0\n",
        "X=[]\n",
        "y=[]\n",
        "while i+2<len(dataX):\n",
        "  #multi-input\n",
        "  X.append(np.array([dataX[i], dataX[i+1], dataX[i+2]]).tolist())\n",
        "  y.append(dataX[i+1])\n",
        "  i += 1\n",
        "  \n",
        "X=np.asarray(X)\n",
        "y=np.asarray(y)\n",
        "print(X[0:2])\n",
        "print(y[0:2])\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[74 70 65 58]\n",
            "  [74 70 65 58]\n",
            "  [74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]\n",
            "  [74 70 65 58]\n",
            "  [74 70 65 58]]]\n",
            "[[74 70 65 58]\n",
            " [74 70 65 58]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMqQLUC19ryw",
        "colab_type": "code",
        "outputId": "0ea7346b-4fa2-46f3-a709-2281774507e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "# the dataset knows the number of features, e.g. 2\n",
        "n_features = X.shape[2]\n",
        "print(n_features)\n",
        "\n",
        "# choose a number of time steps\n",
        "n_steps = 3\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
        "model.add(LSTM(100, activation='relu')) \n",
        "model.add(Dense(n_features)) \n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "# fit model\n",
        "model.fit(X, y, epochs=100, verbose=0)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1d1896d7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flGBYEy0-gdD",
        "colab_type": "code",
        "outputId": "7598ceb8-eaa8-484d-d418-a817babc146e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# demonstrate prediction\n",
        "#print(X_train_000[0:5])\n",
        "x_input = X[5]\n",
        "print(x_input)\n",
        "x_input = x_input.reshape((1, n_steps, n_features))\n",
        "print(x_input)\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "print(yhat)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[75 70 58 55]\n",
            " [75 70 60 55]\n",
            " [75 70 60 55]]\n",
            "[[[75 70 58 55]\n",
            "  [75 70 60 55]\n",
            "  [75 70 60 55]]]\n",
            "[[76.28318  68.759834 59.762547 54.700706]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpvggFUajEDg",
        "colab_type": "text"
      },
      "source": [
        "**Q10[part-2] Then use this model to generate Bach-like music, one note at a time: you can do this by giving the model the start of a chorale and asking it to predict the next time step, then appending these time steps to the input sequence and asking the model for the next note, and so on. Also make sure to check out Google’s Coconet model, which was used for a nice Google doodle about Bach.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W78Jp-nAjKE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0aeece18-b483-4f15-cd08-064e47c41587"
      },
      "source": [
        "# the dataset knows the number of features, e.g. 2\n",
        "n_features = X.shape[2]\n",
        "print(n_features)\n",
        "\n",
        "# choose a number of time steps: random/variale increasing now\n",
        "n_steps = None\n",
        "\n",
        "# define model\n",
        "model_var_input = Sequential()\n",
        "model_var_input.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
        "model_var_input.add(LSTM(100, activation='relu')) \n",
        "model_var_input.add(Dense(n_features)) \n",
        "model_var_input.compile(optimizer='adam', loss='mse')\n",
        "# fit model\n",
        "model_var_input.fit(X, y, epochs=100, verbose=0)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1d1158c9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7FcFXcRWlrZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fdfe57dd-875c-4869-88a0-92f0a50bb49b"
      },
      "source": [
        "# choose a number of time steps\n",
        "n_steps = None\n",
        "# convert into input/output\n",
        "i=0\n",
        "n=1\n",
        "X=[]\n",
        "y=[]\n",
        "while i+1<len(dataX):\n",
        "  #multi-input\n",
        "  X.append(np.array([dataX[i]]).tolist())\n",
        "  y=dataX[i+1]\n",
        "  i += 1\n",
        "  X1=np.asarray(X)\n",
        "  y1=np.asarray(y)\n",
        "  # demonstrate prediction\n",
        "  x_input = X1\n",
        "  print('Input: ', x_input)\n",
        "  print('---------------')\n",
        "  print('Expected Output: ', y1)\n",
        "  print('---------------')\n",
        "  x_input = x_input.reshape((1, n, n_features))\n",
        "  n+=1\n",
        "  yhat = model_var_input.predict(x_input, verbose=1)\n",
        "  print('Predicated Output: ', yhat)\n",
        "  print(\"\\n\\n\")\n",
        "  if i>10:\n",
        "    break"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  [[[74 70 65 58]]]\n",
            "---------------\n",
            "Expected Output:  [74 70 65 58]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicated Output:  [[10.548267  11.647018   6.950516   7.2514567]]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]]\n",
            "---------------\n",
            "Expected Output:  [74 70 65 58]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicated Output:  [[35.980934 34.137165 28.10568  25.292353]]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]]\n",
            "---------------\n",
            "Expected Output:  [74 70 65 58]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 1ms/step\n",
            "Predicated Output:  [[76.00812  68.45716  64.09076  57.399487]]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]]\n",
            "---------------\n",
            "Expected Output:  [75 70 58 55]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 1ms/step\n",
            "Predicated Output:  [[134.52856 106.42965 117.29928  97.77108]]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[75 70 58 55]]]\n",
            "---------------\n",
            "Expected Output:  [75 70 58 55]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicated Output:  [[177.97794 132.78853 159.39134 135.98445]]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[75 70 58 55]]\n",
            "\n",
            " [[75 70 58 55]]]\n",
            "---------------\n",
            "Expected Output:  [75 70 60 55]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicated Output:  [[234.34633 198.65799 176.99179 137.3953 ]]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[75 70 58 55]]\n",
            "\n",
            " [[75 70 58 55]]\n",
            "\n",
            " [[75 70 60 55]]]\n",
            "---------------\n",
            "Expected Output:  [75 70 60 55]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicated Output:  [[342.65424 212.01955 285.52783 137.83517]]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[75 70 58 55]]\n",
            "\n",
            " [[75 70 58 55]]\n",
            "\n",
            " [[75 70 60 55]]\n",
            "\n",
            " [[75 70 60 55]]]\n",
            "---------------\n",
            "Expected Output:  [77 69 62 50]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicated Output:  [[504.4545  332.5229  405.7939  209.82957]]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[75 70 58 55]]\n",
            "\n",
            " [[75 70 58 55]]\n",
            "\n",
            " [[75 70 60 55]]\n",
            "\n",
            " [[75 70 60 55]]\n",
            "\n",
            " [[77 69 62 50]]]\n",
            "---------------\n",
            "Expected Output:  [77 69 62 50]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicated Output:  [[651.69    499.19662 623.97174 255.31717]]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[75 70 58 55]]\n",
            "\n",
            " [[75 70 58 55]]\n",
            "\n",
            " [[75 70 60 55]]\n",
            "\n",
            " [[75 70 60 55]]\n",
            "\n",
            " [[77 69 62 50]]\n",
            "\n",
            " [[77 69 62 50]]]\n",
            "---------------\n",
            "Expected Output:  [77 69 62 50]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 2ms/step\n",
            "Predicated Output:  [[864.04663 714.9714  873.27673 326.31036]]\n",
            "\n",
            "\n",
            "\n",
            "Input:  [[[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[74 70 65 58]]\n",
            "\n",
            " [[75 70 58 55]]\n",
            "\n",
            " [[75 70 58 55]]\n",
            "\n",
            " [[75 70 60 55]]\n",
            "\n",
            " [[75 70 60 55]]\n",
            "\n",
            " [[77 69 62 50]]\n",
            "\n",
            " [[77 69 62 50]]\n",
            "\n",
            " [[77 69 62 50]]]\n",
            "---------------\n",
            "Expected Output:  [77 69 62 50]\n",
            "---------------\n",
            "1/1 [==============================] - 0s 3ms/step\n",
            "Predicated Output:  [[1011.0655  1035.2703  1239.6199   471.31583]]\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjgMqOxYXRvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}