{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECPE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNLUReQv3HJy1xyJv9iihr2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar-abhishek/handson-ml2/blob/master/ECPE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGiWL8CG97xr",
        "colab_type": "text"
      },
      "source": [
        "Algorithm\n",
        "\n",
        "1. Take the document, split into clauses\n",
        "2. Find embeddings of the clauses\n",
        "3. Feed embeddings of clauses into a Bi-LSTM layer(word-level), followed by attention layer \n",
        "4. Output of previous layer gets copied into 2 components.\n",
        "5. 1 component is for emotion extraction and is a Bi-LSTM layer(clause-level)\n",
        "6. 2nd compoent is for cause extraction and is a Bi-LSTM layer(clause-level)\n",
        "7. Loss Lp of the whole model is the weighted sum of two components:\n",
        "Lp=n*Le+(1-n)*Lc\n",
        "where n is a hyper-param\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW2iw-d0JCe3",
        "colab_type": "code",
        "outputId": "fbe16199-fc08-41ae-b823-115422668d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 36kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.27.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.34.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.18.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.9.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 60.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (46.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=74182586624c53fa3faea26c413a71f6afeaa737cf2e685afbe272f1a5b74460\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: gast, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.2\n",
            "    Uninstalling tensorflow-1.15.2:\n",
            "      Successfully uninstalled tensorflow-1.15.2\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-2.2.0rc1 tensorflow-estimator-2.2.0rc0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxaa2j90QWiU",
        "colab_type": "code",
        "outputId": "41ae9eaf-3de0-4dd6-8879-8af636de87dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        }
      },
      "source": [
        "!pip install tensorflow-probability==0.8.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-probability==0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/72/29ef1e5f386b65544d4e7002dfeca1e55b099ed182cd6d405c21a19ae259/tensorflow_probability-0.8.0-py2.py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0) (4.4.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0) (1.18.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0) (1.12.0)\n",
            "Collecting cloudpickle==1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/24/fb/4f92f8c0f40a0d728b4f3d5ec5ff84353e705d8ff5e3e447620ea98b06bd/cloudpickle-1.1.1-py2.py3-none-any.whl\n",
            "Collecting gast<0.3,>=0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=cc89da8567f2f21ad86614c12a82b79eee9a043044a026d8af1c50c5bfc59590\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc2 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: gym 0.17.1 has requirement cloudpickle<1.4.0,>=1.2.0, but you'll have cloudpickle 1.1.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: cloudpickle, gast, tensorflow-probability\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-probability 0.9.0\n",
            "    Uninstalling tensorflow-probability-0.9.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.9.0\n",
            "Successfully installed cloudpickle-1.1.1 gast-0.2.2 tensorflow-probability-0.8.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cloudpickle",
                  "gast"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7ZKapWzJx47",
        "colab_type": "code",
        "outputId": "a32d08ec-2601-4b1f-d258-0bfbacb5ba6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "%tensorflow_version\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently selected TF version: 2.x\n",
            "Available versions:\n",
            "* 1.x\n",
            "* 2.x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_h12sNp3T2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1dRpjtec_sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emotion_seeds = set([\"ashamed\", \"delighted\", \"pleased\", \"concerned\", \"delight\", \"happy\", \"embarrassed\", \"furious\", \"nervous\", \n",
        "                     \"miffed\", \"angry\", \"mad\", \"anger\", \"excitement\", \"horror\", \"resentful\", \"astonished\", \"revulsion\", \n",
        "                     \"frightened\", \"cross\", \"sad\", \"down\", \"astonishment\", \"miserable\", \"worried\", \"sorrow\", \"overjoyed\",\n",
        "                     \"dismay\", \"grief\", \"annoyance\", \"alarmed\", \"astounded\", \"anguish\", \"despair\", \"infuriated\", \n",
        "                     \"embarrassment\", \"peeved\", \"amused\", \"disgruntled\", \"indignant\", \"thrilled\", \"anxious\", \"excited\",\n",
        "                     \"exasperation\", \"petrified\", \"heartbroken\", \"saddened\", \"depressed\", \"dismayed\", \"frustrated\", \"fedup\", \"livid\",\n",
        "                     \"revulsion\", \"bewildered\", \"flabbergasted\", \"happier\", \"ecstatic\", \"elation\", \"exhilarated\", \"exhilaration\",\n",
        "                     \"glee\", \"gleeful\", \"crestfallen\", \"sadness\", \"amusement\", \"dejected\", \"desolate\", \"despondency\", \"horrors\",\n",
        "                     \"agitated\", \"disquiet\", \"horrified\", \"exasperated\", \"irked\", \"disgruntlement\", \"sickened\", \"revolted\",\n",
        "                     \"devastated\", \"heartbreak\", \"inconsolable\", \"bewilderment\", \"nonplussed\", \"puzzlement\", \"disquieted\",\n",
        "                     \"glum\", \"downcast\", \"griefstricken\", \"startled\", \"disgusted\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dCwi_DhBrBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input is sentence, output is emotion cause pairs\n",
        "# Determine clauses by splitting on punctuation.\n",
        "\n",
        "# preprocess\n",
        "\n",
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def remove_nonascii(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "  return w\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = remove_nonascii(w)  \n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM7Kub3NJDyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_cause(text):\n",
        "  cur_cause=''\n",
        "  try:\n",
        "    cur_cause = re.findall('<cause>(.*?)<\\\\\\cause>', text)[0]\n",
        "    # Remove tags from line\n",
        "    text=re.sub('<cause>', '', text)\n",
        "    text=re.sub('<\\\\\\cause>', '', text)\n",
        "  except:\n",
        "    pass\n",
        "  #print('here:', text)\n",
        "  return (cur_cause, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSCLxRvA7mbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_filter_clauses(all_clauses):\n",
        "  cause = ''\n",
        "  clauses=[]\n",
        "  for clause in all_clauses:\n",
        "    e_cause, e_text = extract_cause(clause)\n",
        "    if e_cause!='':\n",
        "      cause = remove_nonascii(e_cause)\n",
        "    clauses.append(remove_nonascii(e_text))\n",
        "  return cause, clauses\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8b9UvTqebR6",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBMZz5Le2f71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "path_to_file = \"data.txt\"  \n",
        "# 1. Remove any accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [document, emotion, cause, clauses list]\n",
        "document=[]\n",
        "emotion=[]\n",
        "cause=[]\n",
        "clause=[]\n",
        "def create_dataset(path, num_examples):\n",
        "  document.clear()\n",
        "  emotion.clear()\n",
        "  cause.clear()\n",
        "  clause.clear()\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  for i, line in enumerate(lines[:num_examples]):\n",
        "    cur_emotion = re.findall('<(.*?)>', line)[0]\n",
        "    # removing emotion tag in document\n",
        "    text_without_emotion=line[2+len(cur_emotion):len(line)-len(cur_emotion)-3]\n",
        "    #document.append(text_without_emotion)\n",
        "    emotion.append(cur_emotion)\n",
        "\n",
        "    # Determine clauses by splitting on punctuation.\n",
        "    all_clauses = re.split(\"[.,!;:\\\"]+\", text_without_emotion)\n",
        "    filter_cause, filter_clauses = clean_filter_clauses(all_clauses)\n",
        "    cause.append(filter_cause)\n",
        "    clause.append([filter_clauses])\n",
        "    doc = extract_cause(text_without_emotion)[1]\n",
        "    # clean up document\n",
        "    clean_doc = preprocess_sentence(doc)\n",
        "    document.append(clean_doc)\n",
        "\n",
        "    # clean up clauses\n",
        "    # TODO\n",
        "  return [document, emotion, cause, clause]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve40pjba24tJ",
        "colab_type": "code",
        "outputId": "7d35fd4f-9c55-472f-bd83-7e39ef05e6ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "document, emotion, cause, clause_list = create_dataset(path_to_file, 500)\n",
        "print(len(document))\n",
        "for i in range(5):\n",
        "  print(document[i])\n",
        "  print(emotion[i])\n",
        "  print(cause[i])\n",
        "  print(clause_list[i])\n",
        "  print('\\n--------\\n')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "<start> i suppose i am happy , being so tiny it means i am able to surprise people with what is generally seen as my confident and outgoing personality . <end>\n",
            "happy\n",
            "being so tiny\n",
            "[['i suppose i am happy', 'being so tiny', 'it means i am able to surprise people with what is generally seen as my confident and outgoing personality', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> lennox has always truly wanted to fight for the world title and was happy , because he was taking the tough route . <end>\n",
            "happy\n",
            "because he was taking the tough route\n",
            "[['lennox has always truly wanted to fight for the world title and was happy', 'because he was taking the tough route', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> he was a professional musician now , still sensitive and happy , doing something he loved . <end>\n",
            "happy\n",
            "doing something he loved\n",
            "[['he was a professional musician now', 'still sensitive and happy', 'doing something he loved', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> holmes is happy , because , he has the freedom of the house when we are out . <end>\n",
            "happy\n",
            "he has the freedom of the house when we are out\n",
            "[['holmes is happy', 'because', 'he has the freedom of the house when we are out', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> i had problems with tutors trying to encourage me to diversify my work and experiment with other styles , but i was quite happy , with the direction my work was heading so i stubbornly stuck to it . <end>\n",
            "happy\n",
            "with the direction my work was heading\n",
            "[['i had problems with tutors trying to encourage me to diversify my work and experiment with other styles', 'but i was quite happy', 'with the direction my work was heading so i stubbornly stuck to it', '']]\n",
            "\n",
            "--------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnB45wnBgip8",
        "colab_type": "code",
        "outputId": "6da21042-bab2-4f96-8bb3-381f7023ba12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "X=document\n",
        "#X=[tf.constant(sentence) for sentence in document]\n",
        "#print(X[0], X[0].shape, (X[0].numpy()))\n",
        "y=emotion\n",
        "print(y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBVHLFB0Ky5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_emotion=keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"xxxxxxx\")\n",
        "tokenizer_emotion.fit_on_texts(emotion_seeds)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iAnFAxmxtVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer=keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"xxxxxxx\")\n",
        "tokenizer_emotion=keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"xxxxxxx\")\n",
        "def use_tokenizer():\n",
        "  tokenizer.fit_on_texts(X)\n",
        "  tokenizer_emotion.fit_on_texts(emotion_seeds)\n",
        "  X_dict=tokenizer.word_index\n",
        "\n",
        "  X_seq=tokenizer.texts_to_sequences(X)\n",
        "  X_padded_seq=pad_sequences(X_seq,padding='post',maxlen=40) \n",
        "  print(X_padded_seq[:3], X_padded_seq.shape, type(X_padded_seq))\n",
        "  print(X_padded_seq.shape)\n",
        "  \n",
        "  return X_padded_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJki4OW_x-vi",
        "colab_type": "code",
        "outputId": "5c9aa9e1-d4a2-4a1c-e33f-f5c788fadd12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "X_padded_seq = use_tokenizer()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  3  10 450  10  56  46  40  30 825  21 451  10  56 246   5 452 113  15\n",
            "   57  28 453 308  31  43 826   8 827 828   2   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  3 829  52 211 830 212   5 831  19   4 114 832   8   9  46 115  11   9\n",
            "  309   4 833 834   2   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  3  11   9  12 454 835  73  96 836   8  46 310 837  11 311   2   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]] (500, 40) <class 'numpy.ndarray'>\n",
            "(500, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkIi0Z7zgFoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=tokenizer_emotion.texts_to_sequences(y)\n",
        "y=np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fS5mXe3ZTlX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc766b1a-b855-498f-85d9-13c1d35aa403"
      },
      "source": [
        "print(y.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImlTCLwqM2g-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "49d089d4-6a48-4caa-f949-141134083aff"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "clauses = keras.layers.Input(shape=X_padded_seq.shape[1:], name='clause_input')\n",
        "clauses_embeddings = keras.layers.Embedding(input_length=40,input_dim=10000,output_dim=50) (clauses)\n",
        "output1 = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True))(clauses_embeddings)\n",
        "\n",
        "attention_output = keras.layers.Attention()([output1, output1, output1]) # no idea why we use output3, 3 times TODO!!!\n",
        "\n",
        "output_emotion = keras.layers.Bidirectional(keras.layers.LSTM(64))(attention_output)\n",
        "output_emotion_dense1 = keras.layers.Dense(128, activation=\"relu\") (output_emotion)\n",
        "output_emotion_dense2 = keras.layers.Dense(len(tokenizer_emotion.word_index), activation='softmax', name='emotion_output') (output_emotion_dense1)\n",
        "\n",
        "\n",
        "output_cause = keras.layers.Bidirectional(keras.layers.LSTM(64))(attention_output)\n",
        "output_cause_dense1 = keras.layers.Dense(128, activation=\"relu\") (output_cause)\n",
        "output_cause_dense2 = keras.layers.Dense(len(tokenizer_emotion.word_index), activation='softmax', name='clause_output') (output_cause_dense1)\n",
        "\n",
        "model = keras.Model(inputs=clauses, outputs=[output_emotion_dense2, output_cause_dense2])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", loss_weights=[0.5, 0.5], metrics=['accuracy'])\n",
        "model.fit({'clause_input': X_padded_seq}, {'emotion_output': y, 'clause_output': y}, epochs=20)\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 3.9162 - emotion_output_loss: 3.9843 - clause_output_loss: 3.8480 - emotion_output_accuracy: 0.3980 - clause_output_accuracy: 0.3800\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 1.5899 - emotion_output_loss: 1.7380 - clause_output_loss: 1.4419 - emotion_output_accuracy: 0.4120 - clause_output_accuracy: 0.4220\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 1.3551 - emotion_output_loss: 1.3663 - clause_output_loss: 1.3440 - emotion_output_accuracy: 0.4200 - clause_output_accuracy: 0.4220\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 1.3348 - emotion_output_loss: 1.3361 - clause_output_loss: 1.3335 - emotion_output_accuracy: 0.4220 - clause_output_accuracy: 0.4220\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 1.3397 - emotion_output_loss: 1.3359 - clause_output_loss: 1.3435 - emotion_output_accuracy: 0.4220 - clause_output_accuracy: 0.4220\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 1.3323 - emotion_output_loss: 1.3296 - clause_output_loss: 1.3350 - emotion_output_accuracy: 0.4220 - clause_output_accuracy: 0.4220\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 1.3230 - emotion_output_loss: 1.3248 - clause_output_loss: 1.3212 - emotion_output_accuracy: 0.4220 - clause_output_accuracy: 0.4220\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 1.2977 - emotion_output_loss: 1.2919 - clause_output_loss: 1.3035 - emotion_output_accuracy: 0.4220 - clause_output_accuracy: 0.4220\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 1.1489 - emotion_output_loss: 1.1020 - clause_output_loss: 1.1957 - emotion_output_accuracy: 0.4740 - clause_output_accuracy: 0.4340\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.8191 - emotion_output_loss: 0.7838 - clause_output_loss: 0.8544 - emotion_output_accuracy: 0.6020 - clause_output_accuracy: 0.5920\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.7802 - emotion_output_loss: 0.7807 - clause_output_loss: 0.7797 - emotion_output_accuracy: 0.5980 - clause_output_accuracy: 0.6000\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.7176 - emotion_output_loss: 0.7223 - clause_output_loss: 0.7128 - emotion_output_accuracy: 0.6120 - clause_output_accuracy: 0.6160\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.6945 - emotion_output_loss: 0.6931 - clause_output_loss: 0.6959 - emotion_output_accuracy: 0.6380 - clause_output_accuracy: 0.6160\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.6944 - emotion_output_loss: 0.6987 - clause_output_loss: 0.6901 - emotion_output_accuracy: 0.6380 - clause_output_accuracy: 0.6300\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.6797 - emotion_output_loss: 0.6804 - clause_output_loss: 0.6790 - emotion_output_accuracy: 0.5960 - clause_output_accuracy: 0.5940\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.6542 - emotion_output_loss: 0.6543 - clause_output_loss: 0.6540 - emotion_output_accuracy: 0.6300 - clause_output_accuracy: 0.6240\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.6567 - emotion_output_loss: 0.6565 - clause_output_loss: 0.6569 - emotion_output_accuracy: 0.5980 - clause_output_accuracy: 0.5980\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.6612 - emotion_output_loss: 0.6611 - clause_output_loss: 0.6613 - emotion_output_accuracy: 0.6320 - clause_output_accuracy: 0.6380\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.6671 - emotion_output_loss: 0.6662 - clause_output_loss: 0.6680 - emotion_output_accuracy: 0.5940 - clause_output_accuracy: 0.5860\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 0.6598 - emotion_output_loss: 0.6591 - clause_output_loss: 0.6604 - emotion_output_accuracy: 0.5900 - clause_output_accuracy: 0.5900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0bbd6bef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdLpJmFgyU-d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d27c9aa1-0777-46b0-99c0-622f9364a68e"
      },
      "source": [
        "model.evaluate(X_padded_seq, y, verbose=0)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6723743081092834,\n",
              " 0.6715744137763977,\n",
              " 0.6731741428375244,\n",
              " 0.6359999775886536,\n",
              " 0.6359999775886536]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83QqHuUhDTzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "db19a8d5-02e1-4e4e-a652-e7b712f5fa58"
      },
      "source": [
        "model.predict({'clause_input': X_padded_seq})"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[2.24426984e-08, 2.94211437e-04, 4.46459154e-08, ...,\n",
              "         2.34549113e-09, 3.98510691e-09, 4.38057768e-08],\n",
              "        [2.06071569e-08, 2.61568348e-04, 4.07083789e-08, ...,\n",
              "         2.07825179e-09, 3.53869312e-09, 4.01161806e-08],\n",
              "        [2.18395524e-08, 2.82198773e-04, 4.32992202e-08, ...,\n",
              "         2.24915397e-09, 3.81770260e-09, 4.25323385e-08],\n",
              "        ...,\n",
              "        [5.87996419e-06, 3.27096820e-01, 7.36006359e-06, ...,\n",
              "         5.77178116e-06, 1.19048336e-05, 1.22625552e-05],\n",
              "        [5.86108354e-06, 3.26503754e-01, 7.33830575e-06, ...,\n",
              "         5.74995738e-06, 1.18494972e-05, 1.22302345e-05],\n",
              "        [5.87799695e-06, 3.25428605e-01, 7.37655682e-06, ...,\n",
              "         5.76614229e-06, 1.18329581e-05, 1.22892952e-05]], dtype=float32),\n",
              " array([[1.6303733e-08, 4.3151993e-04, 1.1161738e-08, ..., 9.2581601e-08,\n",
              "         1.3225321e-08, 4.5378727e-08],\n",
              "        [1.4596586e-08, 3.8640076e-04, 9.8934372e-09, ..., 8.3660829e-08,\n",
              "         1.1561262e-08, 4.0990972e-08],\n",
              "        [1.5907480e-08, 4.1739779e-04, 1.0880015e-08, ..., 9.0495973e-08,\n",
              "         1.2827544e-08, 4.4325844e-08],\n",
              "        ...,\n",
              "        [5.1844022e-06, 3.2822540e-01, 3.9008278e-06, ..., 3.4399905e-06,\n",
              "         2.4639134e-05, 7.1456307e-06],\n",
              "        [5.1951474e-06, 3.2796147e-01, 3.9106608e-06, ..., 3.4490042e-06,\n",
              "         2.4629573e-05, 7.1639042e-06],\n",
              "        [5.2636387e-06, 3.2762823e-01, 3.9656775e-06, ..., 3.4999521e-06,\n",
              "         2.4770387e-05, 7.2560047e-06]], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmLaTMIiApix",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "5aca2b0e-fe5c-4966-924c-df09f9898085"
      },
      "source": [
        "# test code to test the model prediction\n",
        "test_input=['he was mad']\n",
        "test_X_seq=tokenizer.texts_to_sequences(test_input)\n",
        "print(test_X_seq)\n",
        "\n",
        "test_X_padded_seq=pad_sequences(test_X_seq,padding='post',maxlen=40)\n",
        "print(test_X_padded_seq)\n",
        "\n",
        "output = model.predict_classes(test_X_padded_seq)\n",
        "print(output)\n",
        "for word, index in tokenizer_emotion.word_index.items():\n",
        "  if index==output:\n",
        "    print(word, index, output)\n",
        "    break"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[11, 9, 85]]\n",
            "[[11  9 85  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-143-f4b396d55260>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X_padded_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X_padded_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_emotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'predict_classes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYfGGsAHAqLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G35OjyJAqN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk45JO0kAk9E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "03f02889-497f-472d-fd30-4570842870e0"
      },
      "source": [
        "# this piece of code just takes document as input and predicts emotion as the output of the document\n",
        "text_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_length=40,input_dim=10000,output_dim=50),\n",
        "    #tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    #tf.keras.layers.Attention(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(len(tokenizer_emotion.word_index), activation='softmax')\n",
        "])\n",
        "\n",
        "text_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "text_model.summary()\n",
        "\n",
        "print(X_padded_seq.shape, y.shape)\n",
        "text_model.fit(X_padded_seq, y, epochs=5)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_36 (Embedding)     (None, 40, 50)            500000    \n",
            "_________________________________________________________________\n",
            "bidirectional_127 (Bidirecti (None, 128)               58880     \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 89)                11481     \n",
            "=================================================================\n",
            "Total params: 586,873\n",
            "Trainable params: 586,873\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(500, 40) (500, 1)\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 4.2073 - accuracy: 0.3600\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.7361 - accuracy: 0.3880\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.3620 - accuracy: 0.3660\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.3444 - accuracy: 0.4220\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.3253 - accuracy: 0.4220\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fad09986438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF191PQCxbdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d2f4501d-9814-42e4-aa1e-a148ef286b9c"
      },
      "source": [
        "# test code to test the model prediction\n",
        "test_input=['he was mad']\n",
        "test_X_seq=tokenizer.texts_to_sequences(test_input)\n",
        "print(test_X_seq)\n",
        "\n",
        "test_X_padded_seq=pad_sequences(test_X_seq,padding='post',maxlen=40)\n",
        "print(test_X_padded_seq)\n",
        "\n",
        "output = text_model.predict_classes(test_X_padded_seq)\n",
        "print(output)\n",
        "for word, index in tokenizer_emotion.word_index.items():\n",
        "  if index==output:\n",
        "    print(word, index, output)\n",
        "    break"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[11, 9, 85]]\n",
            "[[11  9 85  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
            "[68]\n",
            "happy 68 [68]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XGR9Ukf2GJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zP2vDsB2GMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10Imo1ef2GN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN2pgZyTyqrK",
        "colab_type": "text"
      },
      "source": [
        "Questions:\n",
        "1. Do we need seeding the model?\n",
        "2. HW:\n",
        " a. Use pretrained embeddings\n",
        " **b. use functional apis**\n",
        " c. Limit output to the emotions set(say 10)\n",
        " d. Look at other sources of data(emotion-cause or emotion-entailments)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxoQxxIyEASK",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}