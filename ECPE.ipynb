{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECPE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPHEgTGYfiuHNTdCrvlcKAO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar-abhishek/handson-ml2/blob/master/ECPE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGiWL8CG97xr",
        "colab_type": "text"
      },
      "source": [
        "Algorithm\n",
        "\n",
        "1. Take the document, split into clauses\n",
        "2. Find embeddings of the clauses\n",
        "3. Feed embeddings of clauses into a Bi-LSTM layer(word-level), followed by attention layer \n",
        "4. Output of previous layer gets copied into 2 components.\n",
        "5. 1 component is for emotion extraction and is a Bi-LSTM layer(clause-level)\n",
        "6. 2nd compoent is for cause extraction and is a Bi-LSTM layer(clause-level)\n",
        "7. Loss Lp of the whole model is the weighted sum of two components:\n",
        "Lp=n*Le+(1-n)*Lc\n",
        "where n is a hyper-param\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW2iw-d0JCe3",
        "colab_type": "code",
        "outputId": "fbe16199-fc08-41ae-b823-115422668d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 36kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.27.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.34.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.18.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.9.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 60.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (46.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=74182586624c53fa3faea26c413a71f6afeaa737cf2e685afbe272f1a5b74460\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: gast, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.2\n",
            "    Uninstalling tensorflow-1.15.2:\n",
            "      Successfully uninstalled tensorflow-1.15.2\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-2.2.0rc1 tensorflow-estimator-2.2.0rc0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxaa2j90QWiU",
        "colab_type": "code",
        "outputId": "41ae9eaf-3de0-4dd6-8879-8af636de87dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        }
      },
      "source": [
        "!pip install tensorflow-probability==0.8.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-probability==0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/72/29ef1e5f386b65544d4e7002dfeca1e55b099ed182cd6d405c21a19ae259/tensorflow_probability-0.8.0-py2.py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0) (4.4.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0) (1.18.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.8.0) (1.12.0)\n",
            "Collecting cloudpickle==1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/24/fb/4f92f8c0f40a0d728b4f3d5ec5ff84353e705d8ff5e3e447620ea98b06bd/cloudpickle-1.1.1-py2.py3-none-any.whl\n",
            "Collecting gast<0.3,>=0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=cc89da8567f2f21ad86614c12a82b79eee9a043044a026d8af1c50c5bfc59590\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc2 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: gym 0.17.1 has requirement cloudpickle<1.4.0,>=1.2.0, but you'll have cloudpickle 1.1.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: cloudpickle, gast, tensorflow-probability\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-probability 0.9.0\n",
            "    Uninstalling tensorflow-probability-0.9.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.9.0\n",
            "Successfully installed cloudpickle-1.1.1 gast-0.2.2 tensorflow-probability-0.8.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cloudpickle",
                  "gast"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7ZKapWzJx47",
        "colab_type": "code",
        "outputId": "30a63a41-a887-4b0d-befb-349a149c1e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "%tensorflow_version\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently selected TF version: 2.x\n",
            "Available versions:\n",
            "* 1.x\n",
            "* 2.x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_h12sNp3T2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1dRpjtec_sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emotion_seeds = set([\"ashamed\", \"delighted\", \"pleased\", \"concerned\", \"delight\", \"happy\", \"embarrassed\", \"furious\", \"nervous\", \n",
        "                     \"miffed\", \"angry\", \"mad\", \"anger\", \"excitement\", \"horror\", \"resentful\", \"astonished\", \"revulsion\", \n",
        "                     \"frightened\", \"cross\", \"sad\", \"down\", \"astonishment\", \"miserable\", \"worried\", \"sorrow\", \"overjoyed\",\n",
        "                     \"dismay\", \"grief\", \"annoyance\", \"alarmed\", \"astounded\", \"anguish\", \"despair\", \"infuriated\", \n",
        "                     \"embarrassment\", \"peeved\", \"amused\", \"disgruntled\", \"indignant\", \"thrilled\", \"anxious\", \"excited\",\n",
        "                     \"exasperation\", \"petrified\", \"heartbroken\", \"saddened\", \"depressed\", \"dismayed\", \"frustrated\", \"fedup\", \"livid\",\n",
        "                     \"revulsion\", \"bewildered\", \"flabbergasted\", \"happier\", \"ecstatic\", \"elation\", \"exhilarated\", \"exhilaration\",\n",
        "                     \"glee\", \"gleeful\", \"crestfallen\", \"sadness\", \"amusement\", \"dejected\", \"desolate\", \"despondency\", \"horrors\",\n",
        "                     \"agitated\", \"disquiet\", \"horrified\", \"exasperated\", \"irked\", \"disgruntlement\", \"sickened\", \"revolted\",\n",
        "                     \"devastated\", \"heartbreak\", \"inconsolable\", \"bewilderment\", \"nonplussed\", \"puzzlement\", \"disquieted\",\n",
        "                     \"glum\", \"downcast\", \"griefstricken\", \"startled\", \"disgusted\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dCwi_DhBrBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input is sentence, output is emotion cause pairs\n",
        "# Determine clauses by splitting on punctuation.\n",
        "\n",
        "# preprocess\n",
        "\n",
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def remove_nonascii(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "  return w\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = remove_nonascii(w)  \n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM7Kub3NJDyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_cause(text):\n",
        "  cur_cause=''\n",
        "  try:\n",
        "    cur_cause = re.findall('<cause>(.*?)<\\\\\\cause>', text)[0]\n",
        "    # Remove tags from line\n",
        "    text=re.sub('<cause>', '', text)\n",
        "    text=re.sub('<\\\\\\cause>', '', text)\n",
        "  except:\n",
        "    pass\n",
        "  #print('here:', text)\n",
        "  return (cur_cause, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSCLxRvA7mbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_filter_clauses(all_clauses):\n",
        "  cause = ''\n",
        "  clauses=[]\n",
        "  for clause in all_clauses:\n",
        "    e_cause, e_text = extract_cause(clause)\n",
        "    if e_cause!='':\n",
        "      cause = remove_nonascii(e_cause)\n",
        "    clauses.append(remove_nonascii(e_text))\n",
        "  return cause, clauses\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8b9UvTqebR6",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBMZz5Le2f71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "path_to_file = \"data.txt\"  \n",
        "# 1. Remove any accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [document, emotion, cause, clauses list]\n",
        "document=[]\n",
        "emotion=[]\n",
        "cause=[]\n",
        "clause=[]\n",
        "def create_dataset(path, num_examples):\n",
        "  document.clear()\n",
        "  emotion.clear()\n",
        "  cause.clear()\n",
        "  clause.clear()\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  for i, line in enumerate(lines[:num_examples]):\n",
        "    cur_emotion = re.findall('<(.*?)>', line)[0]\n",
        "    # removing emotion tag in document\n",
        "    text_without_emotion=line[2+len(cur_emotion):len(line)-len(cur_emotion)-3]\n",
        "    #document.append(text_without_emotion)\n",
        "    emotion.append(cur_emotion)\n",
        "\n",
        "    # Determine clauses by splitting on punctuation.\n",
        "    all_clauses = re.split(\"[.,!;:\\\"]+\", text_without_emotion)\n",
        "    filter_cause, filter_clauses = clean_filter_clauses(all_clauses)\n",
        "    cause.append(filter_cause)\n",
        "    clause.append([filter_clauses])\n",
        "    doc = extract_cause(text_without_emotion)[1]\n",
        "    # clean up document\n",
        "    clean_doc = preprocess_sentence(doc)\n",
        "    document.append(clean_doc)\n",
        "\n",
        "    # clean up clauses\n",
        "    # TODO\n",
        "  return [document, emotion, cause, clause]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve40pjba24tJ",
        "colab_type": "code",
        "outputId": "329ce725-be6e-43ee-d83b-15b6bc0ef034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "document, emotion, cause, clause_list = create_dataset(path_to_file, 500)\n",
        "print(len(document))\n",
        "for i in range(5):\n",
        "  print(document[i])\n",
        "  print(emotion[i])\n",
        "  print(cause[i])\n",
        "  print(clause_list[i])\n",
        "  print('\\n--------\\n')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "<start> i suppose i am happy , being so tiny it means i am able to surprise people with what is generally seen as my confident and outgoing personality . <end>\n",
            "happy\n",
            "being so tiny\n",
            "[['i suppose i am happy', 'being so tiny', 'it means i am able to surprise people with what is generally seen as my confident and outgoing personality', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> lennox has always truly wanted to fight for the world title and was happy , because he was taking the tough route . <end>\n",
            "happy\n",
            "because he was taking the tough route\n",
            "[['lennox has always truly wanted to fight for the world title and was happy', 'because he was taking the tough route', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> he was a professional musician now , still sensitive and happy , doing something he loved . <end>\n",
            "happy\n",
            "doing something he loved\n",
            "[['he was a professional musician now', 'still sensitive and happy', 'doing something he loved', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> holmes is happy , because , he has the freedom of the house when we are out . <end>\n",
            "happy\n",
            "he has the freedom of the house when we are out\n",
            "[['holmes is happy', 'because', 'he has the freedom of the house when we are out', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> i had problems with tutors trying to encourage me to diversify my work and experiment with other styles , but i was quite happy , with the direction my work was heading so i stubbornly stuck to it . <end>\n",
            "happy\n",
            "with the direction my work was heading\n",
            "[['i had problems with tutors trying to encourage me to diversify my work and experiment with other styles', 'but i was quite happy', 'with the direction my work was heading so i stubbornly stuck to it', '']]\n",
            "\n",
            "--------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnB45wnBgip8",
        "colab_type": "code",
        "outputId": "73f0a3d0-1e65-43dc-930f-c5edb04c669a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "X=document\n",
        "#X=[tf.constant(sentence) for sentence in document]\n",
        "#print(X[0], X[0].shape, (X[0].numpy()))\n",
        "y=emotion\n",
        "print(y)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger', 'anger']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBVHLFB0Ky5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_emotion=keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"xxxxxxx\")\n",
        "tokenizer_emotion.fit_on_texts(emotion_seeds)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iAnFAxmxtVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer=keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"xxxxxxx\")\n",
        "tokenizer_emotion=keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"xxxxxxx\")\n",
        "def use_tokenizer():\n",
        "  tokenizer.fit_on_texts(X)\n",
        "  tokenizer_emotion.fit_on_texts(emotion_seeds)\n",
        "  X_dict=tokenizer.word_index\n",
        "\n",
        "  X_seq=tokenizer.texts_to_sequences(X)\n",
        "  X_padded_seq=pad_sequences(X_seq,padding='post',maxlen=40) \n",
        "  print(X_padded_seq[:3], X_padded_seq.shape, type(X_padded_seq))\n",
        "  print(X_padded_seq.shape)\n",
        "  \n",
        "  return X_padded_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJki4OW_x-vi",
        "colab_type": "code",
        "outputId": "d2cb864f-4e5c-4808-9582-fe7232db6265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "X_padded_seq = use_tokenizer()"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  3  10 450  10  56  46  40  30 825  21 451  10  56 246   5 452 113  15\n",
            "   57  28 453 308  31  43 826   8 827 828   2   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  3 829  52 211 830 212   5 831  19   4 114 832   8   9  46 115  11   9\n",
            "  309   4 833 834   2   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]\n",
            " [  3  11   9  12 454 835  73  96 836   8  46 310 837  11 311   2   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0]] (500, 40) <class 'numpy.ndarray'>\n",
            "(500, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkIi0Z7zgFoM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "f94d7243-d9e7-4c93-e099-2d0d83eb3c3a"
      },
      "source": [
        "y=tokenizer_emotion.texts_to_sequences(y)\n",
        "y=np.array(y)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-370403ac1870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_emotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \"\"\"\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    308\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                                             self.split)\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \"\"\"\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fS5mXe3ZTlX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb0718b1-4240-457a-85aa-5d5ea8151d04"
      },
      "source": [
        "print(y.shape)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImlTCLwqM2g-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b2580445-d3f9-4e68-8100-cbf018286363"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "clauses = keras.layers.Input(shape=X_padded_seq.shape[1:])\n",
        "clauses_embeddings = keras.layers.Embedding(input_length=40,input_dim=10000,output_dim=50) (clauses)\n",
        "output1 = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True))(clauses_embeddings)\n",
        "\n",
        "attention_output = keras.layers.Attention()([output1, output1, output1]) # no idea why we use output3, 3 times TODO!!!\n",
        "\n",
        "output_emotion = keras.layers.Bidirectional(keras.layers.LSTM(64))(attention_output)\n",
        "output_emotion_dense1 = keras.layers.Dense(128, activation=\"relu\") (output_emotion)\n",
        "output_emotion_dense2 = keras.layers.Dense(len(tokenizer_emotion.word_index), activation='softmax') (output_emotion_dense1)\n",
        "\n",
        "\n",
        "output_cause = keras.layers.Bidirectional(keras.layers.LSTM(64))(attention_output)\n",
        "output_cause_dense1 = keras.layers.Dense(128, activation=\"relu\") (output_cause)\n",
        "output_cause_dense2 = keras.layers.Dense(len(tokenizer_emotion.word_index), activation='softmax') (output_cause_dense1)\n",
        "\n",
        "model = keras.Model(inputs=clauses, outputs=[output_emotion_dense2, output_cause_dense2])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "model.fit(X_padded_seq, y, epochs=2)\n",
        "\n",
        "#loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 7.2910 - dense_54_loss: 3.5418 - dense_56_loss: 3.7492 - dense_54_accuracy: 0.3760 - dense_56_accuracy: 0.3980\n",
            "Epoch 2/2\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 2.8390 - dense_54_loss: 1.3489 - dense_56_loss: 1.4900 - dense_54_accuracy: 0.4240 - dense_56_accuracy: 0.3520\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fad177e8160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdLpJmFgyU-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT1VS2u-gF9N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "699de9fe-86f6-43f3-b46b-a902c996769a"
      },
      "source": [
        "# this piece of code just takes document as input and predicts emotion as the output of the document\n",
        "text_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_length=40,input_dim=10000,output_dim=50),\n",
        "    #tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    #tf.keras.layers.Attention(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(len(tokenizer_emotion.word_index), activation='softmax')\n",
        "])\n",
        "\n",
        "text_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "text_model.summary()\n",
        "\n",
        "print(X_padded_seq.shape, y.shape)\n",
        "text_model.fit(X_padded_seq, y, epochs=5)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_15 (Embedding)     (None, 40, 50)            500000    \n",
            "_________________________________________________________________\n",
            "bidirectional_83 (Bidirectio (None, 128)               58880     \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 89)                11481     \n",
            "=================================================================\n",
            "Total params: 586,873\n",
            "Trainable params: 586,873\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(500, 40) (500, 1)\n",
            "Epoch 1/5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.2503 - accuracy: 0.3500\n",
            "Epoch 2/5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.6925 - accuracy: 0.4220\n",
            "Epoch 3/5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.3488 - accuracy: 0.4220\n",
            "Epoch 4/5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.3505 - accuracy: 0.3740\n",
            "Epoch 5/5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 1.3753 - accuracy: 0.3980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fad250b9b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF191PQCxbdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "503f4522-b305-4000-bbe6-1a3a859ca8bd"
      },
      "source": [
        "# test code to test the model prediction\n",
        "test_input=['he was mad']\n",
        "test_X_seq=tokenizer.texts_to_sequences(test_input)\n",
        "print(test_X_seq)\n",
        "\n",
        "test_X_padded_seq=pad_sequences(test_X_seq,padding='post',maxlen=40)\n",
        "print(test_X_padded_seq)\n",
        "\n",
        "output = text_model.predict_classes(test_X_padded_seq)\n",
        "print(output)\n",
        "for word, index in tokenizer_emotion.word_index.items():\n",
        "  if index==output:\n",
        "    print(word, index, output)\n",
        "    break"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[11, 9, 85]]\n",
            "[[11  9 85  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
            "[68]\n",
            "happy 68 [68]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XGR9Ukf2GJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zP2vDsB2GMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10Imo1ef2GN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN2pgZyTyqrK",
        "colab_type": "text"
      },
      "source": [
        "Questions:\n",
        "1. Do we need seeding the model?\n",
        "2. HW:\n",
        " a. Use pretrained embeddings\n",
        " **b. use functional apis**\n",
        " c. Limit output to the emotions set(say 10)\n",
        " d. Look at other sources of data(emotion-cause or emotion-entailments)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxoQxxIyEASK",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}