{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECPE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNy3vH9iVqNPstJdleA18mk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar-abhishek/handson-ml2/blob/master/ECPE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGiWL8CG97xr",
        "colab_type": "text"
      },
      "source": [
        "Algorithm\n",
        "\n",
        "1. Take the document, split into clauses\n",
        "2. Find embeddings of the clauses\n",
        "3. Feed embeddings of clauses into a Bi-LSTM layer(word-level), followed by attention layer \n",
        "4. Output of previous layer gets copied into 2 components.\n",
        "5. 1 component is for emotion extraction and is a Bi-LSTM layer(clause-level)\n",
        "6. 2nd compoent is for cause extraction and is a Bi-LSTM layer(clause-level)\n",
        "7. Loss Lp of the whole model is the weighted sum of two components:\n",
        "Lp=n*Le+(1-n)*Lc\n",
        "where n is a hyper-param\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_h12sNp3T2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dCwi_DhBrBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input is sentence, output is emotion cause pairs\n",
        "# Determine clauses by splitting on punctuation.\n",
        "\n",
        "# preprocess\n",
        "\n",
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def remove_nonascii(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "  return w\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = remove_nonascii(w)  \n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM7Kub3NJDyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_cause(text):\n",
        "  cur_cause=''\n",
        "  try:\n",
        "    cur_cause = re.findall('<cause>(.*?)<\\\\\\cause>', text)[0]\n",
        "    # Remove tags from line\n",
        "    text=re.sub('<cause>', '', text)\n",
        "    text=re.sub('<\\\\\\cause>', '', text)\n",
        "  except:\n",
        "    pass\n",
        "  #print('here:', text)\n",
        "  return (cur_cause, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSCLxRvA7mbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_filter_clauses(all_clauses):\n",
        "  cause = ''\n",
        "  clauses=[]\n",
        "  for clause in all_clauses:\n",
        "    e_cause, e_text = extract_cause(clause)\n",
        "    if e_cause!='':\n",
        "      cause = remove_nonascii(e_cause)\n",
        "    clauses.append(remove_nonascii(e_text))\n",
        "  return cause, clauses\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8b9UvTqebR6",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBMZz5Le2f71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = \"data.txt\"  \n",
        "# 1. Remove any accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [document, emotion, cause, clauses list]\n",
        "document=[]\n",
        "emotion=[]\n",
        "cause=[]\n",
        "clause=[]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  for i, line in enumerate(lines[:num_examples]):\n",
        "    cur_emotion = re.findall('<(.*?)>', line)[0]\n",
        "    # removing emotion tag in document\n",
        "    text_without_emotion=line[2+len(cur_emotion):len(line)-len(cur_emotion)-3]\n",
        "    #document.append(text_without_emotion)\n",
        "    emotion.append(cur_emotion)\n",
        "\n",
        "    # Determine clauses by splitting on punctuation.\n",
        "    all_clauses = re.split(\"[.,!;:\\\"]+\", text_without_emotion)\n",
        "    filter_cause, filter_clauses = clean_filter_clauses(all_clauses)\n",
        "    cause.append(filter_cause)\n",
        "    clause.append([filter_clauses])\n",
        "    doc = extract_cause(text_without_emotion)[1]\n",
        "    # clean up document\n",
        "    clean_doc = preprocess_sentence(doc)\n",
        "    document.append(clean_doc)\n",
        "\n",
        "    # clean up clauses\n",
        "    # TODO\n",
        "  return [document, emotion, cause, clause]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve40pjba24tJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "916df53a-5a9d-4349-ee0a-45492f690c4e"
      },
      "source": [
        "document, emotion, cause, clause_list = create_dataset(path_to_file, 5)\n",
        "for i in range(5):\n",
        "  print(document[i])\n",
        "  print(emotion[i])\n",
        "  print(cause[i])\n",
        "  print(clause_list[i])\n",
        "  print('\\n--------\\n')"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> i suppose i am happy , being so tiny it means i am able to surprise people with what is generally seen as my confident and outgoing personality . <end>\n",
            "happy\n",
            "being so tiny\n",
            "[['i suppose i am happy', 'being so tiny', 'it means i am able to surprise people with what is generally seen as my confident and outgoing personality', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> lennox has always truly wanted to fight for the world title and was happy , because he was taking the tough route . <end>\n",
            "happy\n",
            "because he was taking the tough route\n",
            "[['lennox has always truly wanted to fight for the world title and was happy', 'because he was taking the tough route', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> he was a professional musician now , still sensitive and happy , doing something he loved . <end>\n",
            "happy\n",
            "doing something he loved\n",
            "[['he was a professional musician now', 'still sensitive and happy', 'doing something he loved', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> holmes is happy , because , he has the freedom of the house when we are out . <end>\n",
            "happy\n",
            "he has the freedom of the house when we are out\n",
            "[['holmes is happy', 'because', 'he has the freedom of the house when we are out', '']]\n",
            "\n",
            "--------\n",
            "\n",
            "<start> i had problems with tutors trying to encourage me to diversify my work and experiment with other styles , but i was quite happy , with the direction my work was heading so i stubbornly stuck to it . <end>\n",
            "happy\n",
            "with the direction my work was heading\n",
            "[['i had problems with tutors trying to encourage me to diversify my work and experiment with other styles', 'but i was quite happy', 'with the direction my work was heading so i stubbornly stuck to it', '']]\n",
            "\n",
            "--------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnB45wnBgip8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b09c45b-22b2-4e84-8abe-306bef14fa73"
      },
      "source": [
        "X=document\n",
        "#y=list(zip(emotion, cause))\n",
        "y=emotion\n",
        "print(y)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['happy', 'happy', 'happy', 'happy', 'happy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6CmGk1oN-bN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN58lnJBfZQK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "10be716c-9a09-4ec7-c21a-6edbf4aa7996"
      },
      "source": [
        "tokenizer=keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"xxxxxxx\")\n",
        "tokenizer.fit_on_texts(X)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_dict=tokenizer.word_index\n",
        "print(len(X_dict))\n",
        "print(X_dict.items())"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "77\n",
            "dict_items([('xxxxxxx', 1), ('i', 2), ('start', 3), ('happy', 4), ('to', 5), ('end', 6), ('the', 7), ('was', 8), ('with', 9), ('and', 10), ('he', 11), ('my', 12), ('am', 13), ('so', 14), ('it', 15), ('is', 16), ('has', 17), ('because', 18), ('work', 19), ('suppose', 20), ('being', 21), ('tiny', 22), ('means', 23), ('able', 24), ('surprise', 25), ('people', 26), ('what', 27), ('generally', 28), ('seen', 29), ('as', 30), ('confident', 31), ('outgoing', 32), ('personality', 33), ('lennox', 34), ('always', 35), ('truly', 36), ('wanted', 37), ('fight', 38), ('for', 39), ('world', 40), ('title', 41), ('taking', 42), ('tough', 43), ('route', 44), ('a', 45), ('professional', 46), ('musician', 47), ('now', 48), ('still', 49), ('sensitive', 50), ('doing', 51), ('something', 52), ('loved', 53), ('holmes', 54), ('freedom', 55), ('of', 56), ('house', 57), ('when', 58), ('we', 59), ('are', 60), ('out', 61), ('had', 62), ('problems', 63), ('tutors', 64), ('trying', 65), ('encourage', 66), ('me', 67), ('diversify', 68), ('experiment', 69), ('other', 70), ('styles', 71), ('but', 72), ('quite', 73), ('direction', 74), ('heading', 75), ('stubbornly', 76), ('stuck', 77)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIt03pLsf8LF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_seq=tokenizer.texts_to_sequences(X)\n",
        "#X_seq[:2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t4xHwpWgB44",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "06890fc6-9cf8-42c2-b763-77bf6f55a575"
      },
      "source": [
        "X_padded_seq=pad_sequences(X_seq,padding='post',maxlen=40)\n",
        "X_padded_seq[:3]"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  2, 20,  2, 13,  4, 21, 14, 22, 15, 23,  2, 13, 24,  5, 25,\n",
              "        26,  9, 27, 16, 28, 29, 30, 12, 31, 10, 32, 33,  6,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 3, 34, 17, 35, 36, 37,  5, 38, 39,  7, 40, 41, 10,  8,  4, 18,\n",
              "        11,  8, 42,  7, 43, 44,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 3, 11,  8, 45, 46, 47, 48, 49, 50, 10,  4, 51, 52, 11, 53,  6,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkIi0Z7zgFoM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1149af35-42a6-481d-bf54-317dc9b0b9f4"
      },
      "source": [
        "print(X_padded_seq.shape)\n",
        "y=np.array(y)\n",
        "\n",
        "print(y.shape)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 40)\n",
            "(5,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT1VS2u-gF9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "text_model = tf.keras.Sequential([tf.keras.layers.Embedding(input_length=40,input_dim=10000,output_dim=50),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(6, activation=\"relu\"),\n",
        "    #tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr5fsVxVgLPn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "42243510-2943-479b-be6c-045bd0c61a8f"
      },
      "source": [
        "text_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "text_model.summary()\n"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, 40, 50)            500000    \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 2000)              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 6)                 12006     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 78)                546       \n",
            "=================================================================\n",
            "Total params: 512,552\n",
            "Trainable params: 512,552\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mABcWG5DyT27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "596JCfhYyUG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPpNWjtpgQka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "33b96558-6211-4652-fea2-57312b6e4709"
      },
      "source": [
        "text_model.fit(X_padded_seq, y, epochs=10)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-193-4e28b9ad4437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_padded_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2535\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2536\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2537\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2539\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    713\u001b[0m         raise ValueError('You are passing a target array of shape ' +\n\u001b[1;32m    714\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m                          \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m                          \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                          \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (5, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxoQxxIyEASK",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}